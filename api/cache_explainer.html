---
title: "API"
description: ""

head: |
  <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  
  <style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
  <style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
  <style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
  <style media="screen and (min-width: 700px)">
    main > .content-wrapper {
      display: flex;
      flex-direction: row-reverse;
      justify-content: flex-end;
    }
  </style>

js: |

  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script>
---

  <article id="content">
    
  

  

  <header>
  <h1 class="title">Module <code>ethik.cache_explainer</code></h1>
  </header>

  <section id="section-intro">
  
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">import collections
import functools
import itertools
import math
import warnings

import numpy as np
import pandas as pd
import plotly.graph_objs as go

from .base_explainer import BaseExplainer
from .query import Query
from .utils import set_fig_size, to_pandas
from .warnings import ConvergenceWarning

__all__ = [&#34;CacheExplainer&#34;]


class CacheExplainer(BaseExplainer):
    &#34;&#34;&#34;Explains the influence of features on model predictions and performance.

    Parameters:
        alpha (float): A `float` between `0` and `0.5` which indicates by how close the `CacheExplainer`
            should look at extreme values of a distribution. The closer to zero, the more so
            extreme values will be accounted for. The default is `0.05` which means that all values
            beyond the 5th and 95th quantiles are ignored.
        n_taus (int): The number of τ values to consider. The results will be more fine-grained the
            higher this value is. However the computation time increases linearly with `n_taus`.
            The default is `41` and corresponds to each τ being separated by it&#39;s neighbors by
            `0.05`.
        n_samples (int): The number of samples to use for the confidence interval.
            If `1`, the default, no confidence interval is computed.
        sample_frac (float): The proportion of lines in the dataset sampled to
            generate the samples for the confidence interval. If `n_samples` is
            `1`, no confidence interval is computed and the whole dataset is used.
            Default is `0.8`.
        conf_level (float): A `float` between `0` and `0.5` which indicates the
            quantile used for the confidence interval. Default is `0.05`, which
            means that the confidence interval contains the data between the 5th
            and 95th quantiles.
        max_iterations (int): The maximum number of iterations used when applying the Newton step
            of the optimization procedure. Default is `5`.
        tol (float): The bottom threshold for the gradient of the optimization
            procedure. When reached, the procedure stops. Otherwise, a warning
            is raised about the fact that the optimization did not converge.
            Default is `1e-4`.
        n_jobs (int): The number of jobs to use for parallel computations. See
            `joblib.Parallel()`. Default is `-1`.
        memoize (bool): Indicates whether or not memoization should be used or not. If `True`, then
            intermediate results will be stored in order to avoid recomputing results that can be
            reused by successively called methods. For example, if you call `plot_influence` followed by
            `plot_influence_ranking` and `memoize` is `True`, then the intermediate results required by
            `plot_influence` will be reused for `plot_influence_ranking`. Memoization is turned off by
            default because it can lead to unexpected behavior depending on your usage.
        verbose (bool): Whether or not to show progress bars during
            computations. Default is `True`.
    &#34;&#34;&#34;

    def __init__(
        self,
        alpha=0.05,
        n_taus=41,
        n_samples=1,
        sample_frac=0.8,
        conf_level=0.05,
        max_iterations=15,
        tol=1e-4,
        n_jobs=1,  # Parallelism is only worth it if the dataset is &#34;large&#34;
        memoize=False,
        verbose=True,
    ):
        super().__init__(
            alpha=alpha,
            n_samples=n_samples,
            sample_frac=sample_frac,
            conf_level=conf_level,
            max_iterations=max_iterations,
            tol=tol,
            n_jobs=n_jobs,
            verbose=verbose,
        )

        if not n_taus &gt; 0:
            raise ValueError(
                f&#34;n_taus must be a strictly positive integer, got {n_taus}&#34;
            )

        # `n_taus` needs to be odd to include the mean (tau == 0)
        if not n_taus % 2:
            n_taus += 1

        self.n_taus = n_taus
        self.memoize = memoize
        self.metric_names = set()
        self._reset_info()

    def _reset_info(self):
        &#34;&#34;&#34;Resets the info dataframe (for when memoization is turned off).&#34;&#34;&#34;
        self.info = pd.DataFrame(
            columns=[
                &#34;group&#34;,
                &#34;feature&#34;,
                &#34;tau&#34;,
                &#34;target&#34;,
                &#34;ksi&#34;,
                &#34;label&#34;,
                &#34;influence&#34;,
                &#34;influence_low&#34;,
                &#34;influence_high&#34;,
            ]
        )

    def get_metric_name(self, metric):
        &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
        performance with respect of the given metric.

        Args:
            metric (callable): The metric to compute the model&#39;s performance.

        Returns:
            str: The name of the column.
        &#34;&#34;&#34;
        name = super().get_metric_name(metric)
        if name in self.info.columns and name not in self.metric_names:
            raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
        return name

    def _explain_with_cache(
        self, X_test, y_pred, explain, link_variables=False, constraints=None
    ):
        if not self.memoize:
            self._reset_info()

        # We need dataframes for the return. To make the conversion faster in the
        # following methods, we do it now.
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))
        X_test = self._one_hot_encode(X_test)

        query = Query.from_taus(
            X_test=X_test,
            labels=y_pred.columns,
            n_taus=self.n_taus,  #  TODO: it&#39;s a lot of points for link_variables=True
            q=[self.alpha, 1 - self.alpha],
            constraints=constraints,
            link_variables=link_variables,
        )

        query_index = pd.MultiIndex.from_arrays([query[&#34;group&#34;], query[&#34;label&#34;]])
        info_index = pd.MultiIndex.from_arrays([self.info[&#34;group&#34;], self.info[&#34;label&#34;]])
        diff_index = ~query_index.isin(info_index)

        self.info = self.info.append(query[diff_index], ignore_index=True, sort=False)
        self.info = explain(query=self.info)

        queried_groups = query[&#34;group&#34;].unique()
        ret = self.info[
            self.info[&#34;group&#34;].isin(queried_groups)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
        ]

        n_not_converged = len(ret[~ret[&#34;converged&#34;]])
        if n_not_converged:
            warnings.warn(
                message=f&#34;{n_not_converged} groups didn&#39;t converge.&#34;,
                category=ConvergenceWarning,
            )

        return ret

    def explain_influence(self, X_test, y_pred, link_variables=False, constraints=None):
        &#34;&#34;&#34;Compute the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                `pd.Series` is expected. For multi-label classification, a
                pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            link_variables (bool, optional): Whether to make a multidimensional
                explanation or not. Default is `False`, which means that all
                the features in `X_test` are considered independently (so the
                correlation are not taken into account).
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high)`. If `explainer.n_samples` is `1`,
                no confidence interval is computed and `influence = influence_low = influence_high`.
                The value of `label` is not important for regression.

        Examples:
            See more examples in `notebooks`.

            Binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            Regression is similar to binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [22, 24, 19]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            For multi-label classification, we need a dataframe to store predictions:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred.columns
            [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
            &gt;&gt;&gt; y_pred.iloc[0]
            [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
        &#34;&#34;&#34;
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_influence, X_test=X_test, y_pred=y_pred
            ),
            link_variables=link_variables,
            constraints=constraints,
        )

    def explain_performance(
        self, X_test, y_test, y_pred, metric, link_variables=False, constraints=None
    ):
        &#34;&#34;&#34;Compute the change in model&#39;s performance for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true values
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.
            link_variables (bool, optional): Whether to make a multidimensional
                explanation or not. Default is `False`, which means that all
                the features in `X_test` are considered independently (so the
                correlation are not taken into account).
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)`.
                If `explainer.n_samples` is `1`, no confidence interval is computed
                and `&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;`.
                The value of `label` is not important for regression.

        Examples:
            See examples in `notebooks`.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        self.metric_names.add(metric_name)
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_performance,
                X_test=X_test,
                y_pred=y_pred,
                y_test=y_test,
                metric=metric,
            ),
            link_variables=link_variables,
            constraints=constraints,
        )

    def rank_by_influence(self, X_test, y_pred):
        &#34;&#34;&#34;Returns a pandas DataFrame containing the importance of each feature
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).

        Returns:
            pd.DataFrame:
                A dataframe with columns `(label, feature, importance)`. The row
                `(setosa, petal length (cm), 0.282507)` means that the feature
                `petal length` of the Iris dataset has an importance of about
                30% in the prediction of the class `setosa`.

                The importance is a real number between 0 and 1. Intuitively,
                if the model influence for the feature `X` is a flat curve (the average
                model prediction is not impacted by the mean of `X`) then we
                can conclude that `X` has no importance for predictions. This
                flat curve is the baseline and satisfies \\(y = influence_{\\tau(0)}\\).
                To compute the importance of a feature, we look at the average
                distance of the influence curve to this baseline:

                $$
                I(X) = \\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mid influence_{\\tau(i)}(X) - influence_{\\tau(0)}(X) \\mid
                $$

                The influence curve is first normalized so that the importance is
                between 0 and 1 (which may not be the case originally for regression
                problems). To normalize, we get the minimum and maximum influences
                *across all features and all classes* and then compute
                `normalized = (influence - min) / (max - min)`.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;

        def get_importance(group, min_influence, max_influence):
            &#34;&#34;&#34;Computes the average absolute difference in influence changes per tau increase.&#34;&#34;&#34;
            #  Normalize influence to get an importance between 0 and 1
            # influence can be outside [0, 1] for regression
            influence = group[&#34;influence&#34;]
            group[&#34;influence&#34;] = (influence - min_influence) / (
                max_influence - min_influence
            )
            baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;influence&#34;]
            return (group[&#34;influence&#34;] - baseline).abs().mean()

        explanation = self.explain_influence(X_test=X_test, y_pred=y_pred)
        min_influence = explanation[&#34;influence&#34;].min()
        max_influence = explanation[&#34;influence&#34;].max()

        return (
            explanation.groupby([&#34;label&#34;, &#34;feature&#34;])
            .apply(
                functools.partial(
                    get_importance,
                    min_influence=min_influence,
                    max_influence=max_influence,
                )
            )
            .to_frame(&#34;importance&#34;)
            .reset_index()
        )

    def rank_by_performance(self, X_test, y_test, y_pred, metric):
        &#34;&#34;&#34;Returns a pandas DataFrame containing
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true output
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, min, max)`. The row
                `(age, 0.862010, 0.996360)` means that the score measured by the
                given metric (e.g. `sklearn.metrics.accuracy_score`) stays bewteen
                86.2% and 99.6% on average when we make the mean age change. With
                such information, we can find the features for which the model
                performs the worst or the best.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)

        def get_aggregates(df):
            return pd.Series(
                [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
            )

        return (
            self.explain_performance(X_test, y_test, y_pred, metric)
            .groupby(&#34;feature&#34;)
            .apply(get_aggregates)
            .reset_index()
        )

    def _plot_explanation(
        self,
        explanation,
        y_col,
        y_label,
        colors=None,
        yrange=None,
        size=None,
        constraints=None,
    ):
        if constraints is not None:
            explanation = explanation[~explanation[&#34;feature&#34;].isin(constraints)]
            constraints_title = &#34;, &#34;.join(f&#34;{f}={v}&#34; for f, v in constraints.items())
        else:
            constraints_title = &#34;&#34;

        features = explanation[&#34;feature&#34;].unique()

        if colors is None:
            colors = {}
        elif type(colors) is str:
            colors = {feat: colors for feat in features}

        #  There are multiple features, we plot them together with taus
        if len(features) &gt; 1:
            fig = go.Figure()

            for i, feat in enumerate(features):
                taus = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;tau&#34;]
                targets = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;target&#34;]
                y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[y_col]
                converged = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;converged&#34;]
                fig.add_trace(
                    go.Scatter(
                        x=taus,
                        y=y,
                        mode=&#34;lines+markers&#34;,
                        hoverinfo=&#34;y&#34;,
                        name=feat,
                        customdata=list(zip(taus, targets)),
                        marker=dict(
                            color=colors.get(feat),
                            opacity=[1 if c else 0.5 for c in converged],
                        ),
                    )
                )

            x_title = &#34;tau&#34;
            if constraints_title:
                x_title += f&#34; (with {constraints_title})&#34;

            fig.update_layout(
                margin=dict(t=30, r=50, b=40),
                xaxis=dict(title=x_title, nticks=5),
                yaxis=dict(title=y_label, range=yrange),
            )
            set_fig_size(fig, size)
            return fig

        #  There is only one feature, we plot it with its nominal values.
        feat = features[0]
        fig = go.Figure()
        x = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;target&#34;]
        y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[y_col]
        converged = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;converged&#34;]

        if self.n_samples &gt; 1:
            low = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{y_col}_low&#34;]
            high = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{y_col}_high&#34;]
            fig.add_trace(
                go.Scatter(
                    x=np.concatenate((x, x[::-1])),
                    y=np.concatenate((low, high[::-1])),
                    name=f&#34;{self.conf_level * 100}% - {(1 - self.conf_level) * 100}%&#34;,
                    fill=&#34;toself&#34;,
                    fillcolor=colors.get(feat),
                    line_color=&#34;rgba(0, 0, 0, 0)&#34;,
                    opacity=0.3,
                )
            )

        fig.add_trace(
            go.Scatter(
                x=x,
                y=y,
                mode=&#34;lines+markers&#34;,
                hoverinfo=&#34;x+y&#34;,
                showlegend=False,
                marker=dict(
                    color=colors.get(feat), opacity=[1 if c else 0.5 for c in converged]
                ),
            )
        )

        mean_row = explanation.query(f&#39;feature == &#34;{feat}&#34; and tau == 0&#39;).iloc[0]
        fig.add_trace(
            go.Scatter(
                x=[mean_row[&#34;target&#34;]],
                y=[mean_row[y_col]],
                text=[&#34;Dataset mean&#34;],
                showlegend=False,
                mode=&#34;markers&#34;,
                name=&#34;Dataset mean&#34;,
                hoverinfo=&#34;text&#34;,
                marker=dict(symbol=&#34;x&#34;, size=9, color=colors.get(feat)),
            )
        )

        x_title = f&#34;Average {feat}&#34;
        if constraints_title:
            x_title += f&#34; (with {constraints_title})&#34;

        fig.update_layout(
            margin=dict(t=30, r=0, b=40),
            xaxis=dict(title=x_title),
            yaxis=dict(title=y_label, range=yrange),
        )
        set_fig_size(fig, size)
        return fig

    def _plot_explanation_2d(
        self,
        explanation,
        z_col,
        z_label,
        z_range=None,
        colorscale=None,
        size=None,
        constraints=None,
    ):
        if constraints is not None:
            explanation = explanation[~explanation[&#34;feature&#34;].isin(constraints)]

        fx, fy = explanation[&#34;feature&#34;].unique()
        x = np.sort(explanation[explanation[&#34;feature&#34;] == fx][&#34;target&#34;].unique())
        y = np.sort(explanation[explanation[&#34;feature&#34;] == fy][&#34;target&#34;].unique())

        z = explanation[explanation[&#34;feature&#34;] == fx][z_col].to_numpy()
        z_converged = explanation[explanation[&#34;feature&#34;] == fx][&#34;converged&#34;].to_numpy()
        z[np.logical_not(z_converged)] = math.nan
        z = z.reshape((len(x), len(y)))

        z_min = z_max = None
        if z_range is not None:
            z_min, z_max = z_range

        fig = go.Figure()
        fig.add_heatmap(
            x=x,
            y=y,
            z=z,
            zmin=z_min,
            zmax=z_max,
            colorscale=colorscale or &#34;Reds&#34;,
            colorbar=dict(title=z_label),
            hoverinfo=&#34;x+y+z&#34;,
        )

        mean_x = explanation.query(f&#39;feature == &#34;{fx}&#34; and tau == 0&#39;).iloc[0][&#34;target&#34;]
        mean_y = explanation.query(f&#39;feature == &#34;{fy}&#34; and tau == 0&#39;).iloc[0][&#34;target&#34;]
        fig.add_scatter(
            x=[mean_x],
            y=[mean_y],
            text=[&#34;Dataset mean&#34;],
            showlegend=False,
            mode=&#34;markers&#34;,
            name=&#34;Dataset mean&#34;,
            hoverinfo=&#34;text&#34;,
            marker=dict(symbol=&#34;x&#34;, size=9, color=&#34;black&#34;),
        )

        title = None
        if constraints is not None:
            title = &#34;Constraints: &#34; + &#34;, &#34;.join(
                f&#34;{f}={v}&#34; for f, v in constraints.items()
            )

        fig.update_layout(
            margin=dict(t=30, b=40),
            xaxis=dict(title=f&#34;Average {fx}&#34;, mirror=True),
            yaxis=dict(title=f&#34;Average {fy}&#34;, mirror=True),
            title=title,
        )
        set_fig_size(fig, size)
        return fig

    def _plot_ranking(
        self, ranking, score_column, title, n_features=None, colors=None, size=None
    ):
        if n_features is None:
            n_features = len(ranking)
        ascending = n_features &gt;= 0
        ranking = ranking.sort_values(by=[score_column], ascending=ascending)
        n_features = abs(n_features)

        fig = go.Figure()
        fig.add_bar(
            x=ranking[score_column][-n_features:],
            y=ranking[&#34;feature&#34;][-n_features:],
            orientation=&#34;h&#34;,
            hoverinfo=&#34;x&#34;,
            marker=dict(color=colors),
        )
        fig.update_layout(
            margin=dict(b=0, t=40, r=10),
            xaxis=dict(title=title, range=[0, 1], side=&#34;top&#34;, fixedrange=True),
            yaxis=dict(fixedrange=True, automargin=True),
            modebar=dict(
                orientation=&#34;v&#34;,
                color=&#34;rgba(0, 0, 0, 0)&#34;,
                activecolor=&#34;rgba(0, 0, 0, 0)&#34;,
                bgcolor=&#34;rgba(0, 0, 0, 0)&#34;,
            ),
        )
        set_fig_size(fig, size, width=500, height=100 + 60 * n_features)
        return fig

    def plot_influence(
        self, X_test, y_pred, colors=None, yrange=None, size=None, constraints=None
    ):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colors (dict, optional): A dictionary that maps features to colors.
                Default is `None` and the colors are choosen automatically.
            yrange (list, optional): A two-item list `[low, high]`. Default is
                `None` and the range is based on the data.
            size (tuple, optional): An optional couple `(width, height)` in pixels.
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.

        Examples:
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
            ...     x0=&#34;blue&#34;,
            ...     x1=&#34;red&#34;,
            ... ))
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
        &#34;&#34;&#34;
        explanation = self.explain_influence(X_test, y_pred, constraints=constraints)
        labels = explanation[&#34;label&#34;].unique()

        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)

        return self._plot_explanation(
            explanation=explanation,
            y_col=&#34;influence&#34;,
            y_label=f&#34;Average {labels[0]}&#34;,
            colors=colors,
            yrange=yrange,
            size=size,
            constraints=constraints,
        )

    def plot_influence_2d(
        self, X_test, y_pred, z_range=None, colorscale=None, size=None, constraints=None
    ):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame): The dataset as a pandas dataframe
                with one column per feature. Must contain exactly two columns.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colorscale (str, optional): The colorscale used for the heatmap.
                See `plotly.graph_objs.Heatmap`. Default is `None`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        n_constraints = 0 if constraints is None else len(constraints)
        if len(X_test.columns) != (2 + n_constraints):
            raise ValueError(
                &#34;`X_test` must contain exactly two columns in addition to the constraints.&#34;
            )

        explanation = self.explain_influence(
            X_test, y_pred, link_variables=True, constraints=constraints
        )
        labels = explanation[&#34;label&#34;].unique()

        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)

        return self._plot_explanation_2d(
            explanation,
            z_col=&#34;influence&#34;,
            z_label=f&#34;Average {labels[0]}&#34;,
            z_range=z_range,
            colorscale=colorscale,
            size=size,
            constraints=constraints,
        )

    def plot_influence_ranking(
        self, X_test, y_pred, n_features=None, colors=None, size=None
    ):
        &#34;&#34;&#34;Plot the ranking of the features based on their influence.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        ranking = self.rank_by_influence(X_test=X_test, y_pred=y_pred)
        return self._plot_ranking(
            ranking=ranking,
            score_column=&#34;importance&#34;,
            title=&#34;Importance&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )

    def plot_performance(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        colors=None,
        yrange=None,
        size=None,
        constraints=None,
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            yrange (list, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): See `CacheExplainer.plot_influence()`.
            constraints (dict, optional): See `CacheExplainer.plot_influence()`.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test,
            y_test=y_test,
            y_pred=y_pred,
            metric=metric,
            constraints=constraints,
        )
        if yrange is None and explanation[metric_name].between(0, 1).all():
            yrange = [0, 1]

        #  The performance is the same for all labels, we remove duplicates
        label = explanation[&#34;label&#34;].unique()[0]
        explanation = explanation[explanation[&#34;label&#34;] == label]

        return self._plot_explanation(
            explanation=explanation,
            y_col=metric_name,
            y_label=f&#34;Average {metric_name}&#34;,
            colors=colors,
            yrange=yrange,
            size=size,
            constraints=constraints,
        )

    def plot_performance_2d(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        z_range=None,
        colorscale=None,
        size=None,
        constraints=None,
    ):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame): The dataset as a pandas dataframe
                with one column per feature. Must contain exactly two columns.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colorscale (str, optional): The colorscale used for the heatmap.
                See `plotly.graph_objs.Heatmap`. Default is `None`.
            size (tuple, optional): See `CacheExplainer.plot_influence()`.
            constraints (dict, optional): See `CacheExplainer.plot_influence()`.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        n_constraints = 0 if constraints is None else len(constraints)
        if len(X_test.columns) != (2 + n_constraints):
            raise ValueError(
                &#34;`X_test` must contain exactly two columns in addition to the constraints.&#34;
            )

        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test,
            y_test=y_test,
            y_pred=y_pred,
            metric=metric,
            link_variables=True,
            constraints=constraints,
        )
        if z_range is None and explanation[metric_name].between(0, 1).all():
            z_range = [0, 1]

        return self._plot_explanation_2d(
            explanation,
            z_col=metric_name,
            z_label=f&#34;Average {metric_name}&#34;,
            z_range=z_range,
            colorscale=colorscale,
            size=size,
            constraints=constraints,
        )

    def plot_performance_ranking(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        criterion,
        n_features=None,
        colors=None,
        size=None,
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            criterion (str): Either &#34;min&#34; or &#34;max&#34; to determine whether, for a
                given feature, we keep the worst or the best performance for all
                the values taken by the mean. See `CacheExplainer.rank_by_performance()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence_ranking()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        ranking = self.rank_by_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        return self._plot_ranking(
            ranking=ranking,
            score_column=criterion,
            title=f&#34;{criterion} {metric_name}&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )

    def plot_weight_distribution(
        self, feature_values, proportion, threshold=None, color=None, size=None
    ):
        &#34;&#34;&#34;Plot, for every target mean, how many individuals capture
        `proportion` of the total weight. For instance, we could see that for
        a target mean of 25 year-old (if the feature is the age), 50% of the
        weight is distributed to 14% of the individuals &#34;only&#34;. If &#34;few&#34; individuals
        get &#34;a lot of&#34; weight, it means that the stressed distribution is &#34;quite&#34;
        different from the original one and that the results are not reliable. Defining
        a relevant threshold is an open question.

        Parameters:
            feature_values (pd.Series): See `ethik.base_explainer.BaseExplainer.compute_weights()`.
            proportion (float): The proportion of weight to check, between 0 and 1.
            threshold (float, optional): An optional threshold to display on the
                plot. Must be between 0 and 1.
            colors (list, optional): An optional list of colors for all targets.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        #  If `feature_values.name` is `None`, converting it to a dataframe
        # will create a column `0`, which will not be matched by the query below
        # since its &#34;feature&#34; column will be `None` and not `0`.
        if feature_values.name is None:
            feature_values = feature_values.rename(&#34;feature&#34;)

        X_test = pd.DataFrame(to_pandas(feature_values))
        targets = self._build_targets(X_test=X_test)[feature_values.name]

        return super().plot_weight_distribution(
            feature_values=feature_values,
            targets=targets,
            proportion=proportion,
            threshold=threshold,
            color=color,
            size=size,
        )</code></pre>
      </details>

  </section>

  <section>
  </section>

  <section>
  </section>

  <section>
  </section>

  <section>
    <h2 class="section-title" id="header-classes">Classes</h2>
    <dl>
      
      <dt id="ethik.cache_explainer.CacheExplainer"><code class="flex name class">
          <span>class <span class="ident">CacheExplainer</span></span>
              <span>(</span><span>alpha=0.05, n_taus=41, n_samples=1, sample_frac=0.8, conf_level=0.05, max_iterations=15, tol=0.0001, n_jobs=1, memoize=False, verbose=True)</span>
      </code></dt>

      <dd>
  
  <section class="desc"><p>Explains the influence of features on model predictions and performance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates by how close the <a title="ethik.cache_explainer.CacheExplainer" href="#ethik.cache_explainer.CacheExplainer"><code>CacheExplainer</code></a>
should look at extreme values of a distribution. The closer to zero, the more so
extreme values will be accounted for. The default is <code>0.05</code> which means that all values
beyond the 5th and 95th quantiles are ignored.</dd>
<dt><strong><code>n_taus</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of τ values to consider. The results will be more fine-grained the
higher this value is. However the computation time increases linearly with <code>n_taus</code>.
The default is <code>41</code> and corresponds to each τ being separated by it's neighbors by
<code>0.05</code>.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples to use for the confidence interval.
If <code>1</code>, the default, no confidence interval is computed.</dd>
<dt><strong><code>sample_frac</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of lines in the dataset sampled to
generate the samples for the confidence interval. If <code>n_samples</code> is
<code>1</code>, no confidence interval is computed and the whole dataset is used.
Default is <code>0.8</code>.</dd>
<dt><strong><code>conf_level</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates the
quantile used for the confidence interval. Default is <code>0.05</code>, which
means that the confidence interval contains the data between the 5th
and 95th quantiles.</dd>
<dt><strong><code>max_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>The maximum number of iterations used when applying the Newton step
of the optimization procedure. Default is <code>5</code>.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>The bottom threshold for the gradient of the optimization
procedure. When reached, the procedure stops. Otherwise, a warning
is raised about the fact that the optimization did not converge.
Default is <code>1e-4</code>.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of jobs to use for parallel computations. See
<code>joblib.Parallel()</code>. Default is <code>-1</code>.</dd>
<dt><strong><code>memoize</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether or not memoization should be used or not. If <code>True</code>, then
intermediate results will be stored in order to avoid recomputing results that can be
reused by successively called methods. For example, if you call <code>plot_influence</code> followed by
<code>plot_influence_ranking</code> and <code>memoize</code> is <code>True</code>, then the intermediate results required by
<code>plot_influence</code> will be reused for <code>plot_influence_ranking</code>. Memoization is turned off by
default because it can lead to unexpected behavior depending on your usage.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not to show progress bars during
computations. Default is <code>True</code>.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">class CacheExplainer(BaseExplainer):
    &#34;&#34;&#34;Explains the influence of features on model predictions and performance.

    Parameters:
        alpha (float): A `float` between `0` and `0.5` which indicates by how close the `CacheExplainer`
            should look at extreme values of a distribution. The closer to zero, the more so
            extreme values will be accounted for. The default is `0.05` which means that all values
            beyond the 5th and 95th quantiles are ignored.
        n_taus (int): The number of τ values to consider. The results will be more fine-grained the
            higher this value is. However the computation time increases linearly with `n_taus`.
            The default is `41` and corresponds to each τ being separated by it&#39;s neighbors by
            `0.05`.
        n_samples (int): The number of samples to use for the confidence interval.
            If `1`, the default, no confidence interval is computed.
        sample_frac (float): The proportion of lines in the dataset sampled to
            generate the samples for the confidence interval. If `n_samples` is
            `1`, no confidence interval is computed and the whole dataset is used.
            Default is `0.8`.
        conf_level (float): A `float` between `0` and `0.5` which indicates the
            quantile used for the confidence interval. Default is `0.05`, which
            means that the confidence interval contains the data between the 5th
            and 95th quantiles.
        max_iterations (int): The maximum number of iterations used when applying the Newton step
            of the optimization procedure. Default is `5`.
        tol (float): The bottom threshold for the gradient of the optimization
            procedure. When reached, the procedure stops. Otherwise, a warning
            is raised about the fact that the optimization did not converge.
            Default is `1e-4`.
        n_jobs (int): The number of jobs to use for parallel computations. See
            `joblib.Parallel()`. Default is `-1`.
        memoize (bool): Indicates whether or not memoization should be used or not. If `True`, then
            intermediate results will be stored in order to avoid recomputing results that can be
            reused by successively called methods. For example, if you call `plot_influence` followed by
            `plot_influence_ranking` and `memoize` is `True`, then the intermediate results required by
            `plot_influence` will be reused for `plot_influence_ranking`. Memoization is turned off by
            default because it can lead to unexpected behavior depending on your usage.
        verbose (bool): Whether or not to show progress bars during
            computations. Default is `True`.
    &#34;&#34;&#34;

    def __init__(
        self,
        alpha=0.05,
        n_taus=41,
        n_samples=1,
        sample_frac=0.8,
        conf_level=0.05,
        max_iterations=15,
        tol=1e-4,
        n_jobs=1,  # Parallelism is only worth it if the dataset is &#34;large&#34;
        memoize=False,
        verbose=True,
    ):
        super().__init__(
            alpha=alpha,
            n_samples=n_samples,
            sample_frac=sample_frac,
            conf_level=conf_level,
            max_iterations=max_iterations,
            tol=tol,
            n_jobs=n_jobs,
            verbose=verbose,
        )

        if not n_taus &gt; 0:
            raise ValueError(
                f&#34;n_taus must be a strictly positive integer, got {n_taus}&#34;
            )

        # `n_taus` needs to be odd to include the mean (tau == 0)
        if not n_taus % 2:
            n_taus += 1

        self.n_taus = n_taus
        self.memoize = memoize
        self.metric_names = set()
        self._reset_info()

    def _reset_info(self):
        &#34;&#34;&#34;Resets the info dataframe (for when memoization is turned off).&#34;&#34;&#34;
        self.info = pd.DataFrame(
            columns=[
                &#34;group&#34;,
                &#34;feature&#34;,
                &#34;tau&#34;,
                &#34;target&#34;,
                &#34;ksi&#34;,
                &#34;label&#34;,
                &#34;influence&#34;,
                &#34;influence_low&#34;,
                &#34;influence_high&#34;,
            ]
        )

    def get_metric_name(self, metric):
        &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
        performance with respect of the given metric.

        Args:
            metric (callable): The metric to compute the model&#39;s performance.

        Returns:
            str: The name of the column.
        &#34;&#34;&#34;
        name = super().get_metric_name(metric)
        if name in self.info.columns and name not in self.metric_names:
            raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
        return name

    def _explain_with_cache(
        self, X_test, y_pred, explain, link_variables=False, constraints=None
    ):
        if not self.memoize:
            self._reset_info()

        # We need dataframes for the return. To make the conversion faster in the
        # following methods, we do it now.
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))
        X_test = self._one_hot_encode(X_test)

        query = Query.from_taus(
            X_test=X_test,
            labels=y_pred.columns,
            n_taus=self.n_taus,  #  TODO: it&#39;s a lot of points for link_variables=True
            q=[self.alpha, 1 - self.alpha],
            constraints=constraints,
            link_variables=link_variables,
        )

        query_index = pd.MultiIndex.from_arrays([query[&#34;group&#34;], query[&#34;label&#34;]])
        info_index = pd.MultiIndex.from_arrays([self.info[&#34;group&#34;], self.info[&#34;label&#34;]])
        diff_index = ~query_index.isin(info_index)

        self.info = self.info.append(query[diff_index], ignore_index=True, sort=False)
        self.info = explain(query=self.info)

        queried_groups = query[&#34;group&#34;].unique()
        ret = self.info[
            self.info[&#34;group&#34;].isin(queried_groups)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
        ]

        n_not_converged = len(ret[~ret[&#34;converged&#34;]])
        if n_not_converged:
            warnings.warn(
                message=f&#34;{n_not_converged} groups didn&#39;t converge.&#34;,
                category=ConvergenceWarning,
            )

        return ret

    def explain_influence(self, X_test, y_pred, link_variables=False, constraints=None):
        &#34;&#34;&#34;Compute the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                `pd.Series` is expected. For multi-label classification, a
                pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            link_variables (bool, optional): Whether to make a multidimensional
                explanation or not. Default is `False`, which means that all
                the features in `X_test` are considered independently (so the
                correlation are not taken into account).
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high)`. If `explainer.n_samples` is `1`,
                no confidence interval is computed and `influence = influence_low = influence_high`.
                The value of `label` is not important for regression.

        Examples:
            See more examples in `notebooks`.

            Binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            Regression is similar to binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [22, 24, 19]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            For multi-label classification, we need a dataframe to store predictions:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred.columns
            [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
            &gt;&gt;&gt; y_pred.iloc[0]
            [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
        &#34;&#34;&#34;
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_influence, X_test=X_test, y_pred=y_pred
            ),
            link_variables=link_variables,
            constraints=constraints,
        )

    def explain_performance(
        self, X_test, y_test, y_pred, metric, link_variables=False, constraints=None
    ):
        &#34;&#34;&#34;Compute the change in model&#39;s performance for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true values
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.
            link_variables (bool, optional): Whether to make a multidimensional
                explanation or not. Default is `False`, which means that all
                the features in `X_test` are considered independently (so the
                correlation are not taken into account).
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)`.
                If `explainer.n_samples` is `1`, no confidence interval is computed
                and `&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;`.
                The value of `label` is not important for regression.

        Examples:
            See examples in `notebooks`.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        self.metric_names.add(metric_name)
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_performance,
                X_test=X_test,
                y_pred=y_pred,
                y_test=y_test,
                metric=metric,
            ),
            link_variables=link_variables,
            constraints=constraints,
        )

    def rank_by_influence(self, X_test, y_pred):
        &#34;&#34;&#34;Returns a pandas DataFrame containing the importance of each feature
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).

        Returns:
            pd.DataFrame:
                A dataframe with columns `(label, feature, importance)`. The row
                `(setosa, petal length (cm), 0.282507)` means that the feature
                `petal length` of the Iris dataset has an importance of about
                30% in the prediction of the class `setosa`.

                The importance is a real number between 0 and 1. Intuitively,
                if the model influence for the feature `X` is a flat curve (the average
                model prediction is not impacted by the mean of `X`) then we
                can conclude that `X` has no importance for predictions. This
                flat curve is the baseline and satisfies \\(y = influence_{\\tau(0)}\\).
                To compute the importance of a feature, we look at the average
                distance of the influence curve to this baseline:

                $$
                I(X) = \\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mid influence_{\\tau(i)}(X) - influence_{\\tau(0)}(X) \\mid
                $$

                The influence curve is first normalized so that the importance is
                between 0 and 1 (which may not be the case originally for regression
                problems). To normalize, we get the minimum and maximum influences
                *across all features and all classes* and then compute
                `normalized = (influence - min) / (max - min)`.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;

        def get_importance(group, min_influence, max_influence):
            &#34;&#34;&#34;Computes the average absolute difference in influence changes per tau increase.&#34;&#34;&#34;
            #  Normalize influence to get an importance between 0 and 1
            # influence can be outside [0, 1] for regression
            influence = group[&#34;influence&#34;]
            group[&#34;influence&#34;] = (influence - min_influence) / (
                max_influence - min_influence
            )
            baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;influence&#34;]
            return (group[&#34;influence&#34;] - baseline).abs().mean()

        explanation = self.explain_influence(X_test=X_test, y_pred=y_pred)
        min_influence = explanation[&#34;influence&#34;].min()
        max_influence = explanation[&#34;influence&#34;].max()

        return (
            explanation.groupby([&#34;label&#34;, &#34;feature&#34;])
            .apply(
                functools.partial(
                    get_importance,
                    min_influence=min_influence,
                    max_influence=max_influence,
                )
            )
            .to_frame(&#34;importance&#34;)
            .reset_index()
        )

    def rank_by_performance(self, X_test, y_test, y_pred, metric):
        &#34;&#34;&#34;Returns a pandas DataFrame containing
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true output
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, min, max)`. The row
                `(age, 0.862010, 0.996360)` means that the score measured by the
                given metric (e.g. `sklearn.metrics.accuracy_score`) stays bewteen
                86.2% and 99.6% on average when we make the mean age change. With
                such information, we can find the features for which the model
                performs the worst or the best.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)

        def get_aggregates(df):
            return pd.Series(
                [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
            )

        return (
            self.explain_performance(X_test, y_test, y_pred, metric)
            .groupby(&#34;feature&#34;)
            .apply(get_aggregates)
            .reset_index()
        )

    def _plot_explanation(
        self,
        explanation,
        y_col,
        y_label,
        colors=None,
        yrange=None,
        size=None,
        constraints=None,
    ):
        if constraints is not None:
            explanation = explanation[~explanation[&#34;feature&#34;].isin(constraints)]
            constraints_title = &#34;, &#34;.join(f&#34;{f}={v}&#34; for f, v in constraints.items())
        else:
            constraints_title = &#34;&#34;

        features = explanation[&#34;feature&#34;].unique()

        if colors is None:
            colors = {}
        elif type(colors) is str:
            colors = {feat: colors for feat in features}

        #  There are multiple features, we plot them together with taus
        if len(features) &gt; 1:
            fig = go.Figure()

            for i, feat in enumerate(features):
                taus = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;tau&#34;]
                targets = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;target&#34;]
                y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[y_col]
                converged = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;converged&#34;]
                fig.add_trace(
                    go.Scatter(
                        x=taus,
                        y=y,
                        mode=&#34;lines+markers&#34;,
                        hoverinfo=&#34;y&#34;,
                        name=feat,
                        customdata=list(zip(taus, targets)),
                        marker=dict(
                            color=colors.get(feat),
                            opacity=[1 if c else 0.5 for c in converged],
                        ),
                    )
                )

            x_title = &#34;tau&#34;
            if constraints_title:
                x_title += f&#34; (with {constraints_title})&#34;

            fig.update_layout(
                margin=dict(t=30, r=50, b=40),
                xaxis=dict(title=x_title, nticks=5),
                yaxis=dict(title=y_label, range=yrange),
            )
            set_fig_size(fig, size)
            return fig

        #  There is only one feature, we plot it with its nominal values.
        feat = features[0]
        fig = go.Figure()
        x = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;target&#34;]
        y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[y_col]
        converged = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;converged&#34;]

        if self.n_samples &gt; 1:
            low = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{y_col}_low&#34;]
            high = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{y_col}_high&#34;]
            fig.add_trace(
                go.Scatter(
                    x=np.concatenate((x, x[::-1])),
                    y=np.concatenate((low, high[::-1])),
                    name=f&#34;{self.conf_level * 100}% - {(1 - self.conf_level) * 100}%&#34;,
                    fill=&#34;toself&#34;,
                    fillcolor=colors.get(feat),
                    line_color=&#34;rgba(0, 0, 0, 0)&#34;,
                    opacity=0.3,
                )
            )

        fig.add_trace(
            go.Scatter(
                x=x,
                y=y,
                mode=&#34;lines+markers&#34;,
                hoverinfo=&#34;x+y&#34;,
                showlegend=False,
                marker=dict(
                    color=colors.get(feat), opacity=[1 if c else 0.5 for c in converged]
                ),
            )
        )

        mean_row = explanation.query(f&#39;feature == &#34;{feat}&#34; and tau == 0&#39;).iloc[0]
        fig.add_trace(
            go.Scatter(
                x=[mean_row[&#34;target&#34;]],
                y=[mean_row[y_col]],
                text=[&#34;Dataset mean&#34;],
                showlegend=False,
                mode=&#34;markers&#34;,
                name=&#34;Dataset mean&#34;,
                hoverinfo=&#34;text&#34;,
                marker=dict(symbol=&#34;x&#34;, size=9, color=colors.get(feat)),
            )
        )

        x_title = f&#34;Average {feat}&#34;
        if constraints_title:
            x_title += f&#34; (with {constraints_title})&#34;

        fig.update_layout(
            margin=dict(t=30, r=0, b=40),
            xaxis=dict(title=x_title),
            yaxis=dict(title=y_label, range=yrange),
        )
        set_fig_size(fig, size)
        return fig

    def _plot_explanation_2d(
        self,
        explanation,
        z_col,
        z_label,
        z_range=None,
        colorscale=None,
        size=None,
        constraints=None,
    ):
        if constraints is not None:
            explanation = explanation[~explanation[&#34;feature&#34;].isin(constraints)]

        fx, fy = explanation[&#34;feature&#34;].unique()
        x = np.sort(explanation[explanation[&#34;feature&#34;] == fx][&#34;target&#34;].unique())
        y = np.sort(explanation[explanation[&#34;feature&#34;] == fy][&#34;target&#34;].unique())

        z = explanation[explanation[&#34;feature&#34;] == fx][z_col].to_numpy()
        z_converged = explanation[explanation[&#34;feature&#34;] == fx][&#34;converged&#34;].to_numpy()
        z[np.logical_not(z_converged)] = math.nan
        z = z.reshape((len(x), len(y)))

        z_min = z_max = None
        if z_range is not None:
            z_min, z_max = z_range

        fig = go.Figure()
        fig.add_heatmap(
            x=x,
            y=y,
            z=z,
            zmin=z_min,
            zmax=z_max,
            colorscale=colorscale or &#34;Reds&#34;,
            colorbar=dict(title=z_label),
            hoverinfo=&#34;x+y+z&#34;,
        )

        mean_x = explanation.query(f&#39;feature == &#34;{fx}&#34; and tau == 0&#39;).iloc[0][&#34;target&#34;]
        mean_y = explanation.query(f&#39;feature == &#34;{fy}&#34; and tau == 0&#39;).iloc[0][&#34;target&#34;]
        fig.add_scatter(
            x=[mean_x],
            y=[mean_y],
            text=[&#34;Dataset mean&#34;],
            showlegend=False,
            mode=&#34;markers&#34;,
            name=&#34;Dataset mean&#34;,
            hoverinfo=&#34;text&#34;,
            marker=dict(symbol=&#34;x&#34;, size=9, color=&#34;black&#34;),
        )

        title = None
        if constraints is not None:
            title = &#34;Constraints: &#34; + &#34;, &#34;.join(
                f&#34;{f}={v}&#34; for f, v in constraints.items()
            )

        fig.update_layout(
            margin=dict(t=30, b=40),
            xaxis=dict(title=f&#34;Average {fx}&#34;, mirror=True),
            yaxis=dict(title=f&#34;Average {fy}&#34;, mirror=True),
            title=title,
        )
        set_fig_size(fig, size)
        return fig

    def _plot_ranking(
        self, ranking, score_column, title, n_features=None, colors=None, size=None
    ):
        if n_features is None:
            n_features = len(ranking)
        ascending = n_features &gt;= 0
        ranking = ranking.sort_values(by=[score_column], ascending=ascending)
        n_features = abs(n_features)

        fig = go.Figure()
        fig.add_bar(
            x=ranking[score_column][-n_features:],
            y=ranking[&#34;feature&#34;][-n_features:],
            orientation=&#34;h&#34;,
            hoverinfo=&#34;x&#34;,
            marker=dict(color=colors),
        )
        fig.update_layout(
            margin=dict(b=0, t=40, r=10),
            xaxis=dict(title=title, range=[0, 1], side=&#34;top&#34;, fixedrange=True),
            yaxis=dict(fixedrange=True, automargin=True),
            modebar=dict(
                orientation=&#34;v&#34;,
                color=&#34;rgba(0, 0, 0, 0)&#34;,
                activecolor=&#34;rgba(0, 0, 0, 0)&#34;,
                bgcolor=&#34;rgba(0, 0, 0, 0)&#34;,
            ),
        )
        set_fig_size(fig, size, width=500, height=100 + 60 * n_features)
        return fig

    def plot_influence(
        self, X_test, y_pred, colors=None, yrange=None, size=None, constraints=None
    ):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colors (dict, optional): A dictionary that maps features to colors.
                Default is `None` and the colors are choosen automatically.
            yrange (list, optional): A two-item list `[low, high]`. Default is
                `None` and the range is based on the data.
            size (tuple, optional): An optional couple `(width, height)` in pixels.
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.

        Examples:
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
            ...     x0=&#34;blue&#34;,
            ...     x1=&#34;red&#34;,
            ... ))
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
        &#34;&#34;&#34;
        explanation = self.explain_influence(X_test, y_pred, constraints=constraints)
        labels = explanation[&#34;label&#34;].unique()

        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)

        return self._plot_explanation(
            explanation=explanation,
            y_col=&#34;influence&#34;,
            y_label=f&#34;Average {labels[0]}&#34;,
            colors=colors,
            yrange=yrange,
            size=size,
            constraints=constraints,
        )

    def plot_influence_2d(
        self, X_test, y_pred, z_range=None, colorscale=None, size=None, constraints=None
    ):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame): The dataset as a pandas dataframe
                with one column per feature. Must contain exactly two columns.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colorscale (str, optional): The colorscale used for the heatmap.
                See `plotly.graph_objs.Heatmap`. Default is `None`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.
            constraints (dict, optional): A dictionary `(feature, mean)` to fix
                the mean of certain features, i.e. to ask questions like &#34;How does
                the model behave for a mean age ranging from 20 to 60 when the
                education level is 10?&#34;

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        n_constraints = 0 if constraints is None else len(constraints)
        if len(X_test.columns) != (2 + n_constraints):
            raise ValueError(
                &#34;`X_test` must contain exactly two columns in addition to the constraints.&#34;
            )

        explanation = self.explain_influence(
            X_test, y_pred, link_variables=True, constraints=constraints
        )
        labels = explanation[&#34;label&#34;].unique()

        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)

        return self._plot_explanation_2d(
            explanation,
            z_col=&#34;influence&#34;,
            z_label=f&#34;Average {labels[0]}&#34;,
            z_range=z_range,
            colorscale=colorscale,
            size=size,
            constraints=constraints,
        )

    def plot_influence_ranking(
        self, X_test, y_pred, n_features=None, colors=None, size=None
    ):
        &#34;&#34;&#34;Plot the ranking of the features based on their influence.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        ranking = self.rank_by_influence(X_test=X_test, y_pred=y_pred)
        return self._plot_ranking(
            ranking=ranking,
            score_column=&#34;importance&#34;,
            title=&#34;Importance&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )

    def plot_performance(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        colors=None,
        yrange=None,
        size=None,
        constraints=None,
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            yrange (list, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): See `CacheExplainer.plot_influence()`.
            constraints (dict, optional): See `CacheExplainer.plot_influence()`.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test,
            y_test=y_test,
            y_pred=y_pred,
            metric=metric,
            constraints=constraints,
        )
        if yrange is None and explanation[metric_name].between(0, 1).all():
            yrange = [0, 1]

        #  The performance is the same for all labels, we remove duplicates
        label = explanation[&#34;label&#34;].unique()[0]
        explanation = explanation[explanation[&#34;label&#34;] == label]

        return self._plot_explanation(
            explanation=explanation,
            y_col=metric_name,
            y_label=f&#34;Average {metric_name}&#34;,
            colors=colors,
            yrange=yrange,
            size=size,
            constraints=constraints,
        )

    def plot_performance_2d(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        z_range=None,
        colorscale=None,
        size=None,
        constraints=None,
    ):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame): The dataset as a pandas dataframe
                with one column per feature. Must contain exactly two columns.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colorscale (str, optional): The colorscale used for the heatmap.
                See `plotly.graph_objs.Heatmap`. Default is `None`.
            size (tuple, optional): See `CacheExplainer.plot_influence()`.
            constraints (dict, optional): See `CacheExplainer.plot_influence()`.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        n_constraints = 0 if constraints is None else len(constraints)
        if len(X_test.columns) != (2 + n_constraints):
            raise ValueError(
                &#34;`X_test` must contain exactly two columns in addition to the constraints.&#34;
            )

        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test,
            y_test=y_test,
            y_pred=y_pred,
            metric=metric,
            link_variables=True,
            constraints=constraints,
        )
        if z_range is None and explanation[metric_name].between(0, 1).all():
            z_range = [0, 1]

        return self._plot_explanation_2d(
            explanation,
            z_col=metric_name,
            z_label=f&#34;Average {metric_name}&#34;,
            z_range=z_range,
            colorscale=colorscale,
            size=size,
            constraints=constraints,
        )

    def plot_performance_ranking(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        criterion,
        n_features=None,
        colors=None,
        size=None,
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            criterion (str): Either &#34;min&#34; or &#34;max&#34; to determine whether, for a
                given feature, we keep the worst or the best performance for all
                the values taken by the mean. See `CacheExplainer.rank_by_performance()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence_ranking()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        ranking = self.rank_by_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        return self._plot_ranking(
            ranking=ranking,
            score_column=criterion,
            title=f&#34;{criterion} {metric_name}&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )

    def plot_weight_distribution(
        self, feature_values, proportion, threshold=None, color=None, size=None
    ):
        &#34;&#34;&#34;Plot, for every target mean, how many individuals capture
        `proportion` of the total weight. For instance, we could see that for
        a target mean of 25 year-old (if the feature is the age), 50% of the
        weight is distributed to 14% of the individuals &#34;only&#34;. If &#34;few&#34; individuals
        get &#34;a lot of&#34; weight, it means that the stressed distribution is &#34;quite&#34;
        different from the original one and that the results are not reliable. Defining
        a relevant threshold is an open question.

        Parameters:
            feature_values (pd.Series): See `ethik.base_explainer.BaseExplainer.compute_weights()`.
            proportion (float): The proportion of weight to check, between 0 and 1.
            threshold (float, optional): An optional threshold to display on the
                plot. Must be between 0 and 1.
            colors (list, optional): An optional list of colors for all targets.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        #  If `feature_values.name` is `None`, converting it to a dataframe
        # will create a column `0`, which will not be matched by the query below
        # since its &#34;feature&#34; column will be `None` and not `0`.
        if feature_values.name is None:
            feature_values = feature_values.rename(&#34;feature&#34;)

        X_test = pd.DataFrame(to_pandas(feature_values))
        targets = self._build_targets(X_test=X_test)[feature_values.name]

        return super().plot_weight_distribution(
            feature_values=feature_values,
            targets=targets,
            proportion=proportion,
            threshold=threshold,
            color=color,
            size=size,
        )</code></pre>
      </details>



          <h3>Ancestors</h3>
          <ul class="hlist">
              <li><a title="ethik.base_explainer.BaseExplainer" href="base_explainer.html#ethik.base_explainer.BaseExplainer">BaseExplainer</a></li>
          </ul>

          <h3>Subclasses</h3>
          <ul class="hlist">
              <li><a title="ethik.classification_explainer.ClassificationExplainer" href="classification_explainer.html#ethik.classification_explainer.ClassificationExplainer">ClassificationExplainer</a></li>
              <li><a title="ethik.regression_explainer.RegressionExplainer" href="regression_explainer.html#ethik.regression_explainer.RegressionExplainer">RegressionExplainer</a></li>
              <li><a title="ethik.image_classification_explainer.ImageClassificationExplainer" href="image_classification_explainer.html#ethik.image_classification_explainer.ImageClassificationExplainer">ImageClassificationExplainer</a></li>
          </ul>
          <h3>Methods</h3>
          <dl>
              
    <dt id="ethik.cache_explainer.CacheExplainer.explain_influence"><code class="name flex">
        
        <span>def <span class="ident">explain_influence</span></span>(<span>self, X_test, y_pred, link_variables=False, constraints=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Compute the influence of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. For binary classification and regression,
<code>pd.Series</code> is expected. For multi-label classification, a
pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
<dt><strong><code>link_variables</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to make a multidimensional
explanation or not. Default is <code>False</code>, which means that all
the features in <code>X_test</code> are considered independently (so the
correlation are not taken into account).</dd>
<dt><strong><code>constraints</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dictionary <code>(feature, mean)</code> to fix
the mean of certain features, i.e. to ask questions like "How does
the model behave for a mean age ranging from 20 to 60 when the
education level is 10?"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>A dataframe with columns <code>(feature, tau, target, ksi, label,
influence, influence_low, influence_high)</code>. If <code>explainer.n_samples</code> is <code>1</code>,
no confidence interval is computed and <code>influence = influence_low = influence_high</code>.
The value of <code>label</code> is not important for regression.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>See more examples in <code>notebooks</code>.</p>
<p>Binary classification:</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model.predict(X_test)
&gt;&gt;&gt; y_pred
[0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
&gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
&gt;&gt;&gt; y_pred = pd.Series(y_pred, name="is_reliable")
&gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
</code></pre>
<p>Regression is similar to binary classification:</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model.predict(X_test)
&gt;&gt;&gt; y_pred
[22, 24, 19]
&gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
&gt;&gt;&gt; y_pred = pd.Series(y_pred, name="price")
&gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
</code></pre>
<p>For multi-label classification, we need a dataframe to store predictions:</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model.predict(X_test)
&gt;&gt;&gt; y_pred.columns
["class0", "class1", "class2"]
&gt;&gt;&gt; y_pred.iloc[0]
[0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
&gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
</code></pre></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def explain_influence(self, X_test, y_pred, link_variables=False, constraints=None):
    &#34;&#34;&#34;Compute the influence of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. For binary classification and regression,
            `pd.Series` is expected. For multi-label classification, a
            pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).
        link_variables (bool, optional): Whether to make a multidimensional
            explanation or not. Default is `False`, which means that all
            the features in `X_test` are considered independently (so the
            correlation are not taken into account).
        constraints (dict, optional): A dictionary `(feature, mean)` to fix
            the mean of certain features, i.e. to ask questions like &#34;How does
            the model behave for a mean age ranging from 20 to 60 when the
            education level is 10?&#34;

    Returns:
        pd.DataFrame:
            A dataframe with columns `(feature, tau, target, ksi, label,
            influence, influence_low, influence_high)`. If `explainer.n_samples` is `1`,
            no confidence interval is computed and `influence = influence_low = influence_high`.
            The value of `label` is not important for regression.

    Examples:
        See more examples in `notebooks`.

        Binary classification:

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model.predict(X_test)
        &gt;&gt;&gt; y_pred
        [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
        &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
        &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
        &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

        Regression is similar to binary classification:

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model.predict(X_test)
        &gt;&gt;&gt; y_pred
        [22, 24, 19]
        &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
        &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
        &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

        For multi-label classification, we need a dataframe to store predictions:

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model.predict(X_test)
        &gt;&gt;&gt; y_pred.columns
        [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
        &gt;&gt;&gt; y_pred.iloc[0]
        [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
        &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
    &#34;&#34;&#34;
    return self._explain_with_cache(
        X_test,
        y_pred,
        explain=functools.partial(
            self._explain_influence, X_test=X_test, y_pred=y_pred
        ),
        link_variables=link_variables,
        constraints=constraints,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.explain_performance"><code class="name flex">
        
        <span>def <span class="ident">explain_performance</span></span>(<span>self, X_test, y_test, y_pred, metric, link_variables=False, constraints=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Compute the change in model's performance for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The true values
for the samples in <code>X_test</code>. For binary classification and regression,
a <code>pd.Series</code> is expected. For multi-label classification,
a pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. The format is the same as <code>y_test</code>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>A scikit-learn-like metric
<code>f(y_true, y_pred, sample_weight=None)</code>. The metric must be able
to handle the <code>y</code> data. For instance, for <code>sklearn.metrics.accuracy_score()</code>,
"the set of labels predicted for a sample must exactly match the
corresponding set of labels in <code>y_true</code>".</dd>
<dt><strong><code>link_variables</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to make a multidimensional
explanation or not. Default is <code>False</code>, which means that all
the features in <code>X_test</code> are considered independently (so the
correlation are not taken into account).</dd>
<dt><strong><code>constraints</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dictionary <code>(feature, mean)</code> to fix
the mean of certain features, i.e. to ask questions like "How does
the model behave for a mean age ranging from 20 to 60 when the
education level is 10?"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>A dataframe with columns <code>(feature, tau, target, ksi, label,
influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)</code>.
If <code>explainer.n_samples</code> is <code>1</code>, no confidence interval is computed
and <code>&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;</code>.
The value of <code>label</code> is not important for regression.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>See examples in <code>notebooks</code>.</p></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def explain_performance(
    self, X_test, y_test, y_pred, metric, link_variables=False, constraints=None
):
    &#34;&#34;&#34;Compute the change in model&#39;s performance for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_test (pd.DataFrame or pd.Series): The true values
            for the samples in `X_test`. For binary classification and regression,
            a `pd.Series` is expected. For multi-label classification,
            a pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. The format is the same as `y_test`.
        metric (callable): A scikit-learn-like metric
            `f(y_true, y_pred, sample_weight=None)`. The metric must be able
            to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
            &#34;the set of labels predicted for a sample must exactly match the
            corresponding set of labels in `y_true`&#34;.
        link_variables (bool, optional): Whether to make a multidimensional
            explanation or not. Default is `False`, which means that all
            the features in `X_test` are considered independently (so the
            correlation are not taken into account).
        constraints (dict, optional): A dictionary `(feature, mean)` to fix
            the mean of certain features, i.e. to ask questions like &#34;How does
            the model behave for a mean age ranging from 20 to 60 when the
            education level is 10?&#34;

    Returns:
        pd.DataFrame:
            A dataframe with columns `(feature, tau, target, ksi, label,
            influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)`.
            If `explainer.n_samples` is `1`, no confidence interval is computed
            and `&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;`.
            The value of `label` is not important for regression.

    Examples:
        See examples in `notebooks`.
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)
    self.metric_names.add(metric_name)
    return self._explain_with_cache(
        X_test,
        y_pred,
        explain=functools.partial(
            self._explain_performance,
            X_test=X_test,
            y_pred=y_pred,
            y_test=y_test,
            metric=metric,
        ),
        link_variables=link_variables,
        constraints=constraints,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_influence"><code class="name flex">
        
        <span>def <span class="ident">plot_influence</span></span>(<span>self, X_test, y_pred, colors=None, yrange=None, size=None, constraints=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the influence of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dictionary that maps features to colors.
Default is <code>None</code> and the colors are choosen automatically.</dd>
<dt><strong><code>yrange</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A two-item list <code>[low, high]</code>. Default is
<code>None</code> and the range is based on the data.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
<dt><strong><code>constraints</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dictionary <code>(feature, mean)</code> to fix
the mean of certain features, i.e. to ask questions like "How does
the model behave for a mean age ranging from 20 to 60 when the
education level is 10?"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
&gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
...     x0="blue",
...     x1="red",
... ))
&gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
</code></pre></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_influence(
    self, X_test, y_pred, colors=None, yrange=None, size=None, constraints=None
):
    &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame): See `CacheExplainer.explain_influence()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
        colors (dict, optional): A dictionary that maps features to colors.
            Default is `None` and the colors are choosen automatically.
        yrange (list, optional): A two-item list `[low, high]`. Default is
            `None` and the range is based on the data.
        size (tuple, optional): An optional couple `(width, height)` in pixels.
        constraints (dict, optional): A dictionary `(feature, mean)` to fix
            the mean of certain features, i.e. to ask questions like &#34;How does
            the model behave for a mean age ranging from 20 to 60 when the
            education level is 10?&#34;

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.

    Examples:
        &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
        &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
        ...     x0=&#34;blue&#34;,
        ...     x1=&#34;red&#34;,
        ... ))
        &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
    &#34;&#34;&#34;
    explanation = self.explain_influence(X_test, y_pred, constraints=constraints)
    labels = explanation[&#34;label&#34;].unique()

    if len(labels) &gt; 1:
        raise ValueError(&#34;Cannot plot multiple labels&#34;)

    return self._plot_explanation(
        explanation=explanation,
        y_col=&#34;influence&#34;,
        y_label=f&#34;Average {labels[0]}&#34;,
        colors=colors,
        yrange=yrange,
        size=size,
        constraints=constraints,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_influence_2d"><code class="name flex">
        
        <span>def <span class="ident">plot_influence_2d</span></span>(<span>self, X_test, y_pred, z_range=None, colorscale=None, size=None, constraints=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the influence of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature. Must contain exactly two columns.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>colorscale</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The colorscale used for the heatmap.
See <code>plotly.graph_objs.Heatmap</code>. Default is <code>None</code>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
<dt><strong><code>constraints</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dictionary <code>(feature, mean)</code> to fix
the mean of certain features, i.e. to ask questions like "How does
the model behave for a mean age ranging from 20 to 60 when the
education level is 10?"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_influence_2d(
    self, X_test, y_pred, z_range=None, colorscale=None, size=None, constraints=None
):
    &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame): The dataset as a pandas dataframe
            with one column per feature. Must contain exactly two columns.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
        colorscale (str, optional): The colorscale used for the heatmap.
            See `plotly.graph_objs.Heatmap`. Default is `None`.
        size (tuple, optional): An optional couple `(width, height)` in pixels.
        constraints (dict, optional): A dictionary `(feature, mean)` to fix
            the mean of certain features, i.e. to ask questions like &#34;How does
            the model behave for a mean age ranging from 20 to 60 when the
            education level is 10?&#34;

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    n_constraints = 0 if constraints is None else len(constraints)
    if len(X_test.columns) != (2 + n_constraints):
        raise ValueError(
            &#34;`X_test` must contain exactly two columns in addition to the constraints.&#34;
        )

    explanation = self.explain_influence(
        X_test, y_pred, link_variables=True, constraints=constraints
    )
    labels = explanation[&#34;label&#34;].unique()

    if len(labels) &gt; 1:
        raise ValueError(&#34;Cannot plot multiple labels&#34;)

    return self._plot_explanation_2d(
        explanation,
        z_col=&#34;influence&#34;,
        z_label=f&#34;Average {labels[0]}&#34;,
        z_range=z_range,
        colorscale=colorscale,
        size=size,
        constraints=constraints,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_influence_ranking"><code class="name flex">
        
        <span>def <span class="ident">plot_influence_ranking</span></span>(<span>self, X_test, y_pred, n_features=None, colors=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the ranking of the features based on their influence.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>n_features</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of features to plot. With the
default (<code>None</code>), all of them are shown. For a positive value,
we keep the <code>n_features</code> first features (the most impactful). For
a negative value, we keep the <code>n_features</code> last features.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_influence_ranking(
    self, X_test, y_pred, n_features=None, colors=None, size=None
):
    &#34;&#34;&#34;Plot the ranking of the features based on their influence.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
        n_features (int, optional): The number of features to plot. With the
            default (`None`), all of them are shown. For a positive value,
            we keep the `n_features` first features (the most impactful). For
            a negative value, we keep the `n_features` last features.
        colors (dict, optional): See `CacheExplainer.plot_influence()`.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    ranking = self.rank_by_influence(X_test=X_test, y_pred=y_pred)
    return self._plot_ranking(
        ranking=ranking,
        score_column=&#34;importance&#34;,
        title=&#34;Importance&#34;,
        n_features=n_features,
        colors=colors,
        size=size,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_performance"><code class="name flex">
        
        <span>def <span class="ident">plot_performance</span></span>(<span>self, X_test, y_test, y_pred, metric, colors=None, yrange=None, size=None, constraints=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the performance of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>yrange</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>constraints</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_performance(
    self,
    X_test,
    y_test,
    y_pred,
    metric,
    colors=None,
    yrange=None,
    size=None,
    constraints=None,
):
    &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
        y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        metric (callable): See `CacheExplainer.explain_performance()`.
        colors (dict, optional): See `CacheExplainer.plot_influence()`.
        yrange (list, optional): See `CacheExplainer.plot_influence()`.
        size (tuple, optional): See `CacheExplainer.plot_influence()`.
        constraints (dict, optional): See `CacheExplainer.plot_influence()`.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)
    explanation = self.explain_performance(
        X_test=X_test,
        y_test=y_test,
        y_pred=y_pred,
        metric=metric,
        constraints=constraints,
    )
    if yrange is None and explanation[metric_name].between(0, 1).all():
        yrange = [0, 1]

    #  The performance is the same for all labels, we remove duplicates
    label = explanation[&#34;label&#34;].unique()[0]
    explanation = explanation[explanation[&#34;label&#34;] == label]

    return self._plot_explanation(
        explanation=explanation,
        y_col=metric_name,
        y_label=f&#34;Average {metric_name}&#34;,
        colors=colors,
        yrange=yrange,
        size=size,
        constraints=constraints,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_performance_2d"><code class="name flex">
        
        <span>def <span class="ident">plot_performance_2d</span></span>(<span>self, X_test, y_test, y_pred, metric, z_range=None, colorscale=None, size=None, constraints=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the influence of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature. Must contain exactly two columns.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>colorscale</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The colorscale used for the heatmap.
See <code>plotly.graph_objs.Heatmap</code>. Default is <code>None</code>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>constraints</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_performance_2d(
    self,
    X_test,
    y_test,
    y_pred,
    metric,
    z_range=None,
    colorscale=None,
    size=None,
    constraints=None,
):
    &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame): The dataset as a pandas dataframe
            with one column per feature. Must contain exactly two columns.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
        colorscale (str, optional): The colorscale used for the heatmap.
            See `plotly.graph_objs.Heatmap`. Default is `None`.
        size (tuple, optional): See `CacheExplainer.plot_influence()`.
        constraints (dict, optional): See `CacheExplainer.plot_influence()`.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    n_constraints = 0 if constraints is None else len(constraints)
    if len(X_test.columns) != (2 + n_constraints):
        raise ValueError(
            &#34;`X_test` must contain exactly two columns in addition to the constraints.&#34;
        )

    metric_name = self.get_metric_name(metric)
    explanation = self.explain_performance(
        X_test=X_test,
        y_test=y_test,
        y_pred=y_pred,
        metric=metric,
        link_variables=True,
        constraints=constraints,
    )
    if z_range is None and explanation[metric_name].between(0, 1).all():
        z_range = [0, 1]

    return self._plot_explanation_2d(
        explanation,
        z_col=metric_name,
        z_label=f&#34;Average {metric_name}&#34;,
        z_range=z_range,
        colorscale=colorscale,
        size=size,
        constraints=constraints,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_performance_ranking"><code class="name flex">
        
        <span>def <span class="ident">plot_performance_ranking</span></span>(<span>self, X_test, y_test, y_pred, metric, criterion, n_features=None, colors=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the performance of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>str</code></dt>
<dd>Either "min" or "max" to determine whether, for a
given feature, we keep the worst or the best performance for all
the values taken by the mean. See <a title="ethik.cache_explainer.CacheExplainer.rank_by_performance" href="#ethik.cache_explainer.CacheExplainer.rank_by_performance"><code>CacheExplainer.rank_by_performance()</code></a>.</dd>
<dt><strong><code>n_features</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of features to plot. With the
default (<code>None</code>), all of them are shown. For a positive value,
we keep the <code>n_features</code> first features (the most impactful). For
a negative value, we keep the <code>n_features</code> last features.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence_ranking" href="#ethik.cache_explainer.CacheExplainer.plot_influence_ranking"><code>CacheExplainer.plot_influence_ranking()</code></a>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_performance_ranking(
    self,
    X_test,
    y_test,
    y_pred,
    metric,
    criterion,
    n_features=None,
    colors=None,
    size=None,
):
    &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
        y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        metric (callable): See `CacheExplainer.explain_performance()`.
        criterion (str): Either &#34;min&#34; or &#34;max&#34; to determine whether, for a
            given feature, we keep the worst or the best performance for all
            the values taken by the mean. See `CacheExplainer.rank_by_performance()`.
        n_features (int, optional): The number of features to plot. With the
            default (`None`), all of them are shown. For a positive value,
            we keep the `n_features` first features (the most impactful). For
            a negative value, we keep the `n_features` last features.
        colors (dict, optional): See `CacheExplainer.plot_influence_ranking()`.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)
    ranking = self.rank_by_performance(
        X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
    )
    return self._plot_ranking(
        ranking=ranking,
        score_column=criterion,
        title=f&#34;{criterion} {metric_name}&#34;,
        n_features=n_features,
        colors=colors,
        size=size,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_weight_distribution"><code class="name flex">
        
        <span>def <span class="ident">plot_weight_distribution</span></span>(<span>self, feature_values, proportion, threshold=None, color=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot, for every target mean, how many individuals capture
<code>proportion</code> of the total weight. For instance, we could see that for
a target mean of 25 year-old (if the feature is the age), 50% of the
weight is distributed to 14% of the individuals "only". If "few" individuals
get "a lot of" weight, it means that the stressed distribution is "quite"
different from the original one and that the results are not reliable. Defining
a relevant threshold is an open question.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>feature_values</code></strong> :&ensp;<code>pd.Series</code></dt>
<dd>See <a title="ethik.base_explainer.BaseExplainer.compute_weights" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compute_weights"><code>BaseExplainer.compute_weights()</code></a>.</dd>
<dt><strong><code>proportion</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of weight to check, between 0 and 1.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>An optional threshold to display on the
plot. Must be between 0 and 1.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>An optional list of colors for all targets.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_weight_distribution(
    self, feature_values, proportion, threshold=None, color=None, size=None
):
    &#34;&#34;&#34;Plot, for every target mean, how many individuals capture
    `proportion` of the total weight. For instance, we could see that for
    a target mean of 25 year-old (if the feature is the age), 50% of the
    weight is distributed to 14% of the individuals &#34;only&#34;. If &#34;few&#34; individuals
    get &#34;a lot of&#34; weight, it means that the stressed distribution is &#34;quite&#34;
    different from the original one and that the results are not reliable. Defining
    a relevant threshold is an open question.

    Parameters:
        feature_values (pd.Series): See `ethik.base_explainer.BaseExplainer.compute_weights()`.
        proportion (float): The proportion of weight to check, between 0 and 1.
        threshold (float, optional): An optional threshold to display on the
            plot. Must be between 0 and 1.
        colors (list, optional): An optional list of colors for all targets.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    #  If `feature_values.name` is `None`, converting it to a dataframe
    # will create a column `0`, which will not be matched by the query below
    # since its &#34;feature&#34; column will be `None` and not `0`.
    if feature_values.name is None:
        feature_values = feature_values.rename(&#34;feature&#34;)

    X_test = pd.DataFrame(to_pandas(feature_values))
    targets = self._build_targets(X_test=X_test)[feature_values.name]

    return super().plot_weight_distribution(
        feature_values=feature_values,
        targets=targets,
        proportion=proportion,
        threshold=threshold,
        color=color,
        size=size,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.rank_by_influence"><code class="name flex">
        
        <span>def <span class="ident">rank_by_influence</span></span>(<span>self, X_test, y_pred)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Returns a pandas DataFrame containing the importance of each feature
per label.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. For binary classification and regression,
a <code>pd.Series</code> is expected. For multi-label classification,
a pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>
<p>A dataframe with columns <code>(label, feature, importance)</code>. The row
<code>(setosa, petal length (cm), 0.282507)</code> means that the feature
<code>petal length</code> of the Iris dataset has an importance of about
30% in the prediction of the class <code>setosa</code>.</p>
<p>The importance is a real number between 0 and 1. Intuitively,
if the model influence for the feature <code>X</code> is a flat curve (the average
model prediction is not impacted by the mean of <code>X</code>) then we
can conclude that <code>X</code> has no importance for predictions. This
flat curve is the baseline and satisfies <span><span class="MathJax_Preview">y = influence_{\tau(0)}</span><script type="math/tex">y = influence_{\tau(0)}</script></span>.
To compute the importance of a feature, we look at the average
distance of the influence curve to this baseline:</p>
<p><span><span class="MathJax_Preview">
I(X) = \frac{1}{n_\tau} \sum_{i=1}^{n_\tau} \mid influence_{\tau(i)}(X) - influence_{\tau(0)}(X) \mid
</span><script type="math/tex; mode=display">
I(X) = \frac{1}{n_\tau} \sum_{i=1}^{n_\tau} \mid influence_{\tau(i)}(X) - influence_{\tau(0)}(X) \mid
</script></span></p>
<p>The influence curve is first normalized so that the importance is
between 0 and 1 (which may not be the case originally for regression
problems). To normalize, we get the minimum and maximum influences
<em>across all features and all classes</em> and then compute
<code>normalized = (influence - min) / (max - min)</code>.</p>
<p>For regression problems, there's one label only and its name
doesn't matter (it's just to have a consistent output).</p>
</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def rank_by_influence(self, X_test, y_pred):
    &#34;&#34;&#34;Returns a pandas DataFrame containing the importance of each feature
    per label.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. For binary classification and regression,
            a `pd.Series` is expected. For multi-label classification,
            a pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).

    Returns:
        pd.DataFrame:
            A dataframe with columns `(label, feature, importance)`. The row
            `(setosa, petal length (cm), 0.282507)` means that the feature
            `petal length` of the Iris dataset has an importance of about
            30% in the prediction of the class `setosa`.

            The importance is a real number between 0 and 1. Intuitively,
            if the model influence for the feature `X` is a flat curve (the average
            model prediction is not impacted by the mean of `X`) then we
            can conclude that `X` has no importance for predictions. This
            flat curve is the baseline and satisfies \\(y = influence_{\\tau(0)}\\).
            To compute the importance of a feature, we look at the average
            distance of the influence curve to this baseline:

            $$
            I(X) = \\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mid influence_{\\tau(i)}(X) - influence_{\\tau(0)}(X) \\mid
            $$

            The influence curve is first normalized so that the importance is
            between 0 and 1 (which may not be the case originally for regression
            problems). To normalize, we get the minimum and maximum influences
            *across all features and all classes* and then compute
            `normalized = (influence - min) / (max - min)`.

            For regression problems, there&#39;s one label only and its name
            doesn&#39;t matter (it&#39;s just to have a consistent output).
    &#34;&#34;&#34;

    def get_importance(group, min_influence, max_influence):
        &#34;&#34;&#34;Computes the average absolute difference in influence changes per tau increase.&#34;&#34;&#34;
        #  Normalize influence to get an importance between 0 and 1
        # influence can be outside [0, 1] for regression
        influence = group[&#34;influence&#34;]
        group[&#34;influence&#34;] = (influence - min_influence) / (
            max_influence - min_influence
        )
        baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;influence&#34;]
        return (group[&#34;influence&#34;] - baseline).abs().mean()

    explanation = self.explain_influence(X_test=X_test, y_pred=y_pred)
    min_influence = explanation[&#34;influence&#34;].min()
    max_influence = explanation[&#34;influence&#34;].max()

    return (
        explanation.groupby([&#34;label&#34;, &#34;feature&#34;])
        .apply(
            functools.partial(
                get_importance,
                min_influence=min_influence,
                max_influence=max_influence,
            )
        )
        .to_frame(&#34;importance&#34;)
        .reset_index()
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.rank_by_performance"><code class="name flex">
        
        <span>def <span class="ident">rank_by_performance</span></span>(<span>self, X_test, y_test, y_pred, metric)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Returns a pandas DataFrame containing
per label.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The true output
for the samples in <code>X_test</code>. For binary classification and regression,
a <code>pd.Series</code> is expected. For multi-label classification,
a pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. The format is the same as <code>y_test</code>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>A scikit-learn-like metric
<code>f(y_true, y_pred, sample_weight=None)</code>. The metric must be able
to handle the <code>y</code> data. For instance, for <code>sklearn.metrics.accuracy_score()</code>,
"the set of labels predicted for a sample must exactly match the
corresponding set of labels in <code>y_true</code>".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>
<p>A dataframe with columns <code>(feature, min, max)</code>. The row
<code>(age, 0.862010, 0.996360)</code> means that the score measured by the
given metric (e.g. <code>sklearn.metrics.accuracy_score</code>) stays bewteen
86.2% and 99.6% on average when we make the mean age change. With
such information, we can find the features for which the model
performs the worst or the best.</p>
<p>For regression problems, there's one label only and its name
doesn't matter (it's just to have a consistent output).</p>
</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def rank_by_performance(self, X_test, y_test, y_pred, metric):
    &#34;&#34;&#34;Returns a pandas DataFrame containing
    per label.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_test (pd.DataFrame or pd.Series): The true output
            for the samples in `X_test`. For binary classification and regression,
            a `pd.Series` is expected. For multi-label classification,
            a pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. The format is the same as `y_test`.
        metric (callable): A scikit-learn-like metric
            `f(y_true, y_pred, sample_weight=None)`. The metric must be able
            to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
            &#34;the set of labels predicted for a sample must exactly match the
            corresponding set of labels in `y_true`&#34;.

    Returns:
        pd.DataFrame:
            A dataframe with columns `(feature, min, max)`. The row
            `(age, 0.862010, 0.996360)` means that the score measured by the
            given metric (e.g. `sklearn.metrics.accuracy_score`) stays bewteen
            86.2% and 99.6% on average when we make the mean age change. With
            such information, we can find the features for which the model
            performs the worst or the best.

            For regression problems, there&#39;s one label only and its name
            doesn&#39;t matter (it&#39;s just to have a consistent output).
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)

    def get_aggregates(df):
        return pd.Series(
            [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
        )

    return (
        self.explain_performance(X_test, y_test, y_pred, metric)
        .groupby(&#34;feature&#34;)
        .apply(get_aggregates)
        .reset_index()
    )</code></pre>
      </details>

</dd>
  
          </dl>

          
              <h3>Inherited members</h3>
              <ul class="hlist">
                  <li><code><b><a title="ethik.base_explainer.BaseExplainer" href="base_explainer.html#ethik.base_explainer.BaseExplainer">BaseExplainer</a></b></code>:
                      <ul class="hlist">
                              <li><code><a title="ethik.base_explainer.BaseExplainer.CAT_COL_SEP" href="base_explainer.html#ethik.base_explainer.BaseExplainer.CAT_COL_SEP">CAT_COL_SEP</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compare_influence" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compare_influence">compare_influence</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compare_performance" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compare_performance">compare_performance</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compute_distributions" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compute_distributions">compute_distributions</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compute_weights" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compute_weights">compute_weights</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.get_metric_name" href="base_explainer.html#ethik.base_explainer.BaseExplainer.get_metric_name">get_metric_name</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_cumulative_weights" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_cumulative_weights">plot_cumulative_weights</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_distributions" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_distributions">plot_distributions</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_influence_comparison" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_influence_comparison">plot_influence_comparison</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_performance_comparison" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_performance_comparison">plot_performance_comparison</a></code></li>
                      </ul>

                  </li>
              </ul>

      </dd>
    </dl>
  </section>

  </article>
  
  
  <nav id="sidebar">

    

    <h1>Index</h1>
    <div class="toc">
<ul></ul>
</div>
    <ul id="index">
    <li><h3>Super-module</h3>
      <ul>
        <li><code><a title="ethik" href="index.html">ethik</a></code></li>
      </ul>
    </li>




    <li><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li>
        <h4><code><a title="ethik.cache_explainer.CacheExplainer" href="#ethik.cache_explainer.CacheExplainer">CacheExplainer</a></code></h4>
        
          
  
  <ul class="">
    <li><code><a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence">explain_influence</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance">explain_performance</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence">plot_influence</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_influence_2d" href="#ethik.cache_explainer.CacheExplainer.plot_influence_2d">plot_influence_2d</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_influence_ranking" href="#ethik.cache_explainer.CacheExplainer.plot_influence_ranking">plot_influence_ranking</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_performance" href="#ethik.cache_explainer.CacheExplainer.plot_performance">plot_performance</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_performance_2d" href="#ethik.cache_explainer.CacheExplainer.plot_performance_2d">plot_performance_2d</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_performance_ranking" href="#ethik.cache_explainer.CacheExplainer.plot_performance_ranking">plot_performance_ranking</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_weight_distribution" href="#ethik.cache_explainer.CacheExplainer.plot_weight_distribution">plot_weight_distribution</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.rank_by_influence" href="#ethik.cache_explainer.CacheExplainer.rank_by_influence">rank_by_influence</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.rank_by_performance" href="#ethik.cache_explainer.CacheExplainer.rank_by_performance">rank_by_performance</a></code></li>
  </ul>

        </li>
      </ul>
    </li>

    </ul>
  </nav>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad()</script>