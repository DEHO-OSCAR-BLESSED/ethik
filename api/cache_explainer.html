---
title: "API"
description: ""

head: |
  <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  
  <style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
  <style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
  <style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
  <style media="screen and (min-width: 700px)">
    main > .content-wrapper {
      display: flex;
      flex-direction: row-reverse;
      justify-content: flex-end;
    }
  </style>

js: |

  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script>
---

  <article id="content">
    
  

  

  <header>
  <h1 class="title">Module <code>ethik.cache_explainer</code></h1>
  </header>

  <section id="section-intro">
  
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">import collections
import functools
import itertools
import warnings

import pandas as pd
import plotly.graph_objs as go

from .base_explainer import BaseExplainer
from .utils import decimal_range, to_pandas
from .warnings import ConstantWarning

__all__ = [&#34;CacheExplainer&#34;]


class CacheExplainer(BaseExplainer):
    &#34;&#34;&#34;Explains the influence of features on model predictions and performance.

    Parameters:
        alpha (float): A `float` between `0` and `0.5` which indicates by how close the `CacheExplainer`
            should look at extreme values of a distribution. The closer to zero, the more so
            extreme values will be accounted for. The default is `0.05` which means that all values
            beyond the 5th and 95th quantiles are ignored.
        n_taus (int): The number of τ values to consider. The results will be more fine-grained the
            higher this value is. However the computation time increases linearly with `n_taus`.
            The default is `41` and corresponds to each τ being separated by it&#39;s neighbors by
            `0.05`.
        n_samples (int): The number of samples to use for the confidence interval.
            If `1`, the default, no confidence interval is computed.
        sample_frac (float): The proportion of lines in the dataset sampled to
            generate the samples for the confidence interval. If `n_samples` is
            `1`, no confidence interval is computed and the whole dataset is used.
            Default is `0.8`.
        conf_level (float): A `float` between `0` and `0.5` which indicates the
            quantile used for the confidence interval. Default is `0.05`, which
            means that the confidence interval contains the data between the 5th
            and 95th quantiles.
        max_iterations (int): The maximum number of iterations used when applying the Newton step
            of the optimization procedure. Default is `5`.
        tol (float): The bottom threshold for the gradient of the optimization
            procedure. When reached, the procedure stops. Otherwise, a warning
            is raised about the fact that the optimization did not converge.
            Default is `1e-4`.
        n_jobs (int): The number of jobs to use for parallel computations. See
            `joblib.Parallel()`. Default is `-1`.
        memoize (bool): Indicates whether or not memoization should be used or not. If `True`, then
            intermediate results will be stored in order to avoid recomputing results that can be
            reused by successively called methods. For example, if you call `plot_influence` followed by
            `plot_influence_ranking` and `memoize` is `True`, then the intermediate results required by
            `plot_influence` will be reused for `plot_influence_ranking`. Memoization is turned off by
            default because it can lead to unexpected behavior depending on your usage.
        verbose (bool): Whether or not to show progress bars during
            computations. Default is `True`.
    &#34;&#34;&#34;

    def __init__(
        self,
        alpha=0.05,
        n_taus=41,
        n_samples=1,
        sample_frac=0.8,
        conf_level=0.05,
        max_iterations=15,
        tol=1e-4,
        n_jobs=1,  # Parallelism is only worth it if the dataset is &#34;large&#34;
        memoize=False,
        verbose=True,
    ):
        super().__init__(
            alpha=alpha,
            n_samples=n_samples,
            sample_frac=sample_frac,
            conf_level=conf_level,
            max_iterations=max_iterations,
            tol=tol,
            n_jobs=n_jobs,
            verbose=verbose,
        )

        if not n_taus &gt; 0:
            raise ValueError(
                f&#34;n_taus must be a strictly positive integer, got {n_taus}&#34;
            )

        self.n_taus = n_taus
        self.memoize = memoize
        self.metric_names = set()
        self._reset_info()

    def _reset_info(self):
        &#34;&#34;&#34;Resets the info dataframe (for when memoization is turned off).&#34;&#34;&#34;
        self.info = pd.DataFrame(
            columns=[
                &#34;feature&#34;,
                &#34;tau&#34;,
                &#34;target&#34;,
                &#34;ksi&#34;,
                &#34;label&#34;,
                &#34;influence&#34;,
                &#34;influence_low&#34;,
                &#34;influence_high&#34;,
            ]
        )

    def get_metric_name(self, metric):
        &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
        performance with respect of the given metric.

        Args:
            metric (callable): The metric to compute the model&#39;s performance.

        Returns:
            str: The name of the column.
        &#34;&#34;&#34;
        name = super().get_metric_name(metric)
        if name in self.info.columns and name not in self.metric_names:
            raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
        return name

    @property
    def features(self):
        return self.info[&#34;feature&#34;].unique().tolist()

    @property
    def taus(self):
        tau_precision = 2 / (self.n_taus - 1)
        return list(decimal_range(-1, 1, tau_precision))

    def _determine_pairs_to_do(self, features, labels):
        to_do_pairs = set(itertools.product(features, labels)) - set(
            self.info.groupby([&#34;feature&#34;, &#34;label&#34;]).groups.keys()
        )
        to_do_map = collections.defaultdict(list)
        for feat, label in to_do_pairs:
            to_do_map[feat].append(label)
        return {feat: list(sorted(labels)) for feat, labels in to_do_map.items()}

    def _build_additional_info(self, X_test, y_pred):
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))
        X_test = self._one_hot_encode(X_test)

        # Check which (feature, label) pairs have to be done
        to_do_map = self._determine_pairs_to_do(
            features=X_test.columns, labels=y_pred.columns
        )
        # We need a list to keep the order of X_test
        to_do_features = list(feat for feat in X_test.columns if feat in to_do_map)
        X_test = X_test[to_do_features]

        if X_test.empty:
            return pd.DataFrame()

        quantiles = X_test.quantile(q=[self.alpha, 1.0 - self.alpha])

        # Issue a warning if a feature doesn&#39;t have distinct quantiles
        for feature, n_unique in quantiles.nunique().to_dict().items():
            if n_unique == 1:
                warnings.warn(
                    message=f&#34;all the values of feature {feature} are identical&#34;,
                    category=ConstantWarning,
                )

        q_mins = quantiles.loc[self.alpha].to_dict()
        q_maxs = quantiles.loc[1.0 - self.alpha].to_dict()
        means = X_test.mean().to_dict()
        info_to_complete = pd.concat(
            [
                pd.DataFrame(
                    {
                        &#34;tau&#34;: self.taus,
                        &#34;target&#34;: [
                            means[feature]
                            + tau
                            * (
                                max(means[feature] - q_mins[feature], 0)
                                if tau &lt; 0
                                else max(q_maxs[feature] - means[feature], 0)
                            )
                            for tau in self.taus
                        ],
                        &#34;feature&#34;: [feature] * len(self.taus),
                        &#34;label&#34;: [label] * len(self.taus),
                    }
                )
                # We need to iterate over `to_do_features` to keep the order of X_test
                for feature in to_do_features
                for label in to_do_map[feature]
            ],
            ignore_index=True,
        )
        return info_to_complete

    def _explain_with_cache(self, X_test, y_pred, explain):
        if not self.memoize:
            self._reset_info()

        # We need dataframes for the return. To make the conversion faster in the
        # following methods, we do it now.
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))
        X_test = self._one_hot_encode(X_test)

        additional_info = self._build_additional_info(X_test, y_pred)
        self.info = self.info.append(additional_info, ignore_index=True, sort=False)
        self.info = explain(query=self.info)

        return self.info[
            self.info[&#34;feature&#34;].isin(X_test.columns)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
        ]

    def explain_influence(self, X_test, y_pred):
        &#34;&#34;&#34;Compute the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                `pd.Series` is expected. For multi-label classification, a
                pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high)`. If `explainer.n_samples` is `1`,
                no confidence interval is computed and `influence = influence_low = influence_high`.
                The value of `label` is not important for regression.

        Examples:
            See more examples in `notebooks`.

            Binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            Regression is similar to binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [22, 24, 19]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            For multi-label classification, we need a dataframe to store predictions:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred.columns
            [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
            &gt;&gt;&gt; y_pred.iloc[0]
            [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
        &#34;&#34;&#34;
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_influence, X_test=X_test, y_pred=y_pred
            ),
        )

    def explain_performance(self, X_test, y_test, y_pred, metric):
        &#34;&#34;&#34;Compute the change in model&#39;s performance for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true values
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)`.
                If `explainer.n_samples` is `1`, no confidence interval is computed
                and `&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;`.
                The value of `label` is not important for regression.

        Examples:
            See examples in `notebooks`.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        self.metric_names.add(metric_name)
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_performance,
                X_test=X_test,
                y_pred=y_pred,
                y_test=y_test,
                metric=metric,
            ),
        )

    def rank_by_influence(self, X_test, y_pred):
        &#34;&#34;&#34;Returns a pandas DataFrame containing the importance of each feature
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).

        Returns:
            pd.DataFrame:
                A dataframe with columns `(label, feature, importance)`. The row
                `(setosa, petal length (cm), 0.282507)` means that the feature
                `petal length` of the Iris dataset has an importance of about
                30% in the prediction of the class `setosa`.

                The importance is a real number between 0 and 1. Intuitively,
                if the model influence for the feature `X` is a flat curve (the average
                model prediction is not impacted by the mean of `X`) then we
                can conclude that `X` has no importance for predictions. This
                flat curve is the baseline and satisfies \\(y = influence_{\\tau(0)}\\).
                To compute the importance of a feature, we look at the average
                distance of the influence curve to this baseline:

                $$
                I(X) = \\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mid influence_{\\tau(i)}(X) - influence_{\\tau(0)}(X) \\mid
                $$

                The influence curve is first normalized so that the importance is
                between 0 and 1 (which may not be the case originally for regression
                problems). To normalize, we get the minimum and maximum influences
                *across all features and all classes* and then compute
                `normalized = (influence - min) / (max - min)`.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;

        def get_importance(group, min_influence, max_influence):
            &#34;&#34;&#34;Computes the average absolute difference in influence changes per tau increase.&#34;&#34;&#34;
            #  Normalize influence to get an importance between 0 and 1
            # influence can be outside [0, 1] for regression
            influence = group[&#34;influence&#34;]
            group[&#34;influence&#34;] = (influence - min_influence) / (
                max_influence - min_influence
            )
            baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;influence&#34;]
            return (group[&#34;influence&#34;] - baseline).abs().mean()

        explanation = self.explain_influence(X_test=X_test, y_pred=y_pred)
        min_influence = explanation[&#34;influence&#34;].min()
        max_influence = explanation[&#34;influence&#34;].max()

        return (
            explanation.groupby([&#34;label&#34;, &#34;feature&#34;])
            .apply(
                functools.partial(
                    get_importance,
                    min_influence=min_influence,
                    max_influence=max_influence,
                )
            )
            .to_frame(&#34;importance&#34;)
            .reset_index()
        )

    def rank_by_performance(self, X_test, y_test, y_pred, metric):
        &#34;&#34;&#34;Returns a pandas DataFrame containing
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true output
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, min, max)`. The row
                `(age, 0.862010, 0.996360)` means that the score measured by the
                given metric (e.g. `sklearn.metrics.accuracy_score`) stays bewteen
                86.2% and 99.6% on average when we make the mean age change. With
                such information, we can find the features for which the model
                performs the worst or the best.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)

        def get_aggregates(df):
            return pd.Series(
                [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
            )

        return (
            self.explain_performance(X_test, y_test, y_pred, metric)
            .groupby(&#34;feature&#34;)
            .apply(get_aggregates)
            .reset_index()
        )

    def _plot_ranking(
        self, ranking, score_column, title, n_features=None, colors=None, size=None
    ):
        if n_features is None:
            n_features = len(ranking)
        ascending = n_features &gt;= 0
        ranking = ranking.sort_values(by=[score_column], ascending=ascending)
        n_features = abs(n_features)

        width = 500
        height = 100 + 60 * n_features
        if size is not None:
            width, height = size

        return go.Figure(
            data=[
                go.Bar(
                    x=ranking[score_column][-n_features:],
                    y=ranking[&#34;feature&#34;][-n_features:],
                    orientation=&#34;h&#34;,
                    hoverinfo=&#34;x&#34;,
                    marker=dict(color=colors),
                )
            ],
            layout=go.Layout(
                width=width,
                height=height,
                margin=dict(b=0, t=40, r=10),
                xaxis=dict(title=title, range=[0, 1], side=&#34;top&#34;, fixedrange=True),
                yaxis=dict(fixedrange=True, automargin=True),
                modebar=dict(
                    orientation=&#34;v&#34;,
                    color=&#34;rgba(0, 0, 0, 0)&#34;,
                    activecolor=&#34;rgba(0, 0, 0, 0)&#34;,
                    bgcolor=&#34;rgba(0, 0, 0, 0)&#34;,
                ),
            ),
        )

    def plot_influence(self, X_test, y_pred, colors=None, yrange=None, size=None):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colors (dict, optional): A dictionary that maps features to colors.
                Default is `None` and the colors are choosen automatically.
            yrange (list, optional): A two-item list `[low, high]`. Default is
                `None` and the range is based on the data.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.

        Examples:
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
            ...     x0=&#34;blue&#34;,
            ...     x1=&#34;red&#34;,
            ... ))
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
        &#34;&#34;&#34;
        explanation = self.explain_influence(X_test, y_pred)
        labels = explanation[&#34;label&#34;].unique()
        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)
        y_label = f&#34;Average &#39;{labels[0]}&#39;&#34;
        return self._plot_explanation(
            explanation, &#34;influence&#34;, y_label, colors=colors, yrange=yrange, size=size
        )

    def plot_influence_ranking(
        self, X_test, y_pred, n_features=None, colors=None, size=None
    ):
        &#34;&#34;&#34;Plot the ranking of the features based on their influence.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        ranking = self.rank_by_influence(X_test=X_test, y_pred=y_pred)
        return self._plot_ranking(
            ranking=ranking,
            score_column=&#34;importance&#34;,
            title=&#34;Importance&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )

    def plot_performance(
        self, X_test, y_test, y_pred, metric, colors=None, yrange=None, size=None
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            yrange (list, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        if yrange is None and explanation[metric_name].between(0, 1).all():
            yrange = [0, 1]

        #  The performance is the same for all labels, we remove duplicates
        label = explanation[&#34;label&#34;].unique()[0]
        explanation = explanation[explanation[&#34;label&#34;] == label]

        return self._plot_explanation(
            explanation,
            metric_name,
            y_label=f&#34;Average {metric_name}&#34;,
            colors=colors,
            yrange=yrange,
            size=size,
        )

    def plot_performance_ranking(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        criterion,
        n_features=None,
        colors=None,
        size=None,
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            criterion (str): Either &#34;min&#34; or &#34;max&#34; to determine whether, for a
                given feature, we keep the worst or the best performance for all
                the values taken by the mean. See `CacheExplainer.rank_by_performance()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence_ranking()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        ranking = self.rank_by_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        return self._plot_ranking(
            ranking=ranking,
            score_column=criterion,
            title=f&#34;{criterion} {metric_name}&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )</code></pre>
      </details>

  </section>

  <section>
  </section>

  <section>
  </section>

  <section>
  </section>

  <section>
    <h2 class="section-title" id="header-classes">Classes</h2>
    <dl>
      
      <dt id="ethik.cache_explainer.CacheExplainer"><code class="flex name class">
          <span>class <span class="ident">CacheExplainer</span></span>
              <span>(</span><span>alpha=0.05, n_taus=41, n_samples=1, sample_frac=0.8, conf_level=0.05, max_iterations=15, tol=0.0001, n_jobs=1, memoize=False, verbose=True)</span>
      </code></dt>

      <dd>
  
  <section class="desc"><p>Explains the influence of features on model predictions and performance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates by how close the <a title="ethik.cache_explainer.CacheExplainer" href="#ethik.cache_explainer.CacheExplainer"><code>CacheExplainer</code></a>
should look at extreme values of a distribution. The closer to zero, the more so
extreme values will be accounted for. The default is <code>0.05</code> which means that all values
beyond the 5th and 95th quantiles are ignored.</dd>
<dt><strong><code>n_taus</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of τ values to consider. The results will be more fine-grained the
higher this value is. However the computation time increases linearly with <code>n_taus</code>.
The default is <code>41</code> and corresponds to each τ being separated by it's neighbors by
<code>0.05</code>.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples to use for the confidence interval.
If <code>1</code>, the default, no confidence interval is computed.</dd>
<dt><strong><code>sample_frac</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of lines in the dataset sampled to
generate the samples for the confidence interval. If <code>n_samples</code> is
<code>1</code>, no confidence interval is computed and the whole dataset is used.
Default is <code>0.8</code>.</dd>
<dt><strong><code>conf_level</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates the
quantile used for the confidence interval. Default is <code>0.05</code>, which
means that the confidence interval contains the data between the 5th
and 95th quantiles.</dd>
<dt><strong><code>max_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>The maximum number of iterations used when applying the Newton step
of the optimization procedure. Default is <code>5</code>.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>The bottom threshold for the gradient of the optimization
procedure. When reached, the procedure stops. Otherwise, a warning
is raised about the fact that the optimization did not converge.
Default is <code>1e-4</code>.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of jobs to use for parallel computations. See
<code>joblib.Parallel()</code>. Default is <code>-1</code>.</dd>
<dt><strong><code>memoize</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether or not memoization should be used or not. If <code>True</code>, then
intermediate results will be stored in order to avoid recomputing results that can be
reused by successively called methods. For example, if you call <code>plot_influence</code> followed by
<code>plot_influence_ranking</code> and <code>memoize</code> is <code>True</code>, then the intermediate results required by
<code>plot_influence</code> will be reused for <code>plot_influence_ranking</code>. Memoization is turned off by
default because it can lead to unexpected behavior depending on your usage.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not to show progress bars during
computations. Default is <code>True</code>.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">class CacheExplainer(BaseExplainer):
    &#34;&#34;&#34;Explains the influence of features on model predictions and performance.

    Parameters:
        alpha (float): A `float` between `0` and `0.5` which indicates by how close the `CacheExplainer`
            should look at extreme values of a distribution. The closer to zero, the more so
            extreme values will be accounted for. The default is `0.05` which means that all values
            beyond the 5th and 95th quantiles are ignored.
        n_taus (int): The number of τ values to consider. The results will be more fine-grained the
            higher this value is. However the computation time increases linearly with `n_taus`.
            The default is `41` and corresponds to each τ being separated by it&#39;s neighbors by
            `0.05`.
        n_samples (int): The number of samples to use for the confidence interval.
            If `1`, the default, no confidence interval is computed.
        sample_frac (float): The proportion of lines in the dataset sampled to
            generate the samples for the confidence interval. If `n_samples` is
            `1`, no confidence interval is computed and the whole dataset is used.
            Default is `0.8`.
        conf_level (float): A `float` between `0` and `0.5` which indicates the
            quantile used for the confidence interval. Default is `0.05`, which
            means that the confidence interval contains the data between the 5th
            and 95th quantiles.
        max_iterations (int): The maximum number of iterations used when applying the Newton step
            of the optimization procedure. Default is `5`.
        tol (float): The bottom threshold for the gradient of the optimization
            procedure. When reached, the procedure stops. Otherwise, a warning
            is raised about the fact that the optimization did not converge.
            Default is `1e-4`.
        n_jobs (int): The number of jobs to use for parallel computations. See
            `joblib.Parallel()`. Default is `-1`.
        memoize (bool): Indicates whether or not memoization should be used or not. If `True`, then
            intermediate results will be stored in order to avoid recomputing results that can be
            reused by successively called methods. For example, if you call `plot_influence` followed by
            `plot_influence_ranking` and `memoize` is `True`, then the intermediate results required by
            `plot_influence` will be reused for `plot_influence_ranking`. Memoization is turned off by
            default because it can lead to unexpected behavior depending on your usage.
        verbose (bool): Whether or not to show progress bars during
            computations. Default is `True`.
    &#34;&#34;&#34;

    def __init__(
        self,
        alpha=0.05,
        n_taus=41,
        n_samples=1,
        sample_frac=0.8,
        conf_level=0.05,
        max_iterations=15,
        tol=1e-4,
        n_jobs=1,  # Parallelism is only worth it if the dataset is &#34;large&#34;
        memoize=False,
        verbose=True,
    ):
        super().__init__(
            alpha=alpha,
            n_samples=n_samples,
            sample_frac=sample_frac,
            conf_level=conf_level,
            max_iterations=max_iterations,
            tol=tol,
            n_jobs=n_jobs,
            verbose=verbose,
        )

        if not n_taus &gt; 0:
            raise ValueError(
                f&#34;n_taus must be a strictly positive integer, got {n_taus}&#34;
            )

        self.n_taus = n_taus
        self.memoize = memoize
        self.metric_names = set()
        self._reset_info()

    def _reset_info(self):
        &#34;&#34;&#34;Resets the info dataframe (for when memoization is turned off).&#34;&#34;&#34;
        self.info = pd.DataFrame(
            columns=[
                &#34;feature&#34;,
                &#34;tau&#34;,
                &#34;target&#34;,
                &#34;ksi&#34;,
                &#34;label&#34;,
                &#34;influence&#34;,
                &#34;influence_low&#34;,
                &#34;influence_high&#34;,
            ]
        )

    def get_metric_name(self, metric):
        &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
        performance with respect of the given metric.

        Args:
            metric (callable): The metric to compute the model&#39;s performance.

        Returns:
            str: The name of the column.
        &#34;&#34;&#34;
        name = super().get_metric_name(metric)
        if name in self.info.columns and name not in self.metric_names:
            raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
        return name

    @property
    def features(self):
        return self.info[&#34;feature&#34;].unique().tolist()

    @property
    def taus(self):
        tau_precision = 2 / (self.n_taus - 1)
        return list(decimal_range(-1, 1, tau_precision))

    def _determine_pairs_to_do(self, features, labels):
        to_do_pairs = set(itertools.product(features, labels)) - set(
            self.info.groupby([&#34;feature&#34;, &#34;label&#34;]).groups.keys()
        )
        to_do_map = collections.defaultdict(list)
        for feat, label in to_do_pairs:
            to_do_map[feat].append(label)
        return {feat: list(sorted(labels)) for feat, labels in to_do_map.items()}

    def _build_additional_info(self, X_test, y_pred):
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))
        X_test = self._one_hot_encode(X_test)

        # Check which (feature, label) pairs have to be done
        to_do_map = self._determine_pairs_to_do(
            features=X_test.columns, labels=y_pred.columns
        )
        # We need a list to keep the order of X_test
        to_do_features = list(feat for feat in X_test.columns if feat in to_do_map)
        X_test = X_test[to_do_features]

        if X_test.empty:
            return pd.DataFrame()

        quantiles = X_test.quantile(q=[self.alpha, 1.0 - self.alpha])

        # Issue a warning if a feature doesn&#39;t have distinct quantiles
        for feature, n_unique in quantiles.nunique().to_dict().items():
            if n_unique == 1:
                warnings.warn(
                    message=f&#34;all the values of feature {feature} are identical&#34;,
                    category=ConstantWarning,
                )

        q_mins = quantiles.loc[self.alpha].to_dict()
        q_maxs = quantiles.loc[1.0 - self.alpha].to_dict()
        means = X_test.mean().to_dict()
        info_to_complete = pd.concat(
            [
                pd.DataFrame(
                    {
                        &#34;tau&#34;: self.taus,
                        &#34;target&#34;: [
                            means[feature]
                            + tau
                            * (
                                max(means[feature] - q_mins[feature], 0)
                                if tau &lt; 0
                                else max(q_maxs[feature] - means[feature], 0)
                            )
                            for tau in self.taus
                        ],
                        &#34;feature&#34;: [feature] * len(self.taus),
                        &#34;label&#34;: [label] * len(self.taus),
                    }
                )
                # We need to iterate over `to_do_features` to keep the order of X_test
                for feature in to_do_features
                for label in to_do_map[feature]
            ],
            ignore_index=True,
        )
        return info_to_complete

    def _explain_with_cache(self, X_test, y_pred, explain):
        if not self.memoize:
            self._reset_info()

        # We need dataframes for the return. To make the conversion faster in the
        # following methods, we do it now.
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))
        X_test = self._one_hot_encode(X_test)

        additional_info = self._build_additional_info(X_test, y_pred)
        self.info = self.info.append(additional_info, ignore_index=True, sort=False)
        self.info = explain(query=self.info)

        return self.info[
            self.info[&#34;feature&#34;].isin(X_test.columns)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
        ]

    def explain_influence(self, X_test, y_pred):
        &#34;&#34;&#34;Compute the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                `pd.Series` is expected. For multi-label classification, a
                pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high)`. If `explainer.n_samples` is `1`,
                no confidence interval is computed and `influence = influence_low = influence_high`.
                The value of `label` is not important for regression.

        Examples:
            See more examples in `notebooks`.

            Binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            Regression is similar to binary classification:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred
            [22, 24, 19]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

            For multi-label classification, we need a dataframe to store predictions:

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model.predict(X_test)
            &gt;&gt;&gt; y_pred.columns
            [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
            &gt;&gt;&gt; y_pred.iloc[0]
            [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
            &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
        &#34;&#34;&#34;
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_influence, X_test=X_test, y_pred=y_pred
            ),
        )

    def explain_performance(self, X_test, y_test, y_pred, metric):
        &#34;&#34;&#34;Compute the change in model&#39;s performance for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true values
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, tau, target, ksi, label,
                influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)`.
                If `explainer.n_samples` is `1`, no confidence interval is computed
                and `&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;`.
                The value of `label` is not important for regression.

        Examples:
            See examples in `notebooks`.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        self.metric_names.add(metric_name)
        return self._explain_with_cache(
            X_test,
            y_pred,
            explain=functools.partial(
                self._explain_performance,
                X_test=X_test,
                y_pred=y_pred,
                y_test=y_test,
                metric=metric,
            ),
        )

    def rank_by_influence(self, X_test, y_pred):
        &#34;&#34;&#34;Returns a pandas DataFrame containing the importance of each feature
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).

        Returns:
            pd.DataFrame:
                A dataframe with columns `(label, feature, importance)`. The row
                `(setosa, petal length (cm), 0.282507)` means that the feature
                `petal length` of the Iris dataset has an importance of about
                30% in the prediction of the class `setosa`.

                The importance is a real number between 0 and 1. Intuitively,
                if the model influence for the feature `X` is a flat curve (the average
                model prediction is not impacted by the mean of `X`) then we
                can conclude that `X` has no importance for predictions. This
                flat curve is the baseline and satisfies \\(y = influence_{\\tau(0)}\\).
                To compute the importance of a feature, we look at the average
                distance of the influence curve to this baseline:

                $$
                I(X) = \\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mid influence_{\\tau(i)}(X) - influence_{\\tau(0)}(X) \\mid
                $$

                The influence curve is first normalized so that the importance is
                between 0 and 1 (which may not be the case originally for regression
                problems). To normalize, we get the minimum and maximum influences
                *across all features and all classes* and then compute
                `normalized = (influence - min) / (max - min)`.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;

        def get_importance(group, min_influence, max_influence):
            &#34;&#34;&#34;Computes the average absolute difference in influence changes per tau increase.&#34;&#34;&#34;
            #  Normalize influence to get an importance between 0 and 1
            # influence can be outside [0, 1] for regression
            influence = group[&#34;influence&#34;]
            group[&#34;influence&#34;] = (influence - min_influence) / (
                max_influence - min_influence
            )
            baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;influence&#34;]
            return (group[&#34;influence&#34;] - baseline).abs().mean()

        explanation = self.explain_influence(X_test=X_test, y_pred=y_pred)
        min_influence = explanation[&#34;influence&#34;].min()
        max_influence = explanation[&#34;influence&#34;].max()

        return (
            explanation.groupby([&#34;label&#34;, &#34;feature&#34;])
            .apply(
                functools.partial(
                    get_importance,
                    min_influence=min_influence,
                    max_influence=max_influence,
                )
            )
            .to_frame(&#34;importance&#34;)
            .reset_index()
        )

    def rank_by_performance(self, X_test, y_test, y_pred, metric):
        &#34;&#34;&#34;Returns a pandas DataFrame containing
        per label.

        Args:
            X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
                with one column per feature or a pandas series for a single feature.
            y_test (pd.DataFrame or pd.Series): The true output
                for the samples in `X_test`. For binary classification and regression,
                a `pd.Series` is expected. For multi-label classification,
                a pandas dataframe with one column per label is
                expected. The values can either be probabilities or `0/1`
                (for a one-hot-encoded output).
            y_pred (pd.DataFrame or pd.Series): The model predictions
                for the samples in `X_test`. The format is the same as `y_test`.
            metric (callable): A scikit-learn-like metric
                `f(y_true, y_pred, sample_weight=None)`. The metric must be able
                to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
                &#34;the set of labels predicted for a sample must exactly match the
                corresponding set of labels in `y_true`&#34;.

        Returns:
            pd.DataFrame:
                A dataframe with columns `(feature, min, max)`. The row
                `(age, 0.862010, 0.996360)` means that the score measured by the
                given metric (e.g. `sklearn.metrics.accuracy_score`) stays bewteen
                86.2% and 99.6% on average when we make the mean age change. With
                such information, we can find the features for which the model
                performs the worst or the best.

                For regression problems, there&#39;s one label only and its name
                doesn&#39;t matter (it&#39;s just to have a consistent output).
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)

        def get_aggregates(df):
            return pd.Series(
                [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
            )

        return (
            self.explain_performance(X_test, y_test, y_pred, metric)
            .groupby(&#34;feature&#34;)
            .apply(get_aggregates)
            .reset_index()
        )

    def _plot_ranking(
        self, ranking, score_column, title, n_features=None, colors=None, size=None
    ):
        if n_features is None:
            n_features = len(ranking)
        ascending = n_features &gt;= 0
        ranking = ranking.sort_values(by=[score_column], ascending=ascending)
        n_features = abs(n_features)

        width = 500
        height = 100 + 60 * n_features
        if size is not None:
            width, height = size

        return go.Figure(
            data=[
                go.Bar(
                    x=ranking[score_column][-n_features:],
                    y=ranking[&#34;feature&#34;][-n_features:],
                    orientation=&#34;h&#34;,
                    hoverinfo=&#34;x&#34;,
                    marker=dict(color=colors),
                )
            ],
            layout=go.Layout(
                width=width,
                height=height,
                margin=dict(b=0, t=40, r=10),
                xaxis=dict(title=title, range=[0, 1], side=&#34;top&#34;, fixedrange=True),
                yaxis=dict(fixedrange=True, automargin=True),
                modebar=dict(
                    orientation=&#34;v&#34;,
                    color=&#34;rgba(0, 0, 0, 0)&#34;,
                    activecolor=&#34;rgba(0, 0, 0, 0)&#34;,
                    bgcolor=&#34;rgba(0, 0, 0, 0)&#34;,
                ),
            ),
        )

    def plot_influence(self, X_test, y_pred, colors=None, yrange=None, size=None):
        &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            colors (dict, optional): A dictionary that maps features to colors.
                Default is `None` and the colors are choosen automatically.
            yrange (list, optional): A two-item list `[low, high]`. Default is
                `None` and the range is based on the data.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.

        Examples:
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
            ...     x0=&#34;blue&#34;,
            ...     x1=&#34;red&#34;,
            ... ))
            &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
        &#34;&#34;&#34;
        explanation = self.explain_influence(X_test, y_pred)
        labels = explanation[&#34;label&#34;].unique()
        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)
        y_label = f&#34;Average &#39;{labels[0]}&#39;&#34;
        return self._plot_explanation(
            explanation, &#34;influence&#34;, y_label, colors=colors, yrange=yrange, size=size
        )

    def plot_influence_ranking(
        self, X_test, y_pred, n_features=None, colors=None, size=None
    ):
        &#34;&#34;&#34;Plot the ranking of the features based on their influence.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        ranking = self.rank_by_influence(X_test=X_test, y_pred=y_pred)
        return self._plot_ranking(
            ranking=ranking,
            score_column=&#34;importance&#34;,
            title=&#34;Importance&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )

    def plot_performance(
        self, X_test, y_test, y_pred, metric, colors=None, yrange=None, size=None
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            colors (dict, optional): See `CacheExplainer.plot_influence()`.
            yrange (list, optional): See `CacheExplainer.plot_influence()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        if yrange is None and explanation[metric_name].between(0, 1).all():
            yrange = [0, 1]

        #  The performance is the same for all labels, we remove duplicates
        label = explanation[&#34;label&#34;].unique()[0]
        explanation = explanation[explanation[&#34;label&#34;] == label]

        return self._plot_explanation(
            explanation,
            metric_name,
            y_label=f&#34;Average {metric_name}&#34;,
            colors=colors,
            yrange=yrange,
            size=size,
        )

    def plot_performance_ranking(
        self,
        X_test,
        y_test,
        y_pred,
        metric,
        criterion,
        n_features=None,
        colors=None,
        size=None,
    ):
        &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
            y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
            metric (callable): See `CacheExplainer.explain_performance()`.
            criterion (str): Either &#34;min&#34; or &#34;max&#34; to determine whether, for a
                given feature, we keep the worst or the best performance for all
                the values taken by the mean. See `CacheExplainer.rank_by_performance()`.
            n_features (int, optional): The number of features to plot. With the
                default (`None`), all of them are shown. For a positive value,
                we keep the `n_features` first features (the most impactful). For
                a negative value, we keep the `n_features` last features.
            colors (dict, optional): See `CacheExplainer.plot_influence_ranking()`.
            size (tuple, optional): An optional couple `(width, height)` in pixels.

        Returns:
            plotly.graph_objs.Figure:
                A Plotly figure. It shows automatically in notebook cells but you
                can also call the `.show()` method to plot multiple charts in the
                same cell.
        &#34;&#34;&#34;
        metric_name = self.get_metric_name(metric)
        ranking = self.rank_by_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        return self._plot_ranking(
            ranking=ranking,
            score_column=criterion,
            title=f&#34;{criterion} {metric_name}&#34;,
            n_features=n_features,
            colors=colors,
            size=size,
        )</code></pre>
      </details>



          <h3>Ancestors</h3>
          <ul class="hlist">
              <li><a title="ethik.base_explainer.BaseExplainer" href="base_explainer.html#ethik.base_explainer.BaseExplainer">BaseExplainer</a></li>
          </ul>

          <h3>Subclasses</h3>
          <ul class="hlist">
              <li><a title="ethik.classification_explainer.ClassificationExplainer" href="classification_explainer.html#ethik.classification_explainer.ClassificationExplainer">ClassificationExplainer</a></li>
              <li><a title="ethik.regression_explainer.RegressionExplainer" href="regression_explainer.html#ethik.regression_explainer.RegressionExplainer">RegressionExplainer</a></li>
              <li><a title="ethik.image_classification_explainer.ImageClassificationExplainer" href="image_classification_explainer.html#ethik.image_classification_explainer.ImageClassificationExplainer">ImageClassificationExplainer</a></li>
          </ul>
          <h3>Instance variables</h3>
          <dl>
              <dt id="ethik.cache_explainer.CacheExplainer.features"><code class="name">var <span class="ident">features</span></code></dt>
              <dd>
  
  <section class="desc"></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">@property
def features(self):
    return self.info[&#34;feature&#34;].unique().tolist()</code></pre>
      </details>

</dd>
              <dt id="ethik.cache_explainer.CacheExplainer.taus"><code class="name">var <span class="ident">taus</span></code></dt>
              <dd>
  
  <section class="desc"></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">@property
def taus(self):
    tau_precision = 2 / (self.n_taus - 1)
    return list(decimal_range(-1, 1, tau_precision))</code></pre>
      </details>

</dd>
          </dl>
          <h3>Methods</h3>
          <dl>
              
    <dt id="ethik.cache_explainer.CacheExplainer.explain_influence"><code class="name flex">
        
        <span>def <span class="ident">explain_influence</span></span>(<span>self, X_test, y_pred)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Compute the influence of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. For binary classification and regression,
<code>pd.Series</code> is expected. For multi-label classification, a
pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>A dataframe with columns <code>(feature, tau, target, ksi, label,
influence, influence_low, influence_high)</code>. If <code>explainer.n_samples</code> is <code>1</code>,
no confidence interval is computed and <code>influence = influence_low = influence_high</code>.
The value of <code>label</code> is not important for regression.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>See more examples in <code>notebooks</code>.</p>
<p>Binary classification:</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model.predict(X_test)
&gt;&gt;&gt; y_pred
[0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
&gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
&gt;&gt;&gt; y_pred = pd.Series(y_pred, name="is_reliable")
&gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
</code></pre>
<p>Regression is similar to binary classification:</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model.predict(X_test)
&gt;&gt;&gt; y_pred
[22, 24, 19]
&gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
&gt;&gt;&gt; y_pred = pd.Series(y_pred, name="price")
&gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
</code></pre>
<p>For multi-label classification, we need a dataframe to store predictions:</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model.predict(X_test)
&gt;&gt;&gt; y_pred.columns
["class0", "class1", "class2"]
&gt;&gt;&gt; y_pred.iloc[0]
[0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
&gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
</code></pre></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def explain_influence(self, X_test, y_pred):
    &#34;&#34;&#34;Compute the influence of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. For binary classification and regression,
            `pd.Series` is expected. For multi-label classification, a
            pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).

    Returns:
        pd.DataFrame:
            A dataframe with columns `(feature, tau, target, ksi, label,
            influence, influence_low, influence_high)`. If `explainer.n_samples` is `1`,
            no confidence interval is computed and `influence = influence_low = influence_high`.
            The value of `label` is not important for regression.

    Examples:
        See more examples in `notebooks`.

        Binary classification:

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model.predict(X_test)
        &gt;&gt;&gt; y_pred
        [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
        &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
        &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
        &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

        Regression is similar to binary classification:

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model.predict(X_test)
        &gt;&gt;&gt; y_pred
        [22, 24, 19]
        &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
        &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
        &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)

        For multi-label classification, we need a dataframe to store predictions:

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model.predict(X_test)
        &gt;&gt;&gt; y_pred.columns
        [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
        &gt;&gt;&gt; y_pred.iloc[0]
        [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
        &gt;&gt;&gt; explainer.explain_influence(X_test, y_pred)
    &#34;&#34;&#34;
    return self._explain_with_cache(
        X_test,
        y_pred,
        explain=functools.partial(
            self._explain_influence, X_test=X_test, y_pred=y_pred
        ),
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.explain_performance"><code class="name flex">
        
        <span>def <span class="ident">explain_performance</span></span>(<span>self, X_test, y_test, y_pred, metric)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Compute the change in model's performance for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The true values
for the samples in <code>X_test</code>. For binary classification and regression,
a <code>pd.Series</code> is expected. For multi-label classification,
a pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. The format is the same as <code>y_test</code>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>A scikit-learn-like metric
<code>f(y_true, y_pred, sample_weight=None)</code>. The metric must be able
to handle the <code>y</code> data. For instance, for <code>sklearn.metrics.accuracy_score()</code>,
"the set of labels predicted for a sample must exactly match the
corresponding set of labels in <code>y_true</code>".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>A dataframe with columns <code>(feature, tau, target, ksi, label,
influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)</code>.
If <code>explainer.n_samples</code> is <code>1</code>, no confidence interval is computed
and <code>&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;</code>.
The value of <code>label</code> is not important for regression.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>See examples in <code>notebooks</code>.</p></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def explain_performance(self, X_test, y_test, y_pred, metric):
    &#34;&#34;&#34;Compute the change in model&#39;s performance for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_test (pd.DataFrame or pd.Series): The true values
            for the samples in `X_test`. For binary classification and regression,
            a `pd.Series` is expected. For multi-label classification,
            a pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. The format is the same as `y_test`.
        metric (callable): A scikit-learn-like metric
            `f(y_true, y_pred, sample_weight=None)`. The metric must be able
            to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
            &#34;the set of labels predicted for a sample must exactly match the
            corresponding set of labels in `y_true`&#34;.

    Returns:
        pd.DataFrame:
            A dataframe with columns `(feature, tau, target, ksi, label,
            influence, influence_low, influence_high, &lt;metric_name&gt;, &lt;metric_name_low&gt;, &lt;metric_name_high&gt;)`.
            If `explainer.n_samples` is `1`, no confidence interval is computed
            and `&lt;metric_name&gt; = &lt;metric_name_low&gt; = &lt;metric_name_high&gt;`.
            The value of `label` is not important for regression.

    Examples:
        See examples in `notebooks`.
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)
    self.metric_names.add(metric_name)
    return self._explain_with_cache(
        X_test,
        y_pred,
        explain=functools.partial(
            self._explain_performance,
            X_test=X_test,
            y_pred=y_pred,
            y_test=y_test,
            metric=metric,
        ),
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_influence"><code class="name flex">
        
        <span>def <span class="ident">plot_influence</span></span>(<span>self, X_test, y_pred, colors=None, yrange=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the influence of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dictionary that maps features to colors.
Default is <code>None</code> and the colors are choosen automatically.</dd>
<dt><strong><code>yrange</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A two-item list <code>[low, high]</code>. Default is
<code>None</code> and the range is based on the data.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
&gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
...     x0="blue",
...     x1="red",
... ))
&gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
</code></pre></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_influence(self, X_test, y_pred, colors=None, yrange=None, size=None):
    &#34;&#34;&#34;Plot the influence of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
        colors (dict, optional): A dictionary that maps features to colors.
            Default is `None` and the colors are choosen automatically.
        yrange (list, optional): A two-item list `[low, high]`. Default is
            `None` and the range is based on the data.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.

    Examples:
        &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred)
        &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, colors=dict(
        ...     x0=&#34;blue&#34;,
        ...     x1=&#34;red&#34;,
        ... ))
        &gt;&gt;&gt; explainer.plot_influence(X_test, y_pred, yrange=[0.5, 1])
    &#34;&#34;&#34;
    explanation = self.explain_influence(X_test, y_pred)
    labels = explanation[&#34;label&#34;].unique()
    if len(labels) &gt; 1:
        raise ValueError(&#34;Cannot plot multiple labels&#34;)
    y_label = f&#34;Average &#39;{labels[0]}&#39;&#34;
    return self._plot_explanation(
        explanation, &#34;influence&#34;, y_label, colors=colors, yrange=yrange, size=size
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_influence_ranking"><code class="name flex">
        
        <span>def <span class="ident">plot_influence_ranking</span></span>(<span>self, X_test, y_pred, n_features=None, colors=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the ranking of the features based on their influence.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence"><code>CacheExplainer.explain_influence()</code></a>.</dd>
<dt><strong><code>n_features</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of features to plot. With the
default (<code>None</code>), all of them are shown. For a positive value,
we keep the <code>n_features</code> first features (the most impactful). For
a negative value, we keep the <code>n_features</code> last features.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_influence_ranking(
    self, X_test, y_pred, n_features=None, colors=None, size=None
):
    &#34;&#34;&#34;Plot the ranking of the features based on their influence.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_influence()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_influence()`.
        n_features (int, optional): The number of features to plot. With the
            default (`None`), all of them are shown. For a positive value,
            we keep the `n_features` first features (the most impactful). For
            a negative value, we keep the `n_features` last features.
        colors (dict, optional): See `CacheExplainer.plot_influence()`.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    ranking = self.rank_by_influence(X_test=X_test, y_pred=y_pred)
    return self._plot_ranking(
        ranking=ranking,
        score_column=&#34;importance&#34;,
        title=&#34;Importance&#34;,
        n_features=n_features,
        colors=colors,
        size=size,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_performance"><code class="name flex">
        
        <span>def <span class="ident">plot_performance</span></span>(<span>self, X_test, y_test, y_pred, metric, colors=None, yrange=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the performance of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>yrange</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence"><code>CacheExplainer.plot_influence()</code></a>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_performance(
    self, X_test, y_test, y_pred, metric, colors=None, yrange=None, size=None
):
    &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
        y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        metric (callable): See `CacheExplainer.explain_performance()`.
        colors (dict, optional): See `CacheExplainer.plot_influence()`.
        yrange (list, optional): See `CacheExplainer.plot_influence()`.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)
    explanation = self.explain_performance(
        X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
    )
    if yrange is None and explanation[metric_name].between(0, 1).all():
        yrange = [0, 1]

    #  The performance is the same for all labels, we remove duplicates
    label = explanation[&#34;label&#34;].unique()[0]
    explanation = explanation[explanation[&#34;label&#34;] == label]

    return self._plot_explanation(
        explanation,
        metric_name,
        y_label=f&#34;Average {metric_name}&#34;,
        colors=colors,
        yrange=yrange,
        size=size,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.plot_performance_ranking"><code class="name flex">
        
        <span>def <span class="ident">plot_performance_ranking</span></span>(<span>self, X_test, y_test, y_pred, metric, criterion, n_features=None, colors=None, size=None)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Plot the performance of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance"><code>CacheExplainer.explain_performance()</code></a>.</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>str</code></dt>
<dd>Either "min" or "max" to determine whether, for a
given feature, we keep the worst or the best performance for all
the values taken by the mean. See <a title="ethik.cache_explainer.CacheExplainer.rank_by_performance" href="#ethik.cache_explainer.CacheExplainer.rank_by_performance"><code>CacheExplainer.rank_by_performance()</code></a>.</dd>
<dt><strong><code>n_features</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of features to plot. With the
default (<code>None</code>), all of them are shown. For a positive value,
we keep the <code>n_features</code> first features (the most impactful). For
a negative value, we keep the <code>n_features</code> last features.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>See <a title="ethik.cache_explainer.CacheExplainer.plot_influence_ranking" href="#ethik.cache_explainer.CacheExplainer.plot_influence_ranking"><code>CacheExplainer.plot_influence_ranking()</code></a>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>An optional couple <code>(width, height)</code> in pixels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>plotly.graph_objs.Figure</code>:</dt>
<dd>A Plotly figure. It shows automatically in notebook cells but you
can also call the <code>.show()</code> method to plot multiple charts in the
same cell.</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def plot_performance_ranking(
    self,
    X_test,
    y_test,
    y_pred,
    metric,
    criterion,
    n_features=None,
    colors=None,
    size=None,
):
    &#34;&#34;&#34;Plot the performance of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or np.array): See `CacheExplainer.explain_performance()`.
        y_test (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        y_pred (pd.DataFrame or pd.Series): See `CacheExplainer.explain_performance()`.
        metric (callable): See `CacheExplainer.explain_performance()`.
        criterion (str): Either &#34;min&#34; or &#34;max&#34; to determine whether, for a
            given feature, we keep the worst or the best performance for all
            the values taken by the mean. See `CacheExplainer.rank_by_performance()`.
        n_features (int, optional): The number of features to plot. With the
            default (`None`), all of them are shown. For a positive value,
            we keep the `n_features` first features (the most impactful). For
            a negative value, we keep the `n_features` last features.
        colors (dict, optional): See `CacheExplainer.plot_influence_ranking()`.
        size (tuple, optional): An optional couple `(width, height)` in pixels.

    Returns:
        plotly.graph_objs.Figure:
            A Plotly figure. It shows automatically in notebook cells but you
            can also call the `.show()` method to plot multiple charts in the
            same cell.
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)
    ranking = self.rank_by_performance(
        X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
    )
    return self._plot_ranking(
        ranking=ranking,
        score_column=criterion,
        title=f&#34;{criterion} {metric_name}&#34;,
        n_features=n_features,
        colors=colors,
        size=size,
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.rank_by_influence"><code class="name flex">
        
        <span>def <span class="ident">rank_by_influence</span></span>(<span>self, X_test, y_pred)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Returns a pandas DataFrame containing the importance of each feature
per label.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. For binary classification and regression,
a <code>pd.Series</code> is expected. For multi-label classification,
a pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>
<p>A dataframe with columns <code>(label, feature, importance)</code>. The row
<code>(setosa, petal length (cm), 0.282507)</code> means that the feature
<code>petal length</code> of the Iris dataset has an importance of about
30% in the prediction of the class <code>setosa</code>.</p>
<p>The importance is a real number between 0 and 1. Intuitively,
if the model influence for the feature <code>X</code> is a flat curve (the average
model prediction is not impacted by the mean of <code>X</code>) then we
can conclude that <code>X</code> has no importance for predictions. This
flat curve is the baseline and satisfies <span><span class="MathJax_Preview">y = influence_{\tau(0)}</span><script type="math/tex">y = influence_{\tau(0)}</script></span>.
To compute the importance of a feature, we look at the average
distance of the influence curve to this baseline:</p>
<p><span><span class="MathJax_Preview">
I(X) = \frac{1}{n_\tau} \sum_{i=1}^{n_\tau} \mid influence_{\tau(i)}(X) - influence_{\tau(0)}(X) \mid
</span><script type="math/tex; mode=display">
I(X) = \frac{1}{n_\tau} \sum_{i=1}^{n_\tau} \mid influence_{\tau(i)}(X) - influence_{\tau(0)}(X) \mid
</script></span></p>
<p>The influence curve is first normalized so that the importance is
between 0 and 1 (which may not be the case originally for regression
problems). To normalize, we get the minimum and maximum influences
<em>across all features and all classes</em> and then compute
<code>normalized = (influence - min) / (max - min)</code>.</p>
<p>For regression problems, there's one label only and its name
doesn't matter (it's just to have a consistent output).</p>
</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def rank_by_influence(self, X_test, y_pred):
    &#34;&#34;&#34;Returns a pandas DataFrame containing the importance of each feature
    per label.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. For binary classification and regression,
            a `pd.Series` is expected. For multi-label classification,
            a pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).

    Returns:
        pd.DataFrame:
            A dataframe with columns `(label, feature, importance)`. The row
            `(setosa, petal length (cm), 0.282507)` means that the feature
            `petal length` of the Iris dataset has an importance of about
            30% in the prediction of the class `setosa`.

            The importance is a real number between 0 and 1. Intuitively,
            if the model influence for the feature `X` is a flat curve (the average
            model prediction is not impacted by the mean of `X`) then we
            can conclude that `X` has no importance for predictions. This
            flat curve is the baseline and satisfies \\(y = influence_{\\tau(0)}\\).
            To compute the importance of a feature, we look at the average
            distance of the influence curve to this baseline:

            $$
            I(X) = \\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mid influence_{\\tau(i)}(X) - influence_{\\tau(0)}(X) \\mid
            $$

            The influence curve is first normalized so that the importance is
            between 0 and 1 (which may not be the case originally for regression
            problems). To normalize, we get the minimum and maximum influences
            *across all features and all classes* and then compute
            `normalized = (influence - min) / (max - min)`.

            For regression problems, there&#39;s one label only and its name
            doesn&#39;t matter (it&#39;s just to have a consistent output).
    &#34;&#34;&#34;

    def get_importance(group, min_influence, max_influence):
        &#34;&#34;&#34;Computes the average absolute difference in influence changes per tau increase.&#34;&#34;&#34;
        #  Normalize influence to get an importance between 0 and 1
        # influence can be outside [0, 1] for regression
        influence = group[&#34;influence&#34;]
        group[&#34;influence&#34;] = (influence - min_influence) / (
            max_influence - min_influence
        )
        baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;influence&#34;]
        return (group[&#34;influence&#34;] - baseline).abs().mean()

    explanation = self.explain_influence(X_test=X_test, y_pred=y_pred)
    min_influence = explanation[&#34;influence&#34;].min()
    max_influence = explanation[&#34;influence&#34;].max()

    return (
        explanation.groupby([&#34;label&#34;, &#34;feature&#34;])
        .apply(
            functools.partial(
                get_importance,
                min_influence=min_influence,
                max_influence=max_influence,
            )
        )
        .to_frame(&#34;importance&#34;)
        .reset_index()
    )</code></pre>
      </details>

</dd>
  
              
    <dt id="ethik.cache_explainer.CacheExplainer.rank_by_performance"><code class="name flex">
        
        <span>def <span class="ident">rank_by_performance</span></span>(<span>self, X_test, y_test, y_pred, metric)</span>
    </code></dt>
    <dd>
  
  <section class="desc"><p>Returns a pandas DataFrame containing
per label.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The dataset as a pandas dataframe
with one column per feature or a pandas series for a single feature.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The true output
for the samples in <code>X_test</code>. For binary classification and regression,
a <code>pd.Series</code> is expected. For multi-label classification,
a pandas dataframe with one column per label is
expected. The values can either be probabilities or <code>0/1</code>
(for a one-hot-encoded output).</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>pd.Series</code></dt>
<dd>The model predictions
for the samples in <code>X_test</code>. The format is the same as <code>y_test</code>.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>A scikit-learn-like metric
<code>f(y_true, y_pred, sample_weight=None)</code>. The metric must be able
to handle the <code>y</code> data. For instance, for <code>sklearn.metrics.accuracy_score()</code>,
"the set of labels predicted for a sample must exactly match the
corresponding set of labels in <code>y_true</code>".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code>:</dt>
<dd>
<p>A dataframe with columns <code>(feature, min, max)</code>. The row
<code>(age, 0.862010, 0.996360)</code> means that the score measured by the
given metric (e.g. <code>sklearn.metrics.accuracy_score</code>) stays bewteen
86.2% and 99.6% on average when we make the mean age change. With
such information, we can find the features for which the model
performs the worst or the best.</p>
<p>For regression problems, there's one label only and its name
doesn't matter (it's just to have a consistent output).</p>
</dd>
</dl></section>
  
    
      <details class="source">
        <summary>
            <span>Expand source code</span>
        </summary>
        <pre><code class="python">def rank_by_performance(self, X_test, y_test, y_pred, metric):
    &#34;&#34;&#34;Returns a pandas DataFrame containing
    per label.

    Args:
        X_test (pd.DataFrame or pd.Series): The dataset as a pandas dataframe
            with one column per feature or a pandas series for a single feature.
        y_test (pd.DataFrame or pd.Series): The true output
            for the samples in `X_test`. For binary classification and regression,
            a `pd.Series` is expected. For multi-label classification,
            a pandas dataframe with one column per label is
            expected. The values can either be probabilities or `0/1`
            (for a one-hot-encoded output).
        y_pred (pd.DataFrame or pd.Series): The model predictions
            for the samples in `X_test`. The format is the same as `y_test`.
        metric (callable): A scikit-learn-like metric
            `f(y_true, y_pred, sample_weight=None)`. The metric must be able
            to handle the `y` data. For instance, for `sklearn.metrics.accuracy_score()`,
            &#34;the set of labels predicted for a sample must exactly match the
            corresponding set of labels in `y_true`&#34;.

    Returns:
        pd.DataFrame:
            A dataframe with columns `(feature, min, max)`. The row
            `(age, 0.862010, 0.996360)` means that the score measured by the
            given metric (e.g. `sklearn.metrics.accuracy_score`) stays bewteen
            86.2% and 99.6% on average when we make the mean age change. With
            such information, we can find the features for which the model
            performs the worst or the best.

            For regression problems, there&#39;s one label only and its name
            doesn&#39;t matter (it&#39;s just to have a consistent output).
    &#34;&#34;&#34;
    metric_name = self.get_metric_name(metric)

    def get_aggregates(df):
        return pd.Series(
            [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
        )

    return (
        self.explain_performance(X_test, y_test, y_pred, metric)
        .groupby(&#34;feature&#34;)
        .apply(get_aggregates)
        .reset_index()
    )</code></pre>
      </details>

</dd>
  
          </dl>

          
              <h3>Inherited members</h3>
              <ul class="hlist">
                  <li><code><b><a title="ethik.base_explainer.BaseExplainer" href="base_explainer.html#ethik.base_explainer.BaseExplainer">BaseExplainer</a></b></code>:
                      <ul class="hlist">
                              <li><code><a title="ethik.base_explainer.BaseExplainer.CAT_COL_SEP" href="base_explainer.html#ethik.base_explainer.BaseExplainer.CAT_COL_SEP">CAT_COL_SEP</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compare_influence" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compare_influence">compare_influence</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compare_performance" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compare_performance">compare_performance</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compute_distributions" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compute_distributions">compute_distributions</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.compute_weights" href="base_explainer.html#ethik.base_explainer.BaseExplainer.compute_weights">compute_weights</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.get_metric_name" href="base_explainer.html#ethik.base_explainer.BaseExplainer.get_metric_name">get_metric_name</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_distributions" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_distributions">plot_distributions</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_influence_comparison" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_influence_comparison">plot_influence_comparison</a></code></li>
                              <li><code><a title="ethik.base_explainer.BaseExplainer.plot_performance_comparison" href="base_explainer.html#ethik.base_explainer.BaseExplainer.plot_performance_comparison">plot_performance_comparison</a></code></li>
                      </ul>

                  </li>
              </ul>

      </dd>
    </dl>
  </section>

  </article>
  
  
  <nav id="sidebar">

    

    <h1>Index</h1>
    <div class="toc">
<ul></ul>
</div>
    <ul id="index">
    <li><h3>Super-module</h3>
      <ul>
        <li><code><a title="ethik" href="index.html">ethik</a></code></li>
      </ul>
    </li>




    <li><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li>
        <h4><code><a title="ethik.cache_explainer.CacheExplainer" href="#ethik.cache_explainer.CacheExplainer">CacheExplainer</a></code></h4>
        
          
  
  <ul class="">
    <li><code><a title="ethik.cache_explainer.CacheExplainer.explain_influence" href="#ethik.cache_explainer.CacheExplainer.explain_influence">explain_influence</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.explain_performance" href="#ethik.cache_explainer.CacheExplainer.explain_performance">explain_performance</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.features" href="#ethik.cache_explainer.CacheExplainer.features">features</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_influence" href="#ethik.cache_explainer.CacheExplainer.plot_influence">plot_influence</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_influence_ranking" href="#ethik.cache_explainer.CacheExplainer.plot_influence_ranking">plot_influence_ranking</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_performance" href="#ethik.cache_explainer.CacheExplainer.plot_performance">plot_performance</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.plot_performance_ranking" href="#ethik.cache_explainer.CacheExplainer.plot_performance_ranking">plot_performance_ranking</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.rank_by_influence" href="#ethik.cache_explainer.CacheExplainer.rank_by_influence">rank_by_influence</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.rank_by_performance" href="#ethik.cache_explainer.CacheExplainer.rank_by_performance">rank_by_performance</a></code></li>
    <li><code><a title="ethik.cache_explainer.CacheExplainer.taus" href="#ethik.cache_explainer.CacheExplainer.taus">taus</a></code></li>
  </ul>

        </li>
      </ul>
    </li>

    </ul>
  </nav>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad()</script>