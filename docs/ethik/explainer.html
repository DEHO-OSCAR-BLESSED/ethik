<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>ethik.explainer API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ethik.explainer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import collections
import functools
import itertools

import joblib
import numpy as np
import pandas as pd
import plotly.graph_objs as go
from scipy import special

from .utils import decimal_range, to_pandas

__all__ = [&#34;Explainer&#34;]


CAT_COL_SEP = &#34; = &#34;


def compute_lambdas(x, target_means, iterations=5, use_previous_lambda=False):
    &#34;&#34;&#34;Find good lambdas for a variable and given target means.

    Args:
        x (pd.Series): The variable&#39;s values.
        target_means (iterator of floats): The means to reach by weighting the
            feature&#39;s values.
        iterations (int): The number of iterations to compute the weights for a
            given target mean. Default is `5`.

    Returns:
        dict: The keys are couples `(name of the variable, target mean)` and the
            values are the lambdas. For instance:

                {
                    (&#34;age&#34;, 20): 1.5,
                    (&#34;age&#34;, 21): 1.6,
                    ...
                }
    &#34;&#34;&#34;

    mean = x.mean()
    lambdas = {}

    λ = 0

    for target_mean in target_means:

        current_mean = mean

        for _ in range(iterations):

            # Update the sample weights and see where the mean is
            sample_weights = special.softmax(λ * x)
            current_mean = np.average(x, weights=sample_weights)

            # Do a Newton step using the difference between the mean and the
            # target mean
            grad = current_mean - target_mean
            hess = np.average((x - current_mean) ** 2, weights=sample_weights)

            if hess == 0:
                # We use a magic number based on experience for the step size, this is subject
                # to change
                λ -= 1e-5 * grad
                break

            step = grad / hess
            λ -= step

        lambdas[(x.name, target_mean)] = λ

    return lambdas


def compute_bias(y_pred, x, lambdas, sample_index):
    &#34;&#34;&#34;Compute the influence of the variable `x` on the predictions.

    Args:
        y_pred (pd.Series): The predictions.
        x (pd.Series): The variable&#39;s values.
        lambdas (iterator of floats): The lambdas for each target mean of `x`.
        sample_index (int): The index of the sample used to compute the confidence
            interval. Only used to merge the data later.

    Returns:
        list of tuples: A list of `(x&#39;s name, y_pred&#39;s name, lambda, sample_index, bias)`
            for each lambda.
    &#34;&#34;&#34;
    return [
        (
            x.name,
            y_pred.name,
            λ,
            sample_index,
            np.average(y_pred, weights=special.softmax(λ * x)),
        )
        for λ in lambdas
    ]


def compute_performance(y_test, y_pred, metric, x, lambdas, sample_index):
    &#34;&#34;&#34;Compute the influence of the variable `x` on the model&#39;s performance.

    Args:
        y_test (pd.Series): The true predictions.
        y_pred (pd.Series): The model&#39;s predictions.
        metric (callable): A scikit-learn-like metric with such an interface:
            `metric(y_test, y_pred, sample_weight=None)`.
        x (pd.Series): The variable&#39;s values.
        lambdas (iterator of floats): The lambdas for each target mean of `x`.
        sample_index (int): The index of the sample used to compute the confidence
            interval. Only used to merge the data later.

    Returns:
        list of tuples: A list of `(x&#39;s name, lambda, sample_index, performance)`
            for each lambda.
    &#34;&#34;&#34;
    return [
        (
            x.name,
            λ,
            sample_index,
            metric(y_test, y_pred, sample_weight=special.softmax(λ * x)),
        )
        for λ in lambdas
    ]


def yield_masks(n_masks, n, p):
    &#34;&#34;&#34;Generates a list of `n_masks` to keep a proportion `p` of `n` items.

    Args:
        n_masks (int): The number of masks to yield. It corresponds to the number
            of samples we use to compute the confidence interval.
        n (int): The number of items being filtered. It corresponds to the size
            of the dataset.
        p (float): The proportion of items to keep.

    Returns:
        generator: A generator of `n_masks` lists of `n` booleans being generated
            with a binomial distribution. As it is a probabilistic approach,
            we may get more or fewer than `p*n` items kept, but it is not a problem
            with large datasets.
    &#34;&#34;&#34;

    if p &lt; 0 or p &gt; 1:
        raise ValueError(f&#34;p must be between 0 and 1, got {p}&#34;)

    if p &lt; 1:
        for _ in range(n_masks):
            yield np.random.binomial(1, p, size=n).astype(bool)
    else:
        for _ in range(n_masks):
            yield np.full(shape=n, fill_value=True)


class Explainer:
    &#34;&#34;&#34;Explains the bias and reliability of model predictions.

    Parameters:
        alpha (float): A `float` between `0` and `0.5` which indicates by how close the `Explainer`
            should look at extreme values of a distribution. The closer to zero, the more so
            extreme values will be accounted for. The default is `0.05` which means that all values
            beyond the 5th and 95th quantiles are ignored.
        n_taus (int): The number of τ values to consider. The results will be more fine-grained the
            higher this value is. However the computation time increases linearly with `n_taus`.
            The default is `41` and corresponds to each τ being separated by it&#39;s neighbors by
            `0.05`.
        lambda_iterations (int): The number of iterations used when applying the Newton step
            of the optimization procedure. Default is `5`.
        n_jobs (int): The number of jobs to use for parallel computations. See
            `joblib.Parallel()`. Default is `-1`.
        verbose (bool): Passed to `joblib.Parallel()` for parallel computations.
            Default is `False`.
        n_samples (int): The number of samples to use for the confidence interval.
            If `1`, the default, no confidence interval is computed.
        sample_frac (float): The proportion of lines in the dataset sampled to
            generate the samples for the confidence interval. If `n_samples` is
            `1`, no confidence interval is computed and the whole dataset is used.
            Default is `0.8`.
        conf_level (float): A `float` between `0` and `0.5` which indicates the
            quantile used for the confidence interval. Default is `0.05`, which
            means that the confidence interval contains the data between the 5th
            and 95th quantiles.
        memoize (bool): Indicates whether or not memoization should be used or not. If `True`, then
            intermediate results will be stored in order to avoid recomputing results that can be
            reused by successively called methods. For example, if you call `plot_bias` followed by
            `plot_bias_ranking` and `memoize` is `True`, then the intermediate results required by
            `plot_bias` will be reused for `plot_bias_ranking`. Memoization is turned off by
            default because it can lead to unexpected behavior depending on your usage.
    &#34;&#34;&#34;

    def __init__(
        self,
        alpha=0.05,
        n_taus=41,
        lambda_iterations=5,
        n_jobs=1,  # Parallelism is only worth it if the dataset is &#34;large&#34;
        verbose=False,
        n_samples=1,
        sample_frac=0.8,
        conf_level=0.05,
        memoize=False,
    ):
        if not 0 &lt;= alpha &lt; 0.5:
            raise ValueError(&#34;alpha must be between 0 and 0.5, got &#34; f&#34;{alpha}&#34;)

        if not n_taus &gt; 0:
            raise ValueError(
                &#34;n_taus must be a strictly positive integer, got &#34; f&#34;{n_taus}&#34;
            )

        if not lambda_iterations &gt; 0:
            raise ValueError(
                &#34;lambda_iterations must be a strictly positive &#34;
                f&#34;integer, got {lambda_iterations}&#34;
            )

        if n_samples &lt; 1:
            raise ValueError(f&#34;n_samples must be strictly positive, got {n_samples}&#34;)

        if not 0 &lt; sample_frac &lt; 1:
            raise ValueError(f&#34;sample_frac must be between 0 and 1, got {sample_frac}&#34;)

        if not 0 &lt; conf_level &lt; 0.5:
            raise ValueError(f&#34;conf_level must be between 0 and 0.5, got {conf_level}&#34;)

        self.alpha = alpha
        self.n_taus = n_taus
        self.lambda_iterations = lambda_iterations
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.metric_names = set()
        self.n_samples = n_samples
        self.sample_frac = sample_frac if n_samples &gt; 1 else 1
        self.conf_level = conf_level
        self.memoize = memoize
        self._reset_info()

    def _reset_info(self):
        &#34;&#34;&#34;Resets the info dataframe (for when memoization is turned off).&#34;&#34;&#34;
        self.info = pd.DataFrame(
            columns=[
                &#34;feature&#34;,
                &#34;tau&#34;,
                &#34;value&#34;,
                &#34;lambda&#34;,
                &#34;label&#34;,
                &#34;bias&#34;,
                &#34;bias_low&#34;,
                &#34;bias_high&#34;,
            ]
        )

    def get_metric_name(self, metric):
        &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
        performance with respect of the given metric.

        Args:
            metric (callable): The metric to compute the model&#39;s performance.

        Returns:
            str: The name of the column.
        &#34;&#34;&#34;
        name = metric.__name__
        if name in self.info.columns and name not in self.metric_names:
            raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
        return name

    @property
    def taus(self):
        tau_precision = 2 / (self.n_taus - 1)
        return list(decimal_range(-1, 1, tau_precision))

    @property
    def features(self):
        return self.info[&#34;feature&#34;].unique().tolist()

    def _determine_pairs_to_do(self, features, labels):
        to_do_pairs = (
            set(itertools.product(features, labels)) -
            set(self.info.groupby([&#34;feature&#34;, &#34;label&#34;]).groups.keys())
        )
        to_do_map = collections.defaultdict(list)
        for feat, label in to_do_pairs:
            to_do_map[feat].append(label)
        return {feat: list(sorted(labels)) for feat, labels in to_do_map.items()}

    def _find_lambdas(self, X_test, y_pred):
        &#34;&#34;&#34;Finds lambda values for each (feature, tau, label) triplet.

        1. A list of $\tau$ values is generated using `n_taus`. The $\tau$ values range from -1 to 1.
        2. A grid of $\eps$ values is generated for each $\tau$ and for each variable. Each $\eps$ represents a shift from a variable&#39;s mean towards a particular quantile.
        3. A grid of $\lambda$ values is generated for each $\eps$. Each $\lambda$ corresponds to the optimal parameter that has to be used to weight the observations in order for the average to reach the associated $\eps$ shift.

        Args:
            X_test (`pandas.DataFrame` or `numpy.ndarray`):
            y_pred (`pandas.DataFrame` or `numpy.ndarray`):
        &#34;&#34;&#34;

        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))

        # One-hot encode the categorical features
        X_test = pd.get_dummies(data=X_test, prefix_sep=CAT_COL_SEP)

        # Check which (feature, label) pairs have to be done
        to_do_map = self._determine_pairs_to_do(features=X_test.columns, labels=y_pred.columns)
        # We need a list to keep the order of X_test
        to_do_features = list(feat for feat in X_test.columns if feat in to_do_map)
        X_test = X_test[to_do_features]

        if X_test.empty:
            return self

        # Make the epsilons for each (feature, label, tau) triplet
        q_mins = X_test.quantile(q=self.alpha).to_dict()
        q_maxs = X_test.quantile(q=1 - self.alpha).to_dict()
        means = X_test.mean().to_dict()
        additional_info = pd.concat(
            [
                pd.DataFrame(
                    {
                        &#34;tau&#34;: self.taus,
                        &#34;value&#34;: [
                            means[feature]
                            + tau
                            * (
                                max(means[feature] - q_mins[feature], 0)
                                if tau &lt; 0 else
                                max(q_maxs[feature] - means[feature], 0)
                            )
                            for tau in self.taus
                        ],
                        &#34;feature&#34;: [feature] * len(self.taus),
                        &#34;label&#34;: [label] * len(self.taus),
                    }
                )
                # We need to iterate over `to_do_features` to keep the order of X_test
                for feature in to_do_features
                for label in to_do_map[feature]
            ],
            ignore_index=True,
        )
        self.info = self.info.append(additional_info, ignore_index=True, sort=False)

        # Find a lambda for each (feature, espilon) pair
        lambdas = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            joblib.delayed(compute_lambdas)(
                x=X_test[feature],
                target_means=part[&#34;value&#34;].unique(),
                iterations=self.lambda_iterations
            )
            for feature, part in self.info.groupby(&#34;feature&#34;)
            if feature in X_test
        )
        lambdas = dict(collections.ChainMap(*lambdas))
        self.info[&#34;lambda&#34;] = self.info.apply(
            lambda r: lambdas.get((r[&#34;feature&#34;], r[&#34;value&#34;]), r[&#34;lambda&#34;]),
            axis=&#34;columns&#34;,
        )
        self.info[&#34;lambda&#34;] = self.info[&#34;lambda&#34;].fillna(0.0)

        return self

    def _explain(self, X_test, y_pred, dest_col, key_cols, compute):

        # Reset info if memoization is turned off
        if not self.memoize:
            self._reset_info()
        if dest_col not in self.info.columns:
            self.info[dest_col] = None
            self.info[f&#34;{dest_col}_low&#34;] = None
            self.info[f&#34;{dest_col}_high&#34;] = None

        # Coerce X_test and y_pred to DataFrames
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))

        # Check X_test and y_pred okay
        if len(X_test) != len(y_pred):
            raise ValueError(&#39;X_test and y_pred are not of the same length&#39;)

        # Find the lambda values for each (feature, tau, label) triplet
        self._find_lambdas(X_test, y_pred)

        # One-hot encode the categorical variables
        X_test = pd.get_dummies(data=X_test, prefix_sep=CAT_COL_SEP)

        # Determine which features are missing explanations; that is they have null biases for at
        # least one lambda value
        relevant = self.info[
            self.info[&#34;feature&#34;].isin(X_test.columns)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
            &amp; self.info[dest_col].isnull()
        ]

        if not relevant.empty:
            # `compute()` will return something like:
            # [
            #   [ # First batch
            #     (*key_cols1, sample_index1, computed_value1),
            #     (*key_cols2, sample_index2, computed_value2),
            #     ...
            #   ],
            #   ...
            # ]
            data = compute(X_test=X_test, y_pred=y_pred, relevant=relevant)
            data = pd.DataFrame(
                [line for batch in data for line in batch],
                columns=[*key_cols, &#34;sample_index&#34;, dest_col],
            )
            # We group by the key to gather the samples and compute the confidence
            #  interval
            data = data.groupby(key_cols)[dest_col].agg(
                [
                    # Mean bias
                    (dest_col, &#34;mean&#34;),
                    # Lower bound on the mean bias
                    (
                        f&#34;{dest_col}_low&#34;,
                        functools.partial(np.quantile, q=self.conf_level),
                    ),
                    # Upper bound on the mean bias
                    (
                        f&#34;{dest_col}_high&#34;,
                        functools.partial(np.quantile, q=1 - self.conf_level),
                    ),
                ]
            )

            #  TODO: merge all columns in a single operation?
            for col in data.columns:
                self.info[col] = self.info.apply(
                    lambda r: data[col].get(tuple([r[k] for k in key_cols]), r.get(col)),
                    axis=&#34;columns&#34;,
                )

        return self.info[
            self.info[&#34;feature&#34;].isin(X_test.columns)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
        ]

    def explain_bias(self, X_test, y_pred):
        &#34;&#34;&#34;Compute the bias of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): The dataset as a pandas dataframe
                or a 2d numpy array of shape `(n_samples, n_features)`.
            y_pred (pd.DataFrame or list-like object): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                a list-like object (`list`, `np.array`, `pd.Series`...) is expected.
                For multi-label classification, a dataframe or a 2d numpy array with
                one column per label is expected. The values can either be
                probabilities or `0/1` (for a one-hot-encoded output).

        Returns:
            (pd.DataFrame): A dataframe with columns
                `(feature, tau, value, lambda, label, bias, bias_low, bias_high)`.
                If `explainer.n_samples` is `1`, no confidence interval is computed
                and `bias = bias_low = bias_high`. The value of `label` is not
                important for regression.

        Examples:
            Binary classification (see `notebooks/Adult.ipynb`):

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model(X_test)
            &gt;&gt;&gt; y_pred
            [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
            &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)

            Regression is similar to binary classification (see `notebooks/Boston.ipynb`):
            
            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model(X_test)
            &gt;&gt;&gt; y_pred
            [22, 24, 19]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
            &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)

            For multi-label classification, we need a dataframe to store predictions
            (see `notebooks/Iris.ipynb`):
            
            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model(X_test)
            &gt;&gt;&gt; y_pred.columns
            [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
            &gt;&gt;&gt; y_pred.iloc[0]
            [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
            &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)
        &#34;&#34;&#34;
        def compute(X_test, y_pred, relevant):
            return joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
                joblib.delayed(compute_bias)(
                    y_pred=y_pred[label][mask],
                    x=X_test[col][mask],
                    lambdas=part[&#34;lambda&#34;].unique(),
                    sample_index=i,
                )
                for label in y_pred.columns
                for col, part in relevant.groupby(&#34;feature&#34;)
                for i, mask in enumerate(
                    yield_masks(self.n_samples, len(X_test), self.sample_frac)
                )
            )

        return self._explain(
            X_test=X_test,
            y_pred=y_pred,
            dest_col=&#34;bias&#34;,
            key_cols=[&#34;feature&#34;, &#34;label&#34;, &#34;lambda&#34;],
            compute=compute,
        )

    def explain_performance(self, X_test, y_test, y_pred, metric):
        metric_name = self.get_metric_name(metric)
        if metric_name not in self.info.columns:
            self.info[metric_name] = None
            self.info[f&#34;{metric_name}_low&#34;] = None
            self.info[f&#34;{metric_name}_high&#34;] = None
        self.metric_names.add(metric_name)

        y_test = np.asarray(y_test)

        def compute(X_test, y_pred, relevant):
            return joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
                joblib.delayed(compute_performance)(
                    y_test=y_test[mask],
                    y_pred=y_pred[mask],
                    metric=metric,
                    x=X_test[col][mask],
                    lambdas=part[&#34;lambda&#34;].unique(),
                    sample_index=i,
                )
                for col, part in relevant.groupby(&#34;feature&#34;)
                for i, mask in enumerate(
                    yield_masks(self.n_samples, len(X_test), self.sample_frac)
                )
            )

        return self._explain(
            X_test=X_test,
            y_pred=y_pred,
            dest_col=metric_name,
            key_cols=[&#34;feature&#34;, &#34;lambda&#34;],
            compute=compute
        )

    def rank_by_bias(self, X_test, y_pred):
        &#34;&#34;&#34;Returns a DataFrame containing the importance of each feature.

        &#34;&#34;&#34;

        def get_importance(group):
            &#34;&#34;&#34;Computes the average absolute difference in bias changes per tau increase.&#34;&#34;&#34;
            #  Normalize bias to get an importance between 0 and 1
            # bias can be outside [0, 1] for regression
            bias = group[&#34;bias&#34;]
            group[&#34;bias&#34;] = (bias - bias.min()) / (bias.max() - bias.min())
            baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;bias&#34;]
            return (group[&#34;bias&#34;] - baseline).abs().mean()

        return (
            self.explain_bias(X_test=X_test, y_pred=y_pred)
            .groupby([&#34;label&#34;, &#34;feature&#34;])
            .apply(get_importance)
            .to_frame(&#34;importance&#34;)
            .reset_index()
        )

    def rank_by_performance(self, X_test, y_test, y_pred, metric):
        metric_name = self.get_metric_name(metric)

        def get_aggregates(df):
            return pd.Series(
                [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
            )

        return (
            self.explain_performance(X_test, y_test, y_pred, metric)
            .groupby(&#34;feature&#34;)
            .apply(get_aggregates)
            .reset_index()
        )

    def _plot_explanation(self, explanation, col, y_label, colors=None, yrange=None):
        features = explanation[&#34;feature&#34;].unique()

        if colors is None:
            colors = {}
        elif type(colors) is str:
            colors = {feat: colors for feat in features}

        #  There are multiple features, we plot them together with taus
        if len(features) &gt; 1:
            fig = go.Figure()
            for feat in features:
                x = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;tau&#34;]
                y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[col]
                fig.add_trace(
                    go.Scatter(
                        x=x,
                        y=y,
                        mode=&#34;lines+markers&#34;,
                        hoverinfo=&#34;x+y+text&#34;,
                        name=feat,
                        text=[
                            f&#34;{feat} = {val}&#34;
                            for val in explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[
                                &#34;value&#34;
                            ]
                        ],
                        marker=dict(color=colors.get(feat)),
                    )
                )

            fig.update_layout(
                margin=dict(t=50, r=50),
                xaxis=dict(title=&#34;tau&#34;, zeroline=False),
                yaxis=dict(title=y_label, range=yrange, showline=True),
                plot_bgcolor=&#34;white&#34;,
            )
            return fig

        #  There is only one feature, we plot it with its nominal values.
        feat = features[0]
        fig = go.Figure()
        x = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;value&#34;]
        y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[col]
        mean_row = explanation.query(f&#39;feature == &#34;{feat}&#34; and tau == 0&#39;).iloc[0]

        if self.n_samples &gt; 1:
            low = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{col}_low&#34;]
            high = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{col}_high&#34;]
            fig.add_trace(
                go.Scatter(
                    x=np.concatenate((x, x[::-1])),
                    y=np.concatenate((low, high[::-1])),
                    name=f&#34;{self.conf_level * 100}% - {(1 - self.conf_level) * 100}%&#34;,
                    fill=&#34;toself&#34;,
                    fillcolor=&#34;#eee&#34;,  # TODO: same color as mean line?
                    line_color=&#34;rgba(0, 0, 0, 0)&#34;,
                )
            )

        fig.add_trace(
            go.Scatter(
                x=x,
                y=y,
                mode=&#34;lines+markers&#34;,
                hoverinfo=&#34;x+y&#34;,
                showlegend=False,
                marker=dict(color=colors.get(feat)),
            )
        )
        fig.add_trace(
            go.Scatter(
                x=[mean_row[&#34;value&#34;]],
                y=[mean_row[col]],
                mode=&#34;markers&#34;,
                name=&#34;Original mean&#34;,
                hoverinfo=&#34;skip&#34;,
                marker=dict(symbol=&#34;x&#34;, size=9, color=&#34;black&#34;),
            )
        )
        fig.update_layout(
            margin=dict(t=50, r=50),
            xaxis=dict(title=f&#34;Average {feat}&#34;, zeroline=False),
            yaxis=dict(title=y_label, range=yrange, showline=True),
            plot_bgcolor=&#34;white&#34;,
        )
        return fig

    def _plot_ranking(self, ranking, score_column, title, colors=None):
        ranking = ranking.sort_values(by=[score_column])

        return go.Figure(
            data=[
                go.Bar(
                    x=ranking[score_column],
                    y=ranking[&#34;feature&#34;],
                    orientation=&#34;h&#34;,
                    hoverinfo=&#34;x&#34;,
                    marker=dict(color=colors),
                )
            ],
            layout=go.Layout(
                margin=dict(l=200, b=0, t=40),
                xaxis=dict(
                    title=title,
                    range=[0, 1],
                    showline=True,
                    zeroline=False,
                    side=&#34;top&#34;,
                    fixedrange=True,
                ),
                yaxis=dict(showline=True, zeroline=False, fixedrange=True),
                plot_bgcolor=&#34;white&#34;,
            ),
        )

    def plot_bias(self, X_test, y_pred, **fig_kwargs):
        explanation = self.explain_bias(X_test, y_pred)
        labels = explanation[&#34;label&#34;].unique()
        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)
        y_label = f&#39;Average &#34;{labels[0]}&#34;&#39;
        return self._plot_explanation(explanation, &#34;bias&#34;, y_label, **fig_kwargs)

    def plot_bias_ranking(self, X_test, y_pred, **fig_kwargs):
        ranking = self.rank_by_bias(X_test=X_test, y_pred=y_pred)
        return self._plot_ranking(ranking, &#34;importance&#34;, &#34;Importance&#34;, **fig_kwargs)

    def plot_performance(self, X_test, y_test, y_pred, metric, **fig_kwargs):
        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        if fig_kwargs.get(&#34;yrange&#34;) is None:
            if explanation[metric_name].between(0, 1).all():
                fig_kwargs[&#34;yrange&#34;] = [0, 1]

        return self._plot_explanation(
            explanation, metric_name, y_label=f&#34;Average {metric_name}&#34;, **fig_kwargs
        )

    def plot_performance_ranking(
        self, X_test, y_test, y_pred, metric, criterion, **fig_kwargs
    ):
        metric_name = self.get_metric_name(metric)
        ranking = self.rank_by_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        return self._plot_ranking(
            ranking, criterion, f&#34;{criterion} {metric_name}&#34;, **fig_kwargs
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ethik.explainer.Explainer"><code class="flex name class">
<span>class <span class="ident">Explainer</span></span>
<span>(</span><span>alpha=0.05, n_taus=41, lambda_iterations=5, n_jobs=1, verbose=False, n_samples=1, sample_frac=0.8, conf_level=0.05, memoize=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Explains the bias and reliability of model predictions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates by how close the <a title="ethik.explainer.Explainer" href="#ethik.explainer.Explainer"><code>Explainer</code></a>
should look at extreme values of a distribution. The closer to zero, the more so
extreme values will be accounted for. The default is <code>0.05</code> which means that all values
beyond the 5th and 95th quantiles are ignored.</dd>
<dt><strong><code>n_taus</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of τ values to consider. The results will be more fine-grained the
higher this value is. However the computation time increases linearly with <code>n_taus</code>.
The default is <code>41</code> and corresponds to each τ being separated by it's neighbors by
<code>0.05</code>.</dd>
<dt><strong><code>lambda_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of iterations used when applying the Newton step
of the optimization procedure. Default is <code>5</code>.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of jobs to use for parallel computations. See
<code>joblib.Parallel()</code>. Default is <code>-1</code>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Passed to <code>joblib.Parallel()</code> for parallel computations.
Default is <code>False</code>.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples to use for the confidence interval.
If <code>1</code>, the default, no confidence interval is computed.</dd>
<dt><strong><code>sample_frac</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of lines in the dataset sampled to
generate the samples for the confidence interval. If <code>n_samples</code> is
<code>1</code>, no confidence interval is computed and the whole dataset is used.
Default is <code>0.8</code>.</dd>
<dt><strong><code>conf_level</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates the
quantile used for the confidence interval. Default is <code>0.05</code>, which
means that the confidence interval contains the data between the 5th
and 95th quantiles.</dd>
<dt><strong><code>memoize</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether or not memoization should be used or not. If <code>True</code>, then
intermediate results will be stored in order to avoid recomputing results that can be
reused by successively called methods. For example, if you call <code>plot_bias</code> followed by
<code>plot_bias_ranking</code> and <code>memoize</code> is <code>True</code>, then the intermediate results required by
<code>plot_bias</code> will be reused for <code>plot_bias_ranking</code>. Memoization is turned off by
default because it can lead to unexpected behavior depending on your usage.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Explainer:
    &#34;&#34;&#34;Explains the bias and reliability of model predictions.

    Parameters:
        alpha (float): A `float` between `0` and `0.5` which indicates by how close the `Explainer`
            should look at extreme values of a distribution. The closer to zero, the more so
            extreme values will be accounted for. The default is `0.05` which means that all values
            beyond the 5th and 95th quantiles are ignored.
        n_taus (int): The number of τ values to consider. The results will be more fine-grained the
            higher this value is. However the computation time increases linearly with `n_taus`.
            The default is `41` and corresponds to each τ being separated by it&#39;s neighbors by
            `0.05`.
        lambda_iterations (int): The number of iterations used when applying the Newton step
            of the optimization procedure. Default is `5`.
        n_jobs (int): The number of jobs to use for parallel computations. See
            `joblib.Parallel()`. Default is `-1`.
        verbose (bool): Passed to `joblib.Parallel()` for parallel computations.
            Default is `False`.
        n_samples (int): The number of samples to use for the confidence interval.
            If `1`, the default, no confidence interval is computed.
        sample_frac (float): The proportion of lines in the dataset sampled to
            generate the samples for the confidence interval. If `n_samples` is
            `1`, no confidence interval is computed and the whole dataset is used.
            Default is `0.8`.
        conf_level (float): A `float` between `0` and `0.5` which indicates the
            quantile used for the confidence interval. Default is `0.05`, which
            means that the confidence interval contains the data between the 5th
            and 95th quantiles.
        memoize (bool): Indicates whether or not memoization should be used or not. If `True`, then
            intermediate results will be stored in order to avoid recomputing results that can be
            reused by successively called methods. For example, if you call `plot_bias` followed by
            `plot_bias_ranking` and `memoize` is `True`, then the intermediate results required by
            `plot_bias` will be reused for `plot_bias_ranking`. Memoization is turned off by
            default because it can lead to unexpected behavior depending on your usage.
    &#34;&#34;&#34;

    def __init__(
        self,
        alpha=0.05,
        n_taus=41,
        lambda_iterations=5,
        n_jobs=1,  # Parallelism is only worth it if the dataset is &#34;large&#34;
        verbose=False,
        n_samples=1,
        sample_frac=0.8,
        conf_level=0.05,
        memoize=False,
    ):
        if not 0 &lt;= alpha &lt; 0.5:
            raise ValueError(&#34;alpha must be between 0 and 0.5, got &#34; f&#34;{alpha}&#34;)

        if not n_taus &gt; 0:
            raise ValueError(
                &#34;n_taus must be a strictly positive integer, got &#34; f&#34;{n_taus}&#34;
            )

        if not lambda_iterations &gt; 0:
            raise ValueError(
                &#34;lambda_iterations must be a strictly positive &#34;
                f&#34;integer, got {lambda_iterations}&#34;
            )

        if n_samples &lt; 1:
            raise ValueError(f&#34;n_samples must be strictly positive, got {n_samples}&#34;)

        if not 0 &lt; sample_frac &lt; 1:
            raise ValueError(f&#34;sample_frac must be between 0 and 1, got {sample_frac}&#34;)

        if not 0 &lt; conf_level &lt; 0.5:
            raise ValueError(f&#34;conf_level must be between 0 and 0.5, got {conf_level}&#34;)

        self.alpha = alpha
        self.n_taus = n_taus
        self.lambda_iterations = lambda_iterations
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.metric_names = set()
        self.n_samples = n_samples
        self.sample_frac = sample_frac if n_samples &gt; 1 else 1
        self.conf_level = conf_level
        self.memoize = memoize
        self._reset_info()

    def _reset_info(self):
        &#34;&#34;&#34;Resets the info dataframe (for when memoization is turned off).&#34;&#34;&#34;
        self.info = pd.DataFrame(
            columns=[
                &#34;feature&#34;,
                &#34;tau&#34;,
                &#34;value&#34;,
                &#34;lambda&#34;,
                &#34;label&#34;,
                &#34;bias&#34;,
                &#34;bias_low&#34;,
                &#34;bias_high&#34;,
            ]
        )

    def get_metric_name(self, metric):
        &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
        performance with respect of the given metric.

        Args:
            metric (callable): The metric to compute the model&#39;s performance.

        Returns:
            str: The name of the column.
        &#34;&#34;&#34;
        name = metric.__name__
        if name in self.info.columns and name not in self.metric_names:
            raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
        return name

    @property
    def taus(self):
        tau_precision = 2 / (self.n_taus - 1)
        return list(decimal_range(-1, 1, tau_precision))

    @property
    def features(self):
        return self.info[&#34;feature&#34;].unique().tolist()

    def _determine_pairs_to_do(self, features, labels):
        to_do_pairs = (
            set(itertools.product(features, labels)) -
            set(self.info.groupby([&#34;feature&#34;, &#34;label&#34;]).groups.keys())
        )
        to_do_map = collections.defaultdict(list)
        for feat, label in to_do_pairs:
            to_do_map[feat].append(label)
        return {feat: list(sorted(labels)) for feat, labels in to_do_map.items()}

    def _find_lambdas(self, X_test, y_pred):
        &#34;&#34;&#34;Finds lambda values for each (feature, tau, label) triplet.

        1. A list of $\tau$ values is generated using `n_taus`. The $\tau$ values range from -1 to 1.
        2. A grid of $\eps$ values is generated for each $\tau$ and for each variable. Each $\eps$ represents a shift from a variable&#39;s mean towards a particular quantile.
        3. A grid of $\lambda$ values is generated for each $\eps$. Each $\lambda$ corresponds to the optimal parameter that has to be used to weight the observations in order for the average to reach the associated $\eps$ shift.

        Args:
            X_test (`pandas.DataFrame` or `numpy.ndarray`):
            y_pred (`pandas.DataFrame` or `numpy.ndarray`):
        &#34;&#34;&#34;

        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))

        # One-hot encode the categorical features
        X_test = pd.get_dummies(data=X_test, prefix_sep=CAT_COL_SEP)

        # Check which (feature, label) pairs have to be done
        to_do_map = self._determine_pairs_to_do(features=X_test.columns, labels=y_pred.columns)
        # We need a list to keep the order of X_test
        to_do_features = list(feat for feat in X_test.columns if feat in to_do_map)
        X_test = X_test[to_do_features]

        if X_test.empty:
            return self

        # Make the epsilons for each (feature, label, tau) triplet
        q_mins = X_test.quantile(q=self.alpha).to_dict()
        q_maxs = X_test.quantile(q=1 - self.alpha).to_dict()
        means = X_test.mean().to_dict()
        additional_info = pd.concat(
            [
                pd.DataFrame(
                    {
                        &#34;tau&#34;: self.taus,
                        &#34;value&#34;: [
                            means[feature]
                            + tau
                            * (
                                max(means[feature] - q_mins[feature], 0)
                                if tau &lt; 0 else
                                max(q_maxs[feature] - means[feature], 0)
                            )
                            for tau in self.taus
                        ],
                        &#34;feature&#34;: [feature] * len(self.taus),
                        &#34;label&#34;: [label] * len(self.taus),
                    }
                )
                # We need to iterate over `to_do_features` to keep the order of X_test
                for feature in to_do_features
                for label in to_do_map[feature]
            ],
            ignore_index=True,
        )
        self.info = self.info.append(additional_info, ignore_index=True, sort=False)

        # Find a lambda for each (feature, espilon) pair
        lambdas = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            joblib.delayed(compute_lambdas)(
                x=X_test[feature],
                target_means=part[&#34;value&#34;].unique(),
                iterations=self.lambda_iterations
            )
            for feature, part in self.info.groupby(&#34;feature&#34;)
            if feature in X_test
        )
        lambdas = dict(collections.ChainMap(*lambdas))
        self.info[&#34;lambda&#34;] = self.info.apply(
            lambda r: lambdas.get((r[&#34;feature&#34;], r[&#34;value&#34;]), r[&#34;lambda&#34;]),
            axis=&#34;columns&#34;,
        )
        self.info[&#34;lambda&#34;] = self.info[&#34;lambda&#34;].fillna(0.0)

        return self

    def _explain(self, X_test, y_pred, dest_col, key_cols, compute):

        # Reset info if memoization is turned off
        if not self.memoize:
            self._reset_info()
        if dest_col not in self.info.columns:
            self.info[dest_col] = None
            self.info[f&#34;{dest_col}_low&#34;] = None
            self.info[f&#34;{dest_col}_high&#34;] = None

        # Coerce X_test and y_pred to DataFrames
        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))

        # Check X_test and y_pred okay
        if len(X_test) != len(y_pred):
            raise ValueError(&#39;X_test and y_pred are not of the same length&#39;)

        # Find the lambda values for each (feature, tau, label) triplet
        self._find_lambdas(X_test, y_pred)

        # One-hot encode the categorical variables
        X_test = pd.get_dummies(data=X_test, prefix_sep=CAT_COL_SEP)

        # Determine which features are missing explanations; that is they have null biases for at
        # least one lambda value
        relevant = self.info[
            self.info[&#34;feature&#34;].isin(X_test.columns)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
            &amp; self.info[dest_col].isnull()
        ]

        if not relevant.empty:
            # `compute()` will return something like:
            # [
            #   [ # First batch
            #     (*key_cols1, sample_index1, computed_value1),
            #     (*key_cols2, sample_index2, computed_value2),
            #     ...
            #   ],
            #   ...
            # ]
            data = compute(X_test=X_test, y_pred=y_pred, relevant=relevant)
            data = pd.DataFrame(
                [line for batch in data for line in batch],
                columns=[*key_cols, &#34;sample_index&#34;, dest_col],
            )
            # We group by the key to gather the samples and compute the confidence
            #  interval
            data = data.groupby(key_cols)[dest_col].agg(
                [
                    # Mean bias
                    (dest_col, &#34;mean&#34;),
                    # Lower bound on the mean bias
                    (
                        f&#34;{dest_col}_low&#34;,
                        functools.partial(np.quantile, q=self.conf_level),
                    ),
                    # Upper bound on the mean bias
                    (
                        f&#34;{dest_col}_high&#34;,
                        functools.partial(np.quantile, q=1 - self.conf_level),
                    ),
                ]
            )

            #  TODO: merge all columns in a single operation?
            for col in data.columns:
                self.info[col] = self.info.apply(
                    lambda r: data[col].get(tuple([r[k] for k in key_cols]), r.get(col)),
                    axis=&#34;columns&#34;,
                )

        return self.info[
            self.info[&#34;feature&#34;].isin(X_test.columns)
            &amp; self.info[&#34;label&#34;].isin(y_pred.columns)
        ]

    def explain_bias(self, X_test, y_pred):
        &#34;&#34;&#34;Compute the bias of the model for the features in `X_test`.

        Args:
            X_test (pd.DataFrame or np.array): The dataset as a pandas dataframe
                or a 2d numpy array of shape `(n_samples, n_features)`.
            y_pred (pd.DataFrame or list-like object): The model predictions
                for the samples in `X_test`. For binary classification and regression,
                a list-like object (`list`, `np.array`, `pd.Series`...) is expected.
                For multi-label classification, a dataframe or a 2d numpy array with
                one column per label is expected. The values can either be
                probabilities or `0/1` (for a one-hot-encoded output).

        Returns:
            (pd.DataFrame): A dataframe with columns
                `(feature, tau, value, lambda, label, bias, bias_low, bias_high)`.
                If `explainer.n_samples` is `1`, no confidence interval is computed
                and `bias = bias_low = bias_high`. The value of `label` is not
                important for regression.

        Examples:
            Binary classification (see `notebooks/Adult.ipynb`):

            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model(X_test)
            &gt;&gt;&gt; y_pred
            [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
            &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)

            Regression is similar to binary classification (see `notebooks/Boston.ipynb`):
            
            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model(X_test)
            &gt;&gt;&gt; y_pred
            [22, 24, 19]
            &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
            &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
            &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)

            For multi-label classification, we need a dataframe to store predictions
            (see `notebooks/Iris.ipynb`):
            
            &gt;&gt;&gt; X_test = pd.DataFrame([
            ...     [1, 2],
            ...     [1.1, 2.2],
            ...     [1.3, 2.3],
            ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
            &gt;&gt;&gt; y_pred = model(X_test)
            &gt;&gt;&gt; y_pred.columns
            [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
            &gt;&gt;&gt; y_pred.iloc[0]
            [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
            &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)
        &#34;&#34;&#34;
        def compute(X_test, y_pred, relevant):
            return joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
                joblib.delayed(compute_bias)(
                    y_pred=y_pred[label][mask],
                    x=X_test[col][mask],
                    lambdas=part[&#34;lambda&#34;].unique(),
                    sample_index=i,
                )
                for label in y_pred.columns
                for col, part in relevant.groupby(&#34;feature&#34;)
                for i, mask in enumerate(
                    yield_masks(self.n_samples, len(X_test), self.sample_frac)
                )
            )

        return self._explain(
            X_test=X_test,
            y_pred=y_pred,
            dest_col=&#34;bias&#34;,
            key_cols=[&#34;feature&#34;, &#34;label&#34;, &#34;lambda&#34;],
            compute=compute,
        )

    def explain_performance(self, X_test, y_test, y_pred, metric):
        metric_name = self.get_metric_name(metric)
        if metric_name not in self.info.columns:
            self.info[metric_name] = None
            self.info[f&#34;{metric_name}_low&#34;] = None
            self.info[f&#34;{metric_name}_high&#34;] = None
        self.metric_names.add(metric_name)

        y_test = np.asarray(y_test)

        def compute(X_test, y_pred, relevant):
            return joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
                joblib.delayed(compute_performance)(
                    y_test=y_test[mask],
                    y_pred=y_pred[mask],
                    metric=metric,
                    x=X_test[col][mask],
                    lambdas=part[&#34;lambda&#34;].unique(),
                    sample_index=i,
                )
                for col, part in relevant.groupby(&#34;feature&#34;)
                for i, mask in enumerate(
                    yield_masks(self.n_samples, len(X_test), self.sample_frac)
                )
            )

        return self._explain(
            X_test=X_test,
            y_pred=y_pred,
            dest_col=metric_name,
            key_cols=[&#34;feature&#34;, &#34;lambda&#34;],
            compute=compute
        )

    def rank_by_bias(self, X_test, y_pred):
        &#34;&#34;&#34;Returns a DataFrame containing the importance of each feature.

        &#34;&#34;&#34;

        def get_importance(group):
            &#34;&#34;&#34;Computes the average absolute difference in bias changes per tau increase.&#34;&#34;&#34;
            #  Normalize bias to get an importance between 0 and 1
            # bias can be outside [0, 1] for regression
            bias = group[&#34;bias&#34;]
            group[&#34;bias&#34;] = (bias - bias.min()) / (bias.max() - bias.min())
            baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;bias&#34;]
            return (group[&#34;bias&#34;] - baseline).abs().mean()

        return (
            self.explain_bias(X_test=X_test, y_pred=y_pred)
            .groupby([&#34;label&#34;, &#34;feature&#34;])
            .apply(get_importance)
            .to_frame(&#34;importance&#34;)
            .reset_index()
        )

    def rank_by_performance(self, X_test, y_test, y_pred, metric):
        metric_name = self.get_metric_name(metric)

        def get_aggregates(df):
            return pd.Series(
                [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
            )

        return (
            self.explain_performance(X_test, y_test, y_pred, metric)
            .groupby(&#34;feature&#34;)
            .apply(get_aggregates)
            .reset_index()
        )

    def _plot_explanation(self, explanation, col, y_label, colors=None, yrange=None):
        features = explanation[&#34;feature&#34;].unique()

        if colors is None:
            colors = {}
        elif type(colors) is str:
            colors = {feat: colors for feat in features}

        #  There are multiple features, we plot them together with taus
        if len(features) &gt; 1:
            fig = go.Figure()
            for feat in features:
                x = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;tau&#34;]
                y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[col]
                fig.add_trace(
                    go.Scatter(
                        x=x,
                        y=y,
                        mode=&#34;lines+markers&#34;,
                        hoverinfo=&#34;x+y+text&#34;,
                        name=feat,
                        text=[
                            f&#34;{feat} = {val}&#34;
                            for val in explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[
                                &#34;value&#34;
                            ]
                        ],
                        marker=dict(color=colors.get(feat)),
                    )
                )

            fig.update_layout(
                margin=dict(t=50, r=50),
                xaxis=dict(title=&#34;tau&#34;, zeroline=False),
                yaxis=dict(title=y_label, range=yrange, showline=True),
                plot_bgcolor=&#34;white&#34;,
            )
            return fig

        #  There is only one feature, we plot it with its nominal values.
        feat = features[0]
        fig = go.Figure()
        x = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[&#34;value&#34;]
        y = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[col]
        mean_row = explanation.query(f&#39;feature == &#34;{feat}&#34; and tau == 0&#39;).iloc[0]

        if self.n_samples &gt; 1:
            low = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{col}_low&#34;]
            high = explanation.query(f&#39;feature == &#34;{feat}&#34;&#39;)[f&#34;{col}_high&#34;]
            fig.add_trace(
                go.Scatter(
                    x=np.concatenate((x, x[::-1])),
                    y=np.concatenate((low, high[::-1])),
                    name=f&#34;{self.conf_level * 100}% - {(1 - self.conf_level) * 100}%&#34;,
                    fill=&#34;toself&#34;,
                    fillcolor=&#34;#eee&#34;,  # TODO: same color as mean line?
                    line_color=&#34;rgba(0, 0, 0, 0)&#34;,
                )
            )

        fig.add_trace(
            go.Scatter(
                x=x,
                y=y,
                mode=&#34;lines+markers&#34;,
                hoverinfo=&#34;x+y&#34;,
                showlegend=False,
                marker=dict(color=colors.get(feat)),
            )
        )
        fig.add_trace(
            go.Scatter(
                x=[mean_row[&#34;value&#34;]],
                y=[mean_row[col]],
                mode=&#34;markers&#34;,
                name=&#34;Original mean&#34;,
                hoverinfo=&#34;skip&#34;,
                marker=dict(symbol=&#34;x&#34;, size=9, color=&#34;black&#34;),
            )
        )
        fig.update_layout(
            margin=dict(t=50, r=50),
            xaxis=dict(title=f&#34;Average {feat}&#34;, zeroline=False),
            yaxis=dict(title=y_label, range=yrange, showline=True),
            plot_bgcolor=&#34;white&#34;,
        )
        return fig

    def _plot_ranking(self, ranking, score_column, title, colors=None):
        ranking = ranking.sort_values(by=[score_column])

        return go.Figure(
            data=[
                go.Bar(
                    x=ranking[score_column],
                    y=ranking[&#34;feature&#34;],
                    orientation=&#34;h&#34;,
                    hoverinfo=&#34;x&#34;,
                    marker=dict(color=colors),
                )
            ],
            layout=go.Layout(
                margin=dict(l=200, b=0, t=40),
                xaxis=dict(
                    title=title,
                    range=[0, 1],
                    showline=True,
                    zeroline=False,
                    side=&#34;top&#34;,
                    fixedrange=True,
                ),
                yaxis=dict(showline=True, zeroline=False, fixedrange=True),
                plot_bgcolor=&#34;white&#34;,
            ),
        )

    def plot_bias(self, X_test, y_pred, **fig_kwargs):
        explanation = self.explain_bias(X_test, y_pred)
        labels = explanation[&#34;label&#34;].unique()
        if len(labels) &gt; 1:
            raise ValueError(&#34;Cannot plot multiple labels&#34;)
        y_label = f&#39;Average &#34;{labels[0]}&#34;&#39;
        return self._plot_explanation(explanation, &#34;bias&#34;, y_label, **fig_kwargs)

    def plot_bias_ranking(self, X_test, y_pred, **fig_kwargs):
        ranking = self.rank_by_bias(X_test=X_test, y_pred=y_pred)
        return self._plot_ranking(ranking, &#34;importance&#34;, &#34;Importance&#34;, **fig_kwargs)

    def plot_performance(self, X_test, y_test, y_pred, metric, **fig_kwargs):
        metric_name = self.get_metric_name(metric)
        explanation = self.explain_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        if fig_kwargs.get(&#34;yrange&#34;) is None:
            if explanation[metric_name].between(0, 1).all():
                fig_kwargs[&#34;yrange&#34;] = [0, 1]

        return self._plot_explanation(
            explanation, metric_name, y_label=f&#34;Average {metric_name}&#34;, **fig_kwargs
        )

    def plot_performance_ranking(
        self, X_test, y_test, y_pred, metric, criterion, **fig_kwargs
    ):
        metric_name = self.get_metric_name(metric)
        ranking = self.rank_by_performance(
            X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
        )
        return self._plot_ranking(
            ranking, criterion, f&#34;{criterion} {metric_name}&#34;, **fig_kwargs
        )</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ethik.classification_explainer.ClassificationExplainer" href="classification_explainer.html#ethik.classification_explainer.ClassificationExplainer">ClassificationExplainer</a></li>
<li><a title="ethik.regression_explainer.RegressionExplainer" href="regression_explainer.html#ethik.regression_explainer.RegressionExplainer">RegressionExplainer</a></li>
<li><a title="ethik.image_classification_explainer.ImageClassificationExplainer" href="image_classification_explainer.html#ethik.image_classification_explainer.ImageClassificationExplainer">ImageClassificationExplainer</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="ethik.explainer.Explainer.features"><code class="name">var <span class="ident">features</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def features(self):
    return self.info[&#34;feature&#34;].unique().tolist()</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.taus"><code class="name">var <span class="ident">taus</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def taus(self):
    tau_precision = 2 / (self.n_taus - 1)
    return list(decimal_range(-1, 1, tau_precision))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ethik.explainer.Explainer.explain_bias"><code class="name flex">
<span>def <span class="ident">explain_bias</span></span>(<span>self, X_test, y_pred)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute the bias of the model for the features in <code>X_test</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.array</code></dt>
<dd>The dataset as a pandas dataframe
or a 2d numpy array of shape <code>(n_samples, n_features)</code>.</dd>
</dl>
<p>y_pred (pd.DataFrame or list-like object): The model predictions
for the samples in <code>X_test</code>. For binary classification and regression,
a list-like object (<code>list</code>, <code>np.array</code>, <code>pd.Series</code>&hellip;) is expected.
For multi-label classification, a dataframe or a 2d numpy array with
one column per label is expected. The values can either be
probabilities or <code>0/1</code> (for a one-hot-encoded output).</p>
<h2 id="returns">Returns</h2>
<p>(pd.DataFrame): A dataframe with columns
<code>(feature, tau, value, lambda, label, bias, bias_low, bias_high)</code>.
If <code>explainer.n_samples</code> is <code>1</code>, no confidence interval is computed
and <code>bias = bias_low = bias_high</code>. The value of <code>label</code> is not
important for regression.</p>
<h2 id="examples">Examples</h2>
<p>Binary classification (see <code>notebooks/Adult.ipynb</code>):</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model(X_test)
&gt;&gt;&gt; y_pred
[0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
&gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
&gt;&gt;&gt; y_pred = pd.Series(y_pred, name="is_reliable")
&gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)
</code></pre>
<p>Regression is similar to binary classification (see <code>notebooks/Boston.ipynb</code>):</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model(X_test)
&gt;&gt;&gt; y_pred
[22, 24, 19]
&gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
&gt;&gt;&gt; y_pred = pd.Series(y_pred, name="price")
&gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)
</code></pre>
<p>For multi-label classification, we need a dataframe to store predictions
(see <code>notebooks/Iris.ipynb</code>):</p>
<pre><code>&gt;&gt;&gt; X_test = pd.DataFrame([
...     [1, 2],
...     [1.1, 2.2],
...     [1.3, 2.3],
... ], columns=["x0", "x1"])
&gt;&gt;&gt; y_pred = model(X_test)
&gt;&gt;&gt; y_pred.columns
["class0", "class1", "class2"]
&gt;&gt;&gt; y_pred.iloc[0]
[0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
&gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def explain_bias(self, X_test, y_pred):
    &#34;&#34;&#34;Compute the bias of the model for the features in `X_test`.

    Args:
        X_test (pd.DataFrame or np.array): The dataset as a pandas dataframe
            or a 2d numpy array of shape `(n_samples, n_features)`.
        y_pred (pd.DataFrame or list-like object): The model predictions
            for the samples in `X_test`. For binary classification and regression,
            a list-like object (`list`, `np.array`, `pd.Series`...) is expected.
            For multi-label classification, a dataframe or a 2d numpy array with
            one column per label is expected. The values can either be
            probabilities or `0/1` (for a one-hot-encoded output).

    Returns:
        (pd.DataFrame): A dataframe with columns
            `(feature, tau, value, lambda, label, bias, bias_low, bias_high)`.
            If `explainer.n_samples` is `1`, no confidence interval is computed
            and `bias = bias_low = bias_high`. The value of `label` is not
            important for regression.

    Examples:
        Binary classification (see `notebooks/Adult.ipynb`):

        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model(X_test)
        &gt;&gt;&gt; y_pred
        [0, 1, 1]  # Can also be probabilities: [0.3, 0.65, 0.8]
        &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
        &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;is_reliable&#34;)
        &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)

        Regression is similar to binary classification (see `notebooks/Boston.ipynb`):
        
        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model(X_test)
        &gt;&gt;&gt; y_pred
        [22, 24, 19]
        &gt;&gt;&gt; # For readibility reasons, we give a name to the predictions
        &gt;&gt;&gt; y_pred = pd.Series(y_pred, name=&#34;price&#34;)
        &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)

        For multi-label classification, we need a dataframe to store predictions
        (see `notebooks/Iris.ipynb`):
        
        &gt;&gt;&gt; X_test = pd.DataFrame([
        ...     [1, 2],
        ...     [1.1, 2.2],
        ...     [1.3, 2.3],
        ... ], columns=[&#34;x0&#34;, &#34;x1&#34;])
        &gt;&gt;&gt; y_pred = model(X_test)
        &gt;&gt;&gt; y_pred.columns
        [&#34;class0&#34;, &#34;class1&#34;, &#34;class2&#34;]
        &gt;&gt;&gt; y_pred.iloc[0]
        [0, 1, 0] # One-hot encoded, or probabilities: [0.15, 0.6, 0.25]
        &gt;&gt;&gt; explainer.explain_bias(X_test, y_pred)
    &#34;&#34;&#34;
    def compute(X_test, y_pred, relevant):
        return joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            joblib.delayed(compute_bias)(
                y_pred=y_pred[label][mask],
                x=X_test[col][mask],
                lambdas=part[&#34;lambda&#34;].unique(),
                sample_index=i,
            )
            for label in y_pred.columns
            for col, part in relevant.groupby(&#34;feature&#34;)
            for i, mask in enumerate(
                yield_masks(self.n_samples, len(X_test), self.sample_frac)
            )
        )

    return self._explain(
        X_test=X_test,
        y_pred=y_pred,
        dest_col=&#34;bias&#34;,
        key_cols=[&#34;feature&#34;, &#34;label&#34;, &#34;lambda&#34;],
        compute=compute,
    )</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.explain_performance"><code class="name flex">
<span>def <span class="ident">explain_performance</span></span>(<span>self, X_test, y_test, y_pred, metric)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def explain_performance(self, X_test, y_test, y_pred, metric):
    metric_name = self.get_metric_name(metric)
    if metric_name not in self.info.columns:
        self.info[metric_name] = None
        self.info[f&#34;{metric_name}_low&#34;] = None
        self.info[f&#34;{metric_name}_high&#34;] = None
    self.metric_names.add(metric_name)

    y_test = np.asarray(y_test)

    def compute(X_test, y_pred, relevant):
        return joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            joblib.delayed(compute_performance)(
                y_test=y_test[mask],
                y_pred=y_pred[mask],
                metric=metric,
                x=X_test[col][mask],
                lambdas=part[&#34;lambda&#34;].unique(),
                sample_index=i,
            )
            for col, part in relevant.groupby(&#34;feature&#34;)
            for i, mask in enumerate(
                yield_masks(self.n_samples, len(X_test), self.sample_frac)
            )
        )

    return self._explain(
        X_test=X_test,
        y_pred=y_pred,
        dest_col=metric_name,
        key_cols=[&#34;feature&#34;, &#34;lambda&#34;],
        compute=compute
    )</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.get_metric_name"><code class="name flex">
<span>def <span class="ident">get_metric_name</span></span>(<span>self, metric)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the name of the column in explainer's info dataframe to store the
performance with respect of the given metric.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metric</code></strong> :&ensp;<code>callable</code></dt>
<dd>The metric to compute the model's performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>str</code></strong></dt>
<dd>The name of the column.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_metric_name(self, metric):
    &#34;&#34;&#34;Get the name of the column in explainer&#39;s info dataframe to store the
    performance with respect of the given metric.

    Args:
        metric (callable): The metric to compute the model&#39;s performance.

    Returns:
        str: The name of the column.
    &#34;&#34;&#34;
    name = metric.__name__
    if name in self.info.columns and name not in self.metric_names:
        raise ValueError(f&#34;Cannot use {name} as a metric name&#34;)
    return name</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.plot_bias"><code class="name flex">
<span>def <span class="ident">plot_bias</span></span>(<span>self, X_test, y_pred, **fig_kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_bias(self, X_test, y_pred, **fig_kwargs):
    explanation = self.explain_bias(X_test, y_pred)
    labels = explanation[&#34;label&#34;].unique()
    if len(labels) &gt; 1:
        raise ValueError(&#34;Cannot plot multiple labels&#34;)
    y_label = f&#39;Average &#34;{labels[0]}&#34;&#39;
    return self._plot_explanation(explanation, &#34;bias&#34;, y_label, **fig_kwargs)</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.plot_bias_ranking"><code class="name flex">
<span>def <span class="ident">plot_bias_ranking</span></span>(<span>self, X_test, y_pred, **fig_kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_bias_ranking(self, X_test, y_pred, **fig_kwargs):
    ranking = self.rank_by_bias(X_test=X_test, y_pred=y_pred)
    return self._plot_ranking(ranking, &#34;importance&#34;, &#34;Importance&#34;, **fig_kwargs)</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.plot_performance"><code class="name flex">
<span>def <span class="ident">plot_performance</span></span>(<span>self, X_test, y_test, y_pred, metric, **fig_kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_performance(self, X_test, y_test, y_pred, metric, **fig_kwargs):
    metric_name = self.get_metric_name(metric)
    explanation = self.explain_performance(
        X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
    )
    if fig_kwargs.get(&#34;yrange&#34;) is None:
        if explanation[metric_name].between(0, 1).all():
            fig_kwargs[&#34;yrange&#34;] = [0, 1]

    return self._plot_explanation(
        explanation, metric_name, y_label=f&#34;Average {metric_name}&#34;, **fig_kwargs
    )</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.plot_performance_ranking"><code class="name flex">
<span>def <span class="ident">plot_performance_ranking</span></span>(<span>self, X_test, y_test, y_pred, metric, criterion, **fig_kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_performance_ranking(
    self, X_test, y_test, y_pred, metric, criterion, **fig_kwargs
):
    metric_name = self.get_metric_name(metric)
    ranking = self.rank_by_performance(
        X_test=X_test, y_test=y_test, y_pred=y_pred, metric=metric
    )
    return self._plot_ranking(
        ranking, criterion, f&#34;{criterion} {metric_name}&#34;, **fig_kwargs
    )</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.rank_by_bias"><code class="name flex">
<span>def <span class="ident">rank_by_bias</span></span>(<span>self, X_test, y_pred)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a DataFrame containing the importance of each feature.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def rank_by_bias(self, X_test, y_pred):
    &#34;&#34;&#34;Returns a DataFrame containing the importance of each feature.

    &#34;&#34;&#34;

    def get_importance(group):
        &#34;&#34;&#34;Computes the average absolute difference in bias changes per tau increase.&#34;&#34;&#34;
        #  Normalize bias to get an importance between 0 and 1
        # bias can be outside [0, 1] for regression
        bias = group[&#34;bias&#34;]
        group[&#34;bias&#34;] = (bias - bias.min()) / (bias.max() - bias.min())
        baseline = group.query(&#34;tau == 0&#34;).iloc[0][&#34;bias&#34;]
        return (group[&#34;bias&#34;] - baseline).abs().mean()

    return (
        self.explain_bias(X_test=X_test, y_pred=y_pred)
        .groupby([&#34;label&#34;, &#34;feature&#34;])
        .apply(get_importance)
        .to_frame(&#34;importance&#34;)
        .reset_index()
    )</code></pre>
</details>
</dd>
<dt id="ethik.explainer.Explainer.rank_by_performance"><code class="name flex">
<span>def <span class="ident">rank_by_performance</span></span>(<span>self, X_test, y_test, y_pred, metric)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def rank_by_performance(self, X_test, y_test, y_pred, metric):
    metric_name = self.get_metric_name(metric)

    def get_aggregates(df):
        return pd.Series(
            [df[metric_name].min(), df[metric_name].max()], index=[&#34;min&#34;, &#34;max&#34;]
        )

    return (
        self.explain_performance(X_test, y_test, y_pred, metric)
        .groupby(&#34;feature&#34;)
        .apply(get_aggregates)
        .reset_index()
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ethik" href="index.html">ethik</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ethik.explainer.Explainer" href="#ethik.explainer.Explainer">Explainer</a></code></h4>
<ul class="">
<li><code><a title="ethik.explainer.Explainer.explain_bias" href="#ethik.explainer.Explainer.explain_bias">explain_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.explain_performance" href="#ethik.explainer.Explainer.explain_performance">explain_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.features" href="#ethik.explainer.Explainer.features">features</a></code></li>
<li><code><a title="ethik.explainer.Explainer.get_metric_name" href="#ethik.explainer.Explainer.get_metric_name">get_metric_name</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias" href="#ethik.explainer.Explainer.plot_bias">plot_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias_ranking" href="#ethik.explainer.Explainer.plot_bias_ranking">plot_bias_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance" href="#ethik.explainer.Explainer.plot_performance">plot_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance_ranking" href="#ethik.explainer.Explainer.plot_performance_ranking">plot_performance_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.rank_by_bias" href="#ethik.explainer.Explainer.rank_by_bias">rank_by_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.rank_by_performance" href="#ethik.explainer.Explainer.rank_by_performance">rank_by_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.taus" href="#ethik.explainer.Explainer.taus">taus</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>