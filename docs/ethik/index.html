<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>ethik API documentation</title>
<meta name="description" content="Ethik …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ethik</code></h1>
</header>
<section id="section-intro">
<h1 id="ethik">Ethik</h1>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#user-guide">User guide</a><ul>
<li><a href="#measuring-model-bias">Measuring model bias</a></li>
<li><a href="#evaluating-model-reliability">Evaluating model reliability</a></li>
<li><a href="#handling-images">Handling images</a></li>
</ul>
</li>
<li><a href="#api">API</a></li>
<li><a href="#authors">Authors</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p><a title="ethik" href="#ethik"><code>ethik</code></a> is a Python package for performing <a href="https://www.microsoft.com/en-us/research/blog/machine-learning-for-fair-decisions/">fair</a> and <a href="https://www.wikiwand.com/en/Explainable_artificial_intelligence">explainable</a> machine learning.</p>
<div align="center">
<img src="figures/overview.svg" width="660px" alt="overview"/>
</div>
<p><a title="ethik" href="#ethik"><code>ethik</code></a> can be used to:</p>
<ol>
<li>Determine if a predictive model is biased with respect to one or more features.</li>
<li>Understand how the performance of the model varies with respect to one or more features.</li>
<li>Visualize which parts of an image influence a model's predictions.</li>
</ol>
<h2 id="installation">Installation</h2>
<p>:warning: Python 3.6 or above is required :snake:</p>
<!---
**Via [PyPI](<https://pypi.org/project/ethik/>)**
<pre><code class="shell">&gt;&gt;&gt; pip install ethik
</code></pre>
-->
<p><strong>Via GitHub for the latest development version</strong></p>
<pre><code class="shell">&gt;&gt;&gt; pip install git+&lt;https://github.com/MaxHalford/ethik&gt;
&gt;&gt;&gt; # Or through SSH:
&gt;&gt;&gt; pip install git+ssh://git@github.com/MaxHalford/ethik.git
</code></pre>
<p><strong>Development installation</strong></p>
<pre><code class="shell">&gt;&gt;&gt; git clone &lt;https://github.com/MaxHalford/ethik&gt;
&gt;&gt;&gt; cd ethik
&gt;&gt;&gt; python setup.py develop
&gt;&gt;&gt; pip install -r requirements-dev.txt
&gt;&gt;&gt; pre-commit install # For black
</code></pre>
<h2 id="user-guide">User guide</h2>
<p>:point_up: Please check out <a href="notebooks/Adult.ipynb">this notebook</a> for more detailed code.</p>
<p>In the following example we'll be using the <a href="https://archive.ics.uci.edu/ml/datasets/adult">"Adult" dataset</a>. This dataset contains a binary label indicating if a person's annual income is larger than $50k. <code>ethik</code> can diagnose a model by looking at the predictions the model makes on a test set. Consequently, you first have to split your dataset in two (train and test).</p>
<pre><code class="python">from sklearn import model_selection

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, shuffle=True, random_state=42)
</code></pre>
<p>You then want to train your model on the training set and make predictions on the test set. In this example we'll train a gradient boosting classifier from the <a href="https://lightgbm.readthedocs.io/en/latest/">LightGBM library</a>. We'll use a variable named <code>y_pred</code> to store the predicted probabilities associated with the <code>True</code> label.</p>
<pre><code class="python">import lightgbm as lgb

model = lgb.LGBMClassifier(random_state=42).fit(X_train, y_train)

# We use a named pandas series to make plot labels more explicit
y_pred = model.predict_proba(X_test)[:, 1]
y_pred = pd.Series(y_pred, name='&gt;$50k')
</code></pre>
<p>We can now initialize an <code>ClassificationExplainer</code> using the default parameters.</p>
<pre><code class="python">import ethik

explainer = ethik.ClassificationExplainer()
</code></pre>
<h3 id="measuring-model-bias">Measuring model bias</h3>
<p><code>ethik</code> can be used to understand how the model predictions vary as a function of one or more features. For example we can look at how the model behaves with respect to the <code>age</code> feature.</p>
<pre><code class="python">explainer.plot_bias(X_test=X_test['age'], y_pred=y_pred)
</code></pre>
<div align="center">
<img src="figures/age_bias.png" alt="Age bias" />
</div>
<p>Recall that the target indicates if a person's annual salary is above $50k. <strong>We can see that the model predicts higher probabilities for older people</strong>. This isn't a surprising result, and could have just as well been observed by looking at the data. However, we can see that the predictions plateau at around 50 years old. Indeed, although salary is correlated with age, some people may retire early or lose their job. Furthermore we can see that the model understands the fact that salaries shrink once people get in age of retiring. This up-and-down relationship is in nature non-linear, and isn't picked up by summary statistics such as correlation coefficients, <a href="https://www.wikiwand.com/en/Odds_ratio">odds ratios</a>, and feature importances in general. Although the observations we made are quite obvious and rather intuitive, it's always good to confirm what the model is thinking. The point is that the curves produced by <code>plot_predictions</code> represent the relationship between a variable and the target according to the model, rather than the data.</p>
<p>We can also plot the distribution of predictions for more than one variable. However, because different variables have different scales we have to use a common measure to display them together. For this purpose we plot the τ ("tau") values. These values are contained between -1 and 1 and simply reflect by how much the variable is shifted from it's mean towards it's lower and upper quantiles. In the following figure a tau value of -1 corresponds to just under 20 years old whereas a tau value of 1 refers to being slightly over 60 years old.</p>
<pre><code class="python">explainer.plot_bias(X_test=X_test['age', 'education-num'], y_pred=y_pred)
</code></pre>
<div align="center">
<img src="figures/age_education_bias.png" alt="Age and education bias" />
</div>
<p>We can observe that the model assigns higher probabilities to people with higher degrees, which makes perfect sense. Again, this conveys much more of a story than summary statistics.</p>
<h3 id="evaluating-model-reliability">Evaluating model reliability</h3>
<p>Our methodology can also be used to evaluate the reliability of a model under different scenarios. Evaluation metrics that are commonly used in machine learning only tell you part of the story. Indeed they tell you the performance of a model <em>on average</em>. A more interesting approach is to visualize how accurate the model is with respect to the distribution of a variable.</p>
<pre><code class="python">explainer.plot_performance(
    X_test=X_test['age'],
    y_test=y_test,
    y_pred=y_pred &gt; 0.5,  # metrics.accuracy_score requires y_pred to be binary
    metric=metrics.accuracy_score
)
</code></pre>
<div align="center">
<img src="figures/age_accuracy.png" alt="Age accuracy" />
</div>
<p>In the above figure <strong>we can see that the model is more reliable for younger people than for older ones</strong>. Having a fine-grained understanding of the accuracy of a model can be of extreme help in real-life scenarios. Moreover this can help you understand from where the error of the model is coming from and guide your data science process.</p>
<p>Similarly to the <code>plot_predictions</code> method, we can display the performance of the model for multiple variables.</p>
<pre><code class="python">explainer.plot_performance(
    X_test=X_test['age', 'education-num'],
    y_test=y_test,
    y_pred=y_pred &gt; 0.5,
    metric=metrics.accuracy_score
)
</code></pre>
<div align="center">
<img src="figures/age_education_accuracy.png" alt="Age and education accuracy" />
</div>
<h3 id="handling-images">Handling images</h3>
<p>A special class named <code>ImageExplainer</code> can be used to analyze image classification models. It has the same API as <code>Explainer</code> but expects to be provided with an array of images. For example we can analyze a CNN run on the MNIST dataset <a href="https://keras.io/examples/mnist_cnn/">from the Keras documendation</a>. The model achieves an accuracy of around 99% on the test set.</p>
<p><strong><code>ImageExplainer</code> is being migrated to the new API and does not work for now.</strong></p>
<!---
We'll first produce predictions for the test set. `x_test` is expected to be an array of shape (`n_images`, `width`, `height`, `n_channels`). Because we are using the MNIST dataset, `x_test` is an array of shape `(10000, 28, 28, 1)`.
<pre><code class="python">y_pred = model.predict_proba(x_test)
</code></pre>
We will now fit an `ImageExplainer` to analyze the images in parallel using `n_jobs=-1`.
<pre><code class="python">import ethik

explainer = ethik.ImageExplainer(n_jobs=-1)
explainer = explainer.fit(x_test)
</code></pre>
-->
<h2 id="api">API</h2>
<p><strong>Working on it.</strong></p>
<h2 id="authors">Authors</h2>
<p>This work is led by members of the <a href="https://www.math.univ-toulouse.fr/?lang=en">Toulouse Institute of Mathematics</a>, namely:</p>
<ul>
<li><a href="https://www.math.univ-toulouse.fr/~fbachoc/">François Bachoc</a></li>
<li><a href="https://www.math.univ-toulouse.fr/~gamboa/newwab/Pages_Fabrice_Gamboa/Main_Page.html">Fabrice Gamboa</a></li>
<li><a href="https://maxhalford.github.io/">Max Halford</a></li>
<li><a href="https://vayel.github.io/">Vincent Lefoulon</a></li>
<li><a href="https://perso.math.univ-toulouse.fr/loubes/">Jean-Michel Loubes</a></li>
<li><a href="http://laurent.risser.free.fr/menuEng.html">Laurent Risser</a></li>
</ul>
<p>This work is financed by the <a href="http://www.cnrs.fr/">Centre National de la Recherche Scientifique (CNRS)</a> and is done in the context of the <a href="https://en.univ-toulouse.fr/aniti">Artificial and Natural Intelligence Toulouse Institute (ANITI)</a> project.</p>
<h2 id="license">License</h2>
<p>This software is released under the <a href="LICENSE">GPL license</a>.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
.. include:: ../README.md
&#34;&#34;&#34;
from . import datasets
from .__version__ import __version__
from .classification_explainer import ClassificationExplainer
from .regression_explainer import RegressionExplainer
from .image_classification_explainer import ImageClassificationExplainer


__all__ = [
    &#34;__version__&#34;,
    &#34;datasets&#34;,
    &#34;RegressionExplainer&#34;,
    &#34;ClassificationExplainer&#34;,
    &#34;ImageClassificationExplainer&#34;,
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="ethik.classification_explainer" href="classification_explainer.html">ethik.classification_explainer</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="ethik.datasets" href="datasets.html">ethik.datasets</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="ethik.explainer" href="explainer.html">ethik.explainer</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="ethik.image_classification_explainer" href="image_classification_explainer.html">ethik.image_classification_explainer</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="ethik.regression_explainer" href="regression_explainer.html">ethik.regression_explainer</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="ethik.utils" href="utils.html">ethik.utils</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ethik.ClassificationExplainer"><code class="flex name class">
<span>class <span class="ident">ClassificationExplainer</span></span>
<span>(</span><span>alpha=0.05, n_taus=41, lambda_iterations=5, n_jobs=1, verbose=False, n_samples=1, sample_frac=0.8, conf_level=0.05, memoize=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Explains the bias and reliability of model predictions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates by how close the <code>Explainer</code>
should look at extreme values of a distribution. The closer to zero, the more so
extreme values will be accounted for. The default is <code>0.05</code> which means that all values
beyond the 5th and 95th quantiles are ignored.</dd>
<dt><strong><code>n_taus</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of τ values to consider. The results will be more fine-grained the
higher this value is. However the computation time increases linearly with <code>n_taus</code>.
The default is <code>41</code> and corresponds to each τ being separated by it's neighbors by
<code>0.05</code>.</dd>
<dt><strong><code>lambda_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of iterations used when applying the Newton step
of the optimization procedure. Default is <code>5</code>.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of jobs to use for parallel computations. See
<code>joblib.Parallel()</code>. Default is <code>-1</code>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Passed to <code>joblib.Parallel()</code> for parallel computations.
Default is <code>False</code>.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples to use for the confidence interval.
If <code>1</code>, the default, no confidence interval is computed.</dd>
<dt><strong><code>sample_frac</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of lines in the dataset sampled to
generate the samples for the confidence interval. If <code>n_samples</code> is
<code>1</code>, no confidence interval is computed and the whole dataset is used.
Default is <code>0.8</code>.</dd>
<dt><strong><code>conf_level</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates the
quantile used for the confidence interval. Default is <code>0.05</code>, which
means that the confidence interval contains the data between the 5th
and 95th quantiles.</dd>
<dt><strong><code>memoize</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether or not memoization should be used or not. If <code>True</code>, then
intermediate results will be stored in order to avoid recomputing results that can be
reused by successively called methods. For example, if you call <code>plot_bias</code> followed by
<code>plot_bias_ranking</code> and <code>memoize</code> is <code>True</code>, then the intermediate results required by
<code>plot_bias</code> will be reused for <code>plot_bias_ranking</code>. Memoization is turned off by
default because it can lead to unexpected behavior depending on your usage.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ClassificationExplainer(Explainer):
    def plot_bias(self, X_test, y_pred, colors=None, yrange=None):
        &#34;&#34;&#34;Plot the bias for the features in `X_test`.

        See `ethik.explainer.Explainer.plot_bias()`.
        &#34;&#34;&#34;
        if yrange is None:
            yrange = [0, 1]

        X_test = pd.DataFrame(to_pandas(X_test))
        y_pred = pd.DataFrame(to_pandas(y_pred))

        if len(y_pred.columns) == 1:
            return super().plot_bias(
                X_test, y_pred.iloc[:, 0], colors=colors, yrange=yrange
            )

        if colors is None:
            features = X_test.columns
            #  Skip the lightest color as it is too light
            scale = cl.interp(cl.scales[&#34;10&#34;][&#34;qual&#34;][&#34;Paired&#34;], len(features) + 1)[1:]
            colors = {feat: scale[i] for i, feat in enumerate(features)}

        labels = y_pred.columns
        plots = []
        for label in labels:
            plots.append(
                super().plot_bias(X_test, y_pred[label], colors=colors, yrange=yrange)
            )

        fig = make_subplots(rows=len(labels), cols=1, shared_xaxes=True)
        for ilabel, (label, plot) in enumerate(zip(labels, plots)):
            fig.update_layout({f&#34;yaxis{ilabel+1}&#34;: dict(title=f&#34;Average {label}&#34;)})
            for trace in plot[&#34;data&#34;]:
                trace[&#34;showlegend&#34;] = ilabel == 0 and trace[&#34;showlegend&#34;]
                trace[&#34;legendgroup&#34;] = trace[&#34;name&#34;]
                fig.add_trace(trace, row=ilabel + 1, col=1)

        xlabel = &#34;tau&#34; if len(X_test.columns) &gt; 1 else f&#34;Average {X_test.columns[0]}&#34;
        fig.update_layout(
            {f&#34;xaxis{len(labels)}&#34;: dict(title=xlabel), &#34;plot_bgcolor&#34;: &#34;white&#34;}
        )
        return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ethik.explainer.Explainer" href="explainer.html#ethik.explainer.Explainer">Explainer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ethik.ClassificationExplainer.plot_bias"><code class="name flex">
<span>def <span class="ident">plot_bias</span></span>(<span>self, X_test, y_pred, colors=None, yrange=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot the bias for the features in <code>X_test</code>.</p>
<p>See <a title="ethik.explainer.Explainer.plot_bias" href="explainer.html#ethik.explainer.Explainer.plot_bias"><code>Explainer.plot_bias()</code></a>.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_bias(self, X_test, y_pred, colors=None, yrange=None):
    &#34;&#34;&#34;Plot the bias for the features in `X_test`.

    See `ethik.explainer.Explainer.plot_bias()`.
    &#34;&#34;&#34;
    if yrange is None:
        yrange = [0, 1]

    X_test = pd.DataFrame(to_pandas(X_test))
    y_pred = pd.DataFrame(to_pandas(y_pred))

    if len(y_pred.columns) == 1:
        return super().plot_bias(
            X_test, y_pred.iloc[:, 0], colors=colors, yrange=yrange
        )

    if colors is None:
        features = X_test.columns
        #  Skip the lightest color as it is too light
        scale = cl.interp(cl.scales[&#34;10&#34;][&#34;qual&#34;][&#34;Paired&#34;], len(features) + 1)[1:]
        colors = {feat: scale[i] for i, feat in enumerate(features)}

    labels = y_pred.columns
    plots = []
    for label in labels:
        plots.append(
            super().plot_bias(X_test, y_pred[label], colors=colors, yrange=yrange)
        )

    fig = make_subplots(rows=len(labels), cols=1, shared_xaxes=True)
    for ilabel, (label, plot) in enumerate(zip(labels, plots)):
        fig.update_layout({f&#34;yaxis{ilabel+1}&#34;: dict(title=f&#34;Average {label}&#34;)})
        for trace in plot[&#34;data&#34;]:
            trace[&#34;showlegend&#34;] = ilabel == 0 and trace[&#34;showlegend&#34;]
            trace[&#34;legendgroup&#34;] = trace[&#34;name&#34;]
            fig.add_trace(trace, row=ilabel + 1, col=1)

    xlabel = &#34;tau&#34; if len(X_test.columns) &gt; 1 else f&#34;Average {X_test.columns[0]}&#34;
    fig.update_layout(
        {f&#34;xaxis{len(labels)}&#34;: dict(title=xlabel), &#34;plot_bgcolor&#34;: &#34;white&#34;}
    )
    return fig</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ethik.explainer.Explainer" href="explainer.html#ethik.explainer.Explainer">Explainer</a></b></code>:
<ul class="hlist">
<li><code><a title="ethik.explainer.Explainer.explain_bias" href="explainer.html#ethik.explainer.Explainer.explain_bias">explain_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.explain_performance" href="explainer.html#ethik.explainer.Explainer.explain_performance">explain_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.get_metric_name" href="explainer.html#ethik.explainer.Explainer.get_metric_name">get_metric_name</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias_ranking" href="explainer.html#ethik.explainer.Explainer.plot_bias_ranking">plot_bias_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance" href="explainer.html#ethik.explainer.Explainer.plot_performance">plot_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance_ranking" href="explainer.html#ethik.explainer.Explainer.plot_performance_ranking">plot_performance_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.rank_by_bias" href="explainer.html#ethik.explainer.Explainer.rank_by_bias">rank_by_bias</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ethik.ImageClassificationExplainer"><code class="flex name class">
<span>class <span class="ident">ImageClassificationExplainer</span></span>
<span>(</span><span>alpha=0.05, lambda_iterations=5, n_jobs=-1, verbose=False, memoize=True)</span>
</code></dt>
<dd>
<section class="desc"><p>An explainer specially suited for image classification.</p>
<p>This has exactly the same API as <code>Explainer</code>, but expects to be provided with an array of
images instead of a tabular dataset.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ImageClassificationExplainer(explainer.Explainer):
    &#34;&#34;&#34;An explainer specially suited for image classification.

    This has exactly the same API as `Explainer`, but expects to be provided with an array of
    images instead of a tabular dataset.

    &#34;&#34;&#34;

    def __init__(self, alpha=0.05, lambda_iterations=5, n_jobs=-1, verbose=False, memoize=True):
        super().__init__(
            alpha=alpha,
            n_taus=2,
            lambda_iterations=lambda_iterations,
            n_jobs=n_jobs,
            verbose=verbose,
            memoize=memoize
        )

    def _set_image_shape(self, images):
        self.img_shape = images[0].shape
        if self.img_shape[-1] == 1:
            self.img_shape = self.img_shape[:-1]

    def explain_bias(self, X_test, y_pred):
        self._set_image_shape(images=X_test)
        return super().explain_bias(X_test=images_to_dataframe(X_test), y_pred=y_pred)

    def explain_performance(self, X_test, y_test, y_pred, metric):
        self._set_image_shape(images=X_test)
        return super().explain_performance(
            X_test=images_to_dataframe(X_test),
            y_test=y_test,
            y_pred=y_pred,
            metric=metric,
        )

    def _plot(self, z_values, n_cols):
        n_plots = len(z_values)
        labels = sorted(z_values)
        n_rows = n_plots // n_cols + 1
        fig = make_subplots(
            rows=n_rows,
            cols=n_cols,
            subplot_titles=list(map(str, labels)),
            shared_xaxes=&#34;all&#34;,
            shared_yaxes=&#34;all&#34;,
            horizontal_spacing=0.2 / n_cols,
            vertical_spacing=0.2 / n_rows,
        )

        # We want all the heatmaps to share the same scale
        zmin = min(np.min(z) for z in z_values.values())
        zmax = max(np.max(z) for z in z_values.values())

        # We want to make sure that 0 is at the center of the scale
        zmin, zmax = min(zmin, -zmax), max(zmax, -zmin)

        for i, label in enumerate(labels):
            fig.add_trace(
                go.Heatmap(
                    z=z_values[label][::-1],
                    x=list(range(self.img_shape[1])),
                    y=list(range(self.img_shape[0])),
                    zmin=zmin,
                    zmax=zmax,
                    colorscale=&#34;RdYlBu&#34;,
                    zsmooth=&#34;best&#34;,
                    showscale=i == 0,
                    name=label,
                    hoverinfo=&#34;x+y+z&#34;,
                    reversescale=True
                ),
                row=i // n_cols + 1,
                col=i % n_cols + 1,
            )

        for i in range(n_plots):
            fig.update_layout(
                {
                    f&#34;xaxis{i+1}&#34;: dict(visible=False),
                    f&#34;yaxis{i+1}&#34;: dict(scaleanchor=f&#34;x{i+1}&#34;, visible=False),
                }
            )
        fig.update_layout(
            margin=dict(t=20, l=20, b=20),
            width=800,
            height=800,
            autosize=False,
            plot_bgcolor=&#34;white&#34;,
        )
        return fig

    def plot_bias(self, X_test, y_pred, n_cols=3):
        biases = self.explain_bias(X_test=X_test, y_pred=y_pred)
        values = {}

        for label, group in biases.groupby(&#34;label&#34;):
            diffs = (
                group.query(&#34;tau == 1&#34;)[&#34;bias&#34;]
                - group.query(&#34;tau == -1&#34;)[&#34;bias&#34;].values
            )
            diffs = diffs.to_numpy().reshape(self.img_shape)
            values[label] = diffs

        return self._plot(values, n_cols=n_cols)

    def plot_metric(self, X_test, y_test, y_pred, metric):

        metrics = self.explain_metric(
            X=images_to_dataframe(images), y=y, y_pred=y_pred, metric=metric
        )

        # Create a plot if none is provided
        fig, ax = plt.subplots()

        diffs = (metrics.iloc[-1] - metrics.iloc[0]).fillna(0.0)
        img = ax.imshow(diffs.to_numpy().reshape(self.img_shape))

        fig.colorbar(img, ax=ax)

        return ax</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ethik.explainer.Explainer" href="explainer.html#ethik.explainer.Explainer">Explainer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ethik.ImageClassificationExplainer.plot_metric"><code class="name flex">
<span>def <span class="ident">plot_metric</span></span>(<span>self, X_test, y_test, y_pred, metric)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_metric(self, X_test, y_test, y_pred, metric):

    metrics = self.explain_metric(
        X=images_to_dataframe(images), y=y, y_pred=y_pred, metric=metric
    )

    # Create a plot if none is provided
    fig, ax = plt.subplots()

    diffs = (metrics.iloc[-1] - metrics.iloc[0]).fillna(0.0)
    img = ax.imshow(diffs.to_numpy().reshape(self.img_shape))

    fig.colorbar(img, ax=ax)

    return ax</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ethik.explainer.Explainer" href="explainer.html#ethik.explainer.Explainer">Explainer</a></b></code>:
<ul class="hlist">
<li><code><a title="ethik.explainer.Explainer.explain_bias" href="explainer.html#ethik.explainer.Explainer.explain_bias">explain_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.explain_performance" href="explainer.html#ethik.explainer.Explainer.explain_performance">explain_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.get_metric_name" href="explainer.html#ethik.explainer.Explainer.get_metric_name">get_metric_name</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias" href="explainer.html#ethik.explainer.Explainer.plot_bias">plot_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias_ranking" href="explainer.html#ethik.explainer.Explainer.plot_bias_ranking">plot_bias_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance" href="explainer.html#ethik.explainer.Explainer.plot_performance">plot_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance_ranking" href="explainer.html#ethik.explainer.Explainer.plot_performance_ranking">plot_performance_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.rank_by_bias" href="explainer.html#ethik.explainer.Explainer.rank_by_bias">rank_by_bias</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ethik.RegressionExplainer"><code class="flex name class">
<span>class <span class="ident">RegressionExplainer</span></span>
<span>(</span><span>alpha=0.05, n_taus=41, lambda_iterations=5, n_jobs=1, verbose=False, n_samples=1, sample_frac=0.8, conf_level=0.05, memoize=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Explains the bias and reliability of model predictions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates by how close the <code>Explainer</code>
should look at extreme values of a distribution. The closer to zero, the more so
extreme values will be accounted for. The default is <code>0.05</code> which means that all values
beyond the 5th and 95th quantiles are ignored.</dd>
<dt><strong><code>n_taus</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of τ values to consider. The results will be more fine-grained the
higher this value is. However the computation time increases linearly with <code>n_taus</code>.
The default is <code>41</code> and corresponds to each τ being separated by it's neighbors by
<code>0.05</code>.</dd>
<dt><strong><code>lambda_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of iterations used when applying the Newton step
of the optimization procedure. Default is <code>5</code>.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of jobs to use for parallel computations. See
<code>joblib.Parallel()</code>. Default is <code>-1</code>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Passed to <code>joblib.Parallel()</code> for parallel computations.
Default is <code>False</code>.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples to use for the confidence interval.
If <code>1</code>, the default, no confidence interval is computed.</dd>
<dt><strong><code>sample_frac</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of lines in the dataset sampled to
generate the samples for the confidence interval. If <code>n_samples</code> is
<code>1</code>, no confidence interval is computed and the whole dataset is used.
Default is <code>0.8</code>.</dd>
<dt><strong><code>conf_level</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>0.5</code> which indicates the
quantile used for the confidence interval. Default is <code>0.05</code>, which
means that the confidence interval contains the data between the 5th
and 95th quantiles.</dd>
<dt><strong><code>memoize</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether or not memoization should be used or not. If <code>True</code>, then
intermediate results will be stored in order to avoid recomputing results that can be
reused by successively called methods. For example, if you call <code>plot_bias</code> followed by
<code>plot_bias_ranking</code> and <code>memoize</code> is <code>True</code>, then the intermediate results required by
<code>plot_bias</code> will be reused for <code>plot_bias_ranking</code>. Memoization is turned off by
default because it can lead to unexpected behavior depending on your usage.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class RegressionExplainer(Explainer):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ethik.explainer.Explainer" href="explainer.html#ethik.explainer.Explainer">Explainer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ethik.explainer.Explainer" href="explainer.html#ethik.explainer.Explainer">Explainer</a></b></code>:
<ul class="hlist">
<li><code><a title="ethik.explainer.Explainer.explain_bias" href="explainer.html#ethik.explainer.Explainer.explain_bias">explain_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.explain_performance" href="explainer.html#ethik.explainer.Explainer.explain_performance">explain_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.get_metric_name" href="explainer.html#ethik.explainer.Explainer.get_metric_name">get_metric_name</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias" href="explainer.html#ethik.explainer.Explainer.plot_bias">plot_bias</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_bias_ranking" href="explainer.html#ethik.explainer.Explainer.plot_bias_ranking">plot_bias_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance" href="explainer.html#ethik.explainer.Explainer.plot_performance">plot_performance</a></code></li>
<li><code><a title="ethik.explainer.Explainer.plot_performance_ranking" href="explainer.html#ethik.explainer.Explainer.plot_performance_ranking">plot_performance_ranking</a></code></li>
<li><code><a title="ethik.explainer.Explainer.rank_by_bias" href="explainer.html#ethik.explainer.Explainer.rank_by_bias">rank_by_bias</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#ethik">Ethik</a><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#user-guide">User guide</a><ul>
<li><a href="#measuring-model-bias">Measuring model bias</a></li>
<li><a href="#evaluating-model-reliability">Evaluating model reliability</a></li>
<li><a href="#handling-images">Handling images</a></li>
</ul>
</li>
<li><a href="#api">API</a></li>
<li><a href="#authors">Authors</a></li>
<li><a href="#license">License</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="ethik.classification_explainer" href="classification_explainer.html">ethik.classification_explainer</a></code></li>
<li><code><a title="ethik.datasets" href="datasets.html">ethik.datasets</a></code></li>
<li><code><a title="ethik.explainer" href="explainer.html">ethik.explainer</a></code></li>
<li><code><a title="ethik.image_classification_explainer" href="image_classification_explainer.html">ethik.image_classification_explainer</a></code></li>
<li><code><a title="ethik.regression_explainer" href="regression_explainer.html">ethik.regression_explainer</a></code></li>
<li><code><a title="ethik.utils" href="utils.html">ethik.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ethik.ClassificationExplainer" href="#ethik.ClassificationExplainer">ClassificationExplainer</a></code></h4>
<ul class="">
<li><code><a title="ethik.ClassificationExplainer.plot_bias" href="#ethik.ClassificationExplainer.plot_bias">plot_bias</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ethik.ImageClassificationExplainer" href="#ethik.ImageClassificationExplainer">ImageClassificationExplainer</a></code></h4>
<ul class="">
<li><code><a title="ethik.ImageClassificationExplainer.plot_metric" href="#ethik.ImageClassificationExplainer.plot_metric">plot_metric</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ethik.RegressionExplainer" href="#ethik.RegressionExplainer">RegressionExplainer</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>