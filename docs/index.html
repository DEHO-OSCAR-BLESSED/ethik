---
head: |
  <style>
    .header {
      text-align: center;
      padding: 50px 20px 20px 20px;
    }

    .header .plots {
      display: flex;
      justify-content: space-around;
    }

    .header .plots > div {
      height: 300px;
    }

    .header .plots .modebar {
      display: none;
    }

    .content {
      padding: 20px;
      max-width: 800px;
      margin: auto;
    }

    .content h3 {
      margin: 0px;
      margin-top: 60px;
    }

    .content .title-underline {
      border: none;
      height: 2px;
      margin: 0px;
      margin-bottom: 15px;
    }

    .content .buttons {
      display: flex;
      justify-content: space-between;
      flex-wrap: wrap;
      margin-top: 40px;
    }
  </style>
js: |
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
  <script>
    (function() {
      for (var id of ["header-plot-left", "header-plot-right"]) {
        (function(id) {
          var baseUrl = ""; // TODO
          var dataUrl = "assets/plots/" + id + ".json";
          fetch(baseUrl + dataUrl)
          .then(function(resp) {
            return resp.json();
          })
          .then(function(json) {
            Plotly.plot(document.getElementById(id), json.data, json.layout);
          });
        })(id);
      }
    })();
  </script>
---

<div class="header mdl-color--accent">
  <h1 class="mdl-typography ethik-font">
    Ethik AI
  </h1>
  <h2 class="mdl-typography--display-1 mdl-typography--font-light">
    A Python package for AI fairness and interpretability.
  </h2>

  <div class="plots">
    <div id="header-plot-left"></div>
    <div id="header-plot-right"></div>
  </div>
</div>

<div class="content">
  <p>
    Machine learning models can largely outperform classical algorithms to make predictions
    about complexe problems, e.g. recognizing trees (which can vary a lot depending on
    the season, the species...). To do so, they learn from data (either from examples or experience)
    instead of following a well-defined sequence of instructions (like a cooking recipe). We humans
    do the same to teach our kids to recognize trees: we don't provide instructions but examples.
  </p>
  <p>
    It leads to the kids not always being able to clearly explain their decisions. Similarly,
    machine learning models can look like black-box decision-making programs. The question is:
    <strong>can we trust them?</strong> Understanding <strong>why a prediction was
    made</strong> is crucial to answer this question. As a bank customer, I want to know why
    the machine doesn't want to lend me money. This is known as <strong>model interpretability</strong>.
  </p>
  <p>
    Model interpretability opens the door to <strong>model fairness</strong>. Was the
    decision made based on an unfair view of the world? We don't want our models to reproduce 
    human discrimination that probably reflects in data. Model fairness consists
    in making sure a decision was not based on protected attributes (e.g. gender, race...
    for a bank loan).
  </p>
  <p>
    <code>ethik</code> is a Python package for performing <a href="{{ "/ai-fairness-and-interpretability" | relative_url }}">fair and explainable</a> machine learning. At it's core, the approach of <code>ethik</code> is to build <strong>counterfactual distributions</strong> that permit answering "what if?" scenarios. The idea is that we are able to stress one or more variables and observe how a machine learning model reacts to the stress. The stress is based on a statistical re-weighting scheme called <strong>entropic variable projection</strong>. The main benefit of our method is that it will only consider realistic scenarios, and will not build fake examples. You may find more information by reading <a href="https://arxiv.org/abs/1810.07924">this paper</a>, the <a href="{{ "/tutorials/binary-classification" | relative_url }}">Get started</a> tutorial and the <a href="{{ "/tutorials/how-it-works" | relative_url }}">How it works</a> page.
  </p>

<div align="center">
  <img src="{{ "/assets/img/overview.svg" | relative_url }}" width="660px" alt="overview"/>
</div>

  <p>
    Currently, <code>ethik</code> can be used for:
  </p>

  <ol>
    <li>Detecting model influence with respect to one or more (protected) attributes.</li>
    <li>Identifying causes for why a model performs poorly on certain inputs.</li>
    <li>Visualizing regions of an image that influence a model's predictions.</li>
  </ol>

  <p>
    We have more plans for the future.
  </p>

  <div class="buttons">
    <a href="{{ "/ai-fairness-and-interpretability" | relative_url }}">
      <button class="mdl-button mdl-js-button mdl-button--raised mdl-button--colored">
        Learn our problematic
      </button>
    </a>
    <a href="{{ "/tutorials/how-it-works" | relative_url }}">
      <button class="mdl-button mdl-js-button mdl-button--raised mdl-button--colored">
        Understand our method
      </button>
    </a>
    <a href="{{ "/tutorials/" | relative_url }}">
      <button class="mdl-button mdl-js-button mdl-button--raised mdl-button--colored">
        Use our package
      </button>
    </a>
  </div>

  <h3 id="about" class="mdl-typography--display-1 mdl-typography--font-light">
    About
  </h3>
  <hr class="title-underline mdl-color--accent" />

  <p>
    This work is led by members of the <a href="https://www.math.univ-toulouse.fr/?lang=en">Toulouse Institute of Mathematics</a>, namely:
  </p>

  <ul>
    <li><a href="https://www.math.univ-toulouse.fr/~fbachoc/">François Bachoc</a></li>
    <li><a href="https://www.math.univ-toulouse.fr/~gamboa/newwab/Pages_Fabrice_Gamboa/Main_Page.html">Fabrice Gamboa</a></li>
    <li><a href="https://maxhalford.github.io/">Max Halford</a></li>
    <li><a href="https://vayel.github.io/">Vincent Lefoulon</a></li>
    <li><a href="https://perso.math.univ-toulouse.fr/loubes/">Jean-Michel Loubes</a></li>
    <li><a href="http://laurent.risser.free.fr/menuEng.html">Laurent Risser</a></li>
  </ul>

  <p>
    You can contact us at <a href="mailto:jm.loubes@gmail.com">jm.loubes@gmail.com</a> and/or <a href="mailto:maxhalford25@gmail.com">maxhalford25@gmail.com</a>.
  </p>

  <p>
  This work is supported by the <a href="http://www.cnrs.fr/">Centre National de la Recherche Scientifique (CNRS)</a> and is done in the context of the <a href="https://en.univ-toulouse.fr/aniti">Artificial and Natural Intelligence Toulouse Institute (ANITI)</a> project.
  </p>

  <p>
    This software is released under the <a href="https://github.com/MaxHalford/ethik/blob/master/LICENSE">GPL license</a>.
  </p>
</div>
